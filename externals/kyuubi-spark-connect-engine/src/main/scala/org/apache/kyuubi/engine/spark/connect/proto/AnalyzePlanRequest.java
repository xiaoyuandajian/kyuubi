// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: spark/connect/base.proto

package org.apache.kyuubi.engine.spark.connect.proto;

/**
 * <pre>
 * Request to perform plan analyze, optionally to explain the plan.
 * </pre>
 *
 * Protobuf type {@code spark.connect.AnalyzePlanRequest}
 */
public final class AnalyzePlanRequest extends
    com.google.protobuf.GeneratedMessageV3 implements
    // @@protoc_insertion_point(message_implements:spark.connect.AnalyzePlanRequest)
    AnalyzePlanRequestOrBuilder {
private static final long serialVersionUID = 0L;
  // Use AnalyzePlanRequest.newBuilder() to construct.
  private AnalyzePlanRequest(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
    super(builder);
  }
  private AnalyzePlanRequest() {
    sessionId_ = "";
    clientType_ = "";
  }

  @Override
  @SuppressWarnings({"unused"})
  protected Object newInstance(
      UnusedPrivateParameter unused) {
    return new AnalyzePlanRequest();
  }

  public static final com.google.protobuf.Descriptors.Descriptor
      getDescriptor() {
    return org.apache.spark.connect.proto.Base.internal_static_spark_connect_AnalyzePlanRequest_descriptor;
  }

  @Override
  protected FieldAccessorTable
      internalGetFieldAccessorTable() {
    return org.apache.spark.connect.proto.Base.internal_static_spark_connect_AnalyzePlanRequest_fieldAccessorTable
        .ensureFieldAccessorsInitialized(
            org.apache.spark.connect.proto.AnalyzePlanRequest.class, org.apache.spark.connect.proto.AnalyzePlanRequest.Builder.class);
  }

  public interface SchemaOrBuilder extends
      // @@protoc_insertion_point(interface_extends:spark.connect.AnalyzePlanRequest.Schema)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <pre>
     * (Required) The logical plan to be analyzed.
     * </pre>
     *
     * <code>.spark.connect.Plan plan = 1;</code>
     * @return Whether the plan field is set.
     */
    boolean hasPlan();
    /**
     * <pre>
     * (Required) The logical plan to be analyzed.
     * </pre>
     *
     * <code>.spark.connect.Plan plan = 1;</code>
     * @return The plan.
     */
    org.apache.spark.connect.proto.Plan getPlan();
    /**
     * <pre>
     * (Required) The logical plan to be analyzed.
     * </pre>
     *
     * <code>.spark.connect.Plan plan = 1;</code>
     */
    org.apache.spark.connect.proto.PlanOrBuilder getPlanOrBuilder();
  }
  /**
   * Protobuf type {@code spark.connect.AnalyzePlanRequest.Schema}
   */
  public static final class Schema extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:spark.connect.AnalyzePlanRequest.Schema)
      SchemaOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use Schema.newBuilder() to construct.
    private Schema(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private Schema() {
    }

    @Override
    @SuppressWarnings({"unused"})
    protected Object newInstance(
        UnusedPrivateParameter unused) {
      return new Schema();
    }

    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.connect.proto.Base.internal_static_spark_connect_AnalyzePlanRequest_Schema_descriptor;
    }

    @Override
    protected FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.connect.proto.Base.internal_static_spark_connect_AnalyzePlanRequest_Schema_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.connect.proto.AnalyzePlanRequest.Schema.class, org.apache.spark.connect.proto.AnalyzePlanRequest.Schema.Builder.class);
    }

    public static final int PLAN_FIELD_NUMBER = 1;
    private org.apache.spark.connect.proto.Plan plan_;
    /**
     * <pre>
     * (Required) The logical plan to be analyzed.
     * </pre>
     *
     * <code>.spark.connect.Plan plan = 1;</code>
     * @return Whether the plan field is set.
     */
    @Override
    public boolean hasPlan() {
      return plan_ != null;
    }
    /**
     * <pre>
     * (Required) The logical plan to be analyzed.
     * </pre>
     *
     * <code>.spark.connect.Plan plan = 1;</code>
     * @return The plan.
     */
    @Override
    public org.apache.spark.connect.proto.Plan getPlan() {
      return plan_ == null ? org.apache.spark.connect.proto.Plan.getDefaultInstance() : plan_;
    }
    /**
     * <pre>
     * (Required) The logical plan to be analyzed.
     * </pre>
     *
     * <code>.spark.connect.Plan plan = 1;</code>
     */
    @Override
    public org.apache.spark.connect.proto.PlanOrBuilder getPlanOrBuilder() {
      return plan_ == null ? org.apache.spark.connect.proto.Plan.getDefaultInstance() : plan_;
    }

    private byte memoizedIsInitialized = -1;
    @Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (plan_ != null) {
        output.writeMessage(1, getPlan());
      }
      getUnknownFields().writeTo(output);
    }

    @Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (plan_ != null) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, getPlan());
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @Override
    public boolean equals(final Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.connect.proto.AnalyzePlanRequest.Schema)) {
        return super.equals(obj);
      }
      org.apache.spark.connect.proto.AnalyzePlanRequest.Schema other = (org.apache.spark.connect.proto.AnalyzePlanRequest.Schema) obj;

      if (hasPlan() != other.hasPlan()) return false;
      if (hasPlan()) {
        if (!getPlan()
            .equals(other.getPlan())) return false;
      }
      if (!getUnknownFields().equals(other.getUnknownFields())) return false;
      return true;
    }

    @Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasPlan()) {
        hash = (37 * hash) + PLAN_FIELD_NUMBER;
        hash = (53 * hash) + getPlan().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.connect.proto.AnalyzePlanRequest.Schema parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.Schema parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.Schema parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.Schema parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.Schema parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.Schema parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.Schema parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.Schema parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    public static org.apache.spark.connect.proto.AnalyzePlanRequest.Schema parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }

    public static org.apache.spark.connect.proto.AnalyzePlanRequest.Schema parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.Schema parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.Schema parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.connect.proto.AnalyzePlanRequest.Schema prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @Override
    protected Builder newBuilderForType(
        BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code spark.connect.AnalyzePlanRequest.Schema}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:spark.connect.AnalyzePlanRequest.Schema)
        org.apache.spark.connect.proto.AnalyzePlanRequest.SchemaOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.connect.proto.Base.internal_static_spark_connect_AnalyzePlanRequest_Schema_descriptor;
      }

      @Override
      protected FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.connect.proto.Base.internal_static_spark_connect_AnalyzePlanRequest_Schema_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.connect.proto.AnalyzePlanRequest.Schema.class, org.apache.spark.connect.proto.AnalyzePlanRequest.Schema.Builder.class);
      }

      // Construct using org.apache.spark.connect.proto.AnalyzePlanRequest.Schema.newBuilder()
      private Builder() {

      }

      private Builder(
          BuilderParent parent) {
        super(parent);

      }
      @Override
      public Builder clear() {
        super.clear();
        bitField0_ = 0;
        plan_ = null;
        if (planBuilder_ != null) {
          planBuilder_.dispose();
          planBuilder_ = null;
        }
        return this;
      }

      @Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.connect.proto.Base.internal_static_spark_connect_AnalyzePlanRequest_Schema_descriptor;
      }

      @Override
      public org.apache.spark.connect.proto.AnalyzePlanRequest.Schema getDefaultInstanceForType() {
        return org.apache.spark.connect.proto.AnalyzePlanRequest.Schema.getDefaultInstance();
      }

      @Override
      public org.apache.spark.connect.proto.AnalyzePlanRequest.Schema build() {
        org.apache.spark.connect.proto.AnalyzePlanRequest.Schema result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @Override
      public org.apache.spark.connect.proto.AnalyzePlanRequest.Schema buildPartial() {
        org.apache.spark.connect.proto.AnalyzePlanRequest.Schema result = new org.apache.spark.connect.proto.AnalyzePlanRequest.Schema(this);
        if (bitField0_ != 0) { buildPartial0(result); }
        onBuilt();
        return result;
      }

      private void buildPartial0(org.apache.spark.connect.proto.AnalyzePlanRequest.Schema result) {
        int from_bitField0_ = bitField0_;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          result.plan_ = planBuilder_ == null
              ? plan_
              : planBuilder_.build();
        }
      }

      @Override
      public Builder clone() {
        return super.clone();
      }
      @Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          Object value) {
        return super.setField(field, value);
      }
      @Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          Object value) {
        return super.addRepeatedField(field, value);
      }
      @Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.connect.proto.AnalyzePlanRequest.Schema) {
          return mergeFrom((org.apache.spark.connect.proto.AnalyzePlanRequest.Schema)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.connect.proto.AnalyzePlanRequest.Schema other) {
        if (other == org.apache.spark.connect.proto.AnalyzePlanRequest.Schema.getDefaultInstance()) return this;
        if (other.hasPlan()) {
          mergePlan(other.getPlan());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        onChanged();
        return this;
      }

      @Override
      public final boolean isInitialized() {
        return true;
      }

      @Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        if (extensionRegistry == null) {
          throw new NullPointerException();
        }
        try {
          boolean done = false;
          while (!done) {
            int tag = input.readTag();
            switch (tag) {
              case 0:
                done = true;
                break;
              case 10: {
                input.readMessage(
                    getPlanFieldBuilder().getBuilder(),
                    extensionRegistry);
                bitField0_ |= 0x00000001;
                break;
              } // case 10
              default: {
                if (!super.parseUnknownField(input, extensionRegistry, tag)) {
                  done = true; // was an endgroup tag
                }
                break;
              } // default:
            } // switch (tag)
          } // while (!done)
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.unwrapIOException();
        } finally {
          onChanged();
        } // finally
        return this;
      }
      private int bitField0_;

      private org.apache.spark.connect.proto.Plan plan_;
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.connect.proto.Plan, org.apache.spark.connect.proto.Plan.Builder, org.apache.spark.connect.proto.PlanOrBuilder> planBuilder_;
      /**
       * <pre>
       * (Required) The logical plan to be analyzed.
       * </pre>
       *
       * <code>.spark.connect.Plan plan = 1;</code>
       * @return Whether the plan field is set.
       */
      public boolean hasPlan() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <pre>
       * (Required) The logical plan to be analyzed.
       * </pre>
       *
       * <code>.spark.connect.Plan plan = 1;</code>
       * @return The plan.
       */
      public org.apache.spark.connect.proto.Plan getPlan() {
        if (planBuilder_ == null) {
          return plan_ == null ? org.apache.spark.connect.proto.Plan.getDefaultInstance() : plan_;
        } else {
          return planBuilder_.getMessage();
        }
      }
      /**
       * <pre>
       * (Required) The logical plan to be analyzed.
       * </pre>
       *
       * <code>.spark.connect.Plan plan = 1;</code>
       */
      public Builder setPlan(org.apache.spark.connect.proto.Plan value) {
        if (planBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          plan_ = value;
        } else {
          planBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * (Required) The logical plan to be analyzed.
       * </pre>
       *
       * <code>.spark.connect.Plan plan = 1;</code>
       */
      public Builder setPlan(
          org.apache.spark.connect.proto.Plan.Builder builderForValue) {
        if (planBuilder_ == null) {
          plan_ = builderForValue.build();
        } else {
          planBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * (Required) The logical plan to be analyzed.
       * </pre>
       *
       * <code>.spark.connect.Plan plan = 1;</code>
       */
      public Builder mergePlan(org.apache.spark.connect.proto.Plan value) {
        if (planBuilder_ == null) {
          if (((bitField0_ & 0x00000001) != 0) &&
            plan_ != null &&
            plan_ != org.apache.spark.connect.proto.Plan.getDefaultInstance()) {
            getPlanBuilder().mergeFrom(value);
          } else {
            plan_ = value;
          }
        } else {
          planBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * (Required) The logical plan to be analyzed.
       * </pre>
       *
       * <code>.spark.connect.Plan plan = 1;</code>
       */
      public Builder clearPlan() {
        bitField0_ = (bitField0_ & ~0x00000001);
        plan_ = null;
        if (planBuilder_ != null) {
          planBuilder_.dispose();
          planBuilder_ = null;
        }
        onChanged();
        return this;
      }
      /**
       * <pre>
       * (Required) The logical plan to be analyzed.
       * </pre>
       *
       * <code>.spark.connect.Plan plan = 1;</code>
       */
      public org.apache.spark.connect.proto.Plan.Builder getPlanBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getPlanFieldBuilder().getBuilder();
      }
      /**
       * <pre>
       * (Required) The logical plan to be analyzed.
       * </pre>
       *
       * <code>.spark.connect.Plan plan = 1;</code>
       */
      public org.apache.spark.connect.proto.PlanOrBuilder getPlanOrBuilder() {
        if (planBuilder_ != null) {
          return planBuilder_.getMessageOrBuilder();
        } else {
          return plan_ == null ?
              org.apache.spark.connect.proto.Plan.getDefaultInstance() : plan_;
        }
      }
      /**
       * <pre>
       * (Required) The logical plan to be analyzed.
       * </pre>
       *
       * <code>.spark.connect.Plan plan = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.connect.proto.Plan, org.apache.spark.connect.proto.Plan.Builder, org.apache.spark.connect.proto.PlanOrBuilder> 
          getPlanFieldBuilder() {
        if (planBuilder_ == null) {
          planBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.apache.spark.connect.proto.Plan, org.apache.spark.connect.proto.Plan.Builder, org.apache.spark.connect.proto.PlanOrBuilder>(
                  getPlan(),
                  getParentForChildren(),
                  isClean());
          plan_ = null;
        }
        return planBuilder_;
      }
      @Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:spark.connect.AnalyzePlanRequest.Schema)
    }

    // @@protoc_insertion_point(class_scope:spark.connect.AnalyzePlanRequest.Schema)
    private static final org.apache.spark.connect.proto.AnalyzePlanRequest.Schema DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.connect.proto.AnalyzePlanRequest.Schema();
    }

    public static org.apache.spark.connect.proto.AnalyzePlanRequest.Schema getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<Schema>
        PARSER = new com.google.protobuf.AbstractParser<Schema>() {
      @Override
      public Schema parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        Builder builder = newBuilder();
        try {
          builder.mergeFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.setUnfinishedMessage(builder.buildPartial());
        } catch (com.google.protobuf.UninitializedMessageException e) {
          throw e.asInvalidProtocolBufferException().setUnfinishedMessage(builder.buildPartial());
        } catch (java.io.IOException e) {
          throw new com.google.protobuf.InvalidProtocolBufferException(e)
              .setUnfinishedMessage(builder.buildPartial());
        }
        return builder.buildPartial();
      }
    };

    public static com.google.protobuf.Parser<Schema> parser() {
      return PARSER;
    }

    @Override
    public com.google.protobuf.Parser<Schema> getParserForType() {
      return PARSER;
    }

    @Override
    public org.apache.spark.connect.proto.AnalyzePlanRequest.Schema getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface ExplainOrBuilder extends
      // @@protoc_insertion_point(interface_extends:spark.connect.AnalyzePlanRequest.Explain)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <pre>
     * (Required) The logical plan to be analyzed.
     * </pre>
     *
     * <code>.spark.connect.Plan plan = 1;</code>
     * @return Whether the plan field is set.
     */
    boolean hasPlan();
    /**
     * <pre>
     * (Required) The logical plan to be analyzed.
     * </pre>
     *
     * <code>.spark.connect.Plan plan = 1;</code>
     * @return The plan.
     */
    org.apache.spark.connect.proto.Plan getPlan();
    /**
     * <pre>
     * (Required) The logical plan to be analyzed.
     * </pre>
     *
     * <code>.spark.connect.Plan plan = 1;</code>
     */
    org.apache.spark.connect.proto.PlanOrBuilder getPlanOrBuilder();

    /**
     * <pre>
     * (Required) For analyzePlan rpc calls, configure the mode to explain plan in strings.
     * </pre>
     *
     * <code>.spark.connect.AnalyzePlanRequest.Explain.ExplainMode explain_mode = 2;</code>
     * @return The enum numeric value on the wire for explainMode.
     */
    int getExplainModeValue();
    /**
     * <pre>
     * (Required) For analyzePlan rpc calls, configure the mode to explain plan in strings.
     * </pre>
     *
     * <code>.spark.connect.AnalyzePlanRequest.Explain.ExplainMode explain_mode = 2;</code>
     * @return The explainMode.
     */
    org.apache.spark.connect.proto.AnalyzePlanRequest.Explain.ExplainMode getExplainMode();
  }
  /**
   * <pre>
   * Explains the input plan based on a configurable mode.
   * </pre>
   *
   * Protobuf type {@code spark.connect.AnalyzePlanRequest.Explain}
   */
  public static final class Explain extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:spark.connect.AnalyzePlanRequest.Explain)
      ExplainOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use Explain.newBuilder() to construct.
    private Explain(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private Explain() {
      explainMode_ = 0;
    }

    @Override
    @SuppressWarnings({"unused"})
    protected Object newInstance(
        UnusedPrivateParameter unused) {
      return new Explain();
    }

    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.connect.proto.Base.internal_static_spark_connect_AnalyzePlanRequest_Explain_descriptor;
    }

    @Override
    protected FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.connect.proto.Base.internal_static_spark_connect_AnalyzePlanRequest_Explain_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.connect.proto.AnalyzePlanRequest.Explain.class, org.apache.spark.connect.proto.AnalyzePlanRequest.Explain.Builder.class);
    }

    /**
     * <pre>
     * Plan explanation mode.
     * </pre>
     *
     * Protobuf enum {@code spark.connect.AnalyzePlanRequest.Explain.ExplainMode}
     */
    public enum ExplainMode
        implements com.google.protobuf.ProtocolMessageEnum {
      /**
       * <code>EXPLAIN_MODE_UNSPECIFIED = 0;</code>
       */
      EXPLAIN_MODE_UNSPECIFIED(0),
      /**
       * <pre>
       * Generates only physical plan.
       * </pre>
       *
       * <code>EXPLAIN_MODE_SIMPLE = 1;</code>
       */
      EXPLAIN_MODE_SIMPLE(1),
      /**
       * <pre>
       * Generates parsed logical plan, analyzed logical plan, optimized logical plan and physical plan.
       * Parsed Logical plan is a unresolved plan that extracted from the query. Analyzed logical plans
       * transforms which translates unresolvedAttribute and unresolvedRelation into fully typed objects.
       * The optimized logical plan transforms through a set of optimization rules, resulting in the
       * physical plan.
       * </pre>
       *
       * <code>EXPLAIN_MODE_EXTENDED = 2;</code>
       */
      EXPLAIN_MODE_EXTENDED(2),
      /**
       * <pre>
       * Generates code for the statement, if any and a physical plan.
       * </pre>
       *
       * <code>EXPLAIN_MODE_CODEGEN = 3;</code>
       */
      EXPLAIN_MODE_CODEGEN(3),
      /**
       * <pre>
       * If plan node statistics are available, generates a logical plan and also the statistics.
       * </pre>
       *
       * <code>EXPLAIN_MODE_COST = 4;</code>
       */
      EXPLAIN_MODE_COST(4),
      /**
       * <pre>
       * Generates a physical plan outline and also node details.
       * </pre>
       *
       * <code>EXPLAIN_MODE_FORMATTED = 5;</code>
       */
      EXPLAIN_MODE_FORMATTED(5),
      UNRECOGNIZED(-1),
      ;

      /**
       * <code>EXPLAIN_MODE_UNSPECIFIED = 0;</code>
       */
      public static final int EXPLAIN_MODE_UNSPECIFIED_VALUE = 0;
      /**
       * <pre>
       * Generates only physical plan.
       * </pre>
       *
       * <code>EXPLAIN_MODE_SIMPLE = 1;</code>
       */
      public static final int EXPLAIN_MODE_SIMPLE_VALUE = 1;
      /**
       * <pre>
       * Generates parsed logical plan, analyzed logical plan, optimized logical plan and physical plan.
       * Parsed Logical plan is a unresolved plan that extracted from the query. Analyzed logical plans
       * transforms which translates unresolvedAttribute and unresolvedRelation into fully typed objects.
       * The optimized logical plan transforms through a set of optimization rules, resulting in the
       * physical plan.
       * </pre>
       *
       * <code>EXPLAIN_MODE_EXTENDED = 2;</code>
       */
      public static final int EXPLAIN_MODE_EXTENDED_VALUE = 2;
      /**
       * <pre>
       * Generates code for the statement, if any and a physical plan.
       * </pre>
       *
       * <code>EXPLAIN_MODE_CODEGEN = 3;</code>
       */
      public static final int EXPLAIN_MODE_CODEGEN_VALUE = 3;
      /**
       * <pre>
       * If plan node statistics are available, generates a logical plan and also the statistics.
       * </pre>
       *
       * <code>EXPLAIN_MODE_COST = 4;</code>
       */
      public static final int EXPLAIN_MODE_COST_VALUE = 4;
      /**
       * <pre>
       * Generates a physical plan outline and also node details.
       * </pre>
       *
       * <code>EXPLAIN_MODE_FORMATTED = 5;</code>
       */
      public static final int EXPLAIN_MODE_FORMATTED_VALUE = 5;


      public final int getNumber() {
        if (this == UNRECOGNIZED) {
          throw new IllegalArgumentException(
              "Can't get the number of an unknown enum value.");
        }
        return value;
      }

      /**
       * @param value The numeric wire value of the corresponding enum entry.
       * @return The enum associated with the given numeric wire value.
       * @deprecated Use {@link #forNumber(int)} instead.
       */
      @Deprecated
      public static ExplainMode valueOf(int value) {
        return forNumber(value);
      }

      /**
       * @param value The numeric wire value of the corresponding enum entry.
       * @return The enum associated with the given numeric wire value.
       */
      public static ExplainMode forNumber(int value) {
        switch (value) {
          case 0: return EXPLAIN_MODE_UNSPECIFIED;
          case 1: return EXPLAIN_MODE_SIMPLE;
          case 2: return EXPLAIN_MODE_EXTENDED;
          case 3: return EXPLAIN_MODE_CODEGEN;
          case 4: return EXPLAIN_MODE_COST;
          case 5: return EXPLAIN_MODE_FORMATTED;
          default: return null;
        }
      }

      public static com.google.protobuf.Internal.EnumLiteMap<ExplainMode>
          internalGetValueMap() {
        return internalValueMap;
      }
      private static final com.google.protobuf.Internal.EnumLiteMap<
          ExplainMode> internalValueMap =
            new com.google.protobuf.Internal.EnumLiteMap<ExplainMode>() {
              public ExplainMode findValueByNumber(int number) {
                return ExplainMode.forNumber(number);
              }
            };

      public final com.google.protobuf.Descriptors.EnumValueDescriptor
          getValueDescriptor() {
        if (this == UNRECOGNIZED) {
          throw new IllegalStateException(
              "Can't get the descriptor of an unrecognized enum value.");
        }
        return getDescriptor().getValues().get(ordinal());
      }
      public final com.google.protobuf.Descriptors.EnumDescriptor
          getDescriptorForType() {
        return getDescriptor();
      }
      public static final com.google.protobuf.Descriptors.EnumDescriptor
          getDescriptor() {
        return org.apache.spark.connect.proto.AnalyzePlanRequest.Explain.getDescriptor().getEnumTypes().get(0);
      }

      private static final ExplainMode[] VALUES = values();

      public static ExplainMode valueOf(
          com.google.protobuf.Descriptors.EnumValueDescriptor desc) {
        if (desc.getType() != getDescriptor()) {
          throw new IllegalArgumentException(
            "EnumValueDescriptor is not for this type.");
        }
        if (desc.getIndex() == -1) {
          return UNRECOGNIZED;
        }
        return VALUES[desc.getIndex()];
      }

      private final int value;

      private ExplainMode(int value) {
        this.value = value;
      }

      // @@protoc_insertion_point(enum_scope:spark.connect.AnalyzePlanRequest.Explain.ExplainMode)
    }

    public static final int PLAN_FIELD_NUMBER = 1;
    private org.apache.spark.connect.proto.Plan plan_;
    /**
     * <pre>
     * (Required) The logical plan to be analyzed.
     * </pre>
     *
     * <code>.spark.connect.Plan plan = 1;</code>
     * @return Whether the plan field is set.
     */
    @Override
    public boolean hasPlan() {
      return plan_ != null;
    }
    /**
     * <pre>
     * (Required) The logical plan to be analyzed.
     * </pre>
     *
     * <code>.spark.connect.Plan plan = 1;</code>
     * @return The plan.
     */
    @Override
    public org.apache.spark.connect.proto.Plan getPlan() {
      return plan_ == null ? org.apache.spark.connect.proto.Plan.getDefaultInstance() : plan_;
    }
    /**
     * <pre>
     * (Required) The logical plan to be analyzed.
     * </pre>
     *
     * <code>.spark.connect.Plan plan = 1;</code>
     */
    @Override
    public org.apache.spark.connect.proto.PlanOrBuilder getPlanOrBuilder() {
      return plan_ == null ? org.apache.spark.connect.proto.Plan.getDefaultInstance() : plan_;
    }

    public static final int EXPLAIN_MODE_FIELD_NUMBER = 2;
    private int explainMode_ = 0;
    /**
     * <pre>
     * (Required) For analyzePlan rpc calls, configure the mode to explain plan in strings.
     * </pre>
     *
     * <code>.spark.connect.AnalyzePlanRequest.Explain.ExplainMode explain_mode = 2;</code>
     * @return The enum numeric value on the wire for explainMode.
     */
    @Override public int getExplainModeValue() {
      return explainMode_;
    }
    /**
     * <pre>
     * (Required) For analyzePlan rpc calls, configure the mode to explain plan in strings.
     * </pre>
     *
     * <code>.spark.connect.AnalyzePlanRequest.Explain.ExplainMode explain_mode = 2;</code>
     * @return The explainMode.
     */
    @Override public org.apache.spark.connect.proto.AnalyzePlanRequest.Explain.ExplainMode getExplainMode() {
      org.apache.spark.connect.proto.AnalyzePlanRequest.Explain.ExplainMode result = org.apache.spark.connect.proto.AnalyzePlanRequest.Explain.ExplainMode.forNumber(explainMode_);
      return result == null ? org.apache.spark.connect.proto.AnalyzePlanRequest.Explain.ExplainMode.UNRECOGNIZED : result;
    }

    private byte memoizedIsInitialized = -1;
    @Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (plan_ != null) {
        output.writeMessage(1, getPlan());
      }
      if (explainMode_ != org.apache.spark.connect.proto.AnalyzePlanRequest.Explain.ExplainMode.EXPLAIN_MODE_UNSPECIFIED.getNumber()) {
        output.writeEnum(2, explainMode_);
      }
      getUnknownFields().writeTo(output);
    }

    @Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (plan_ != null) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, getPlan());
      }
      if (explainMode_ != org.apache.spark.connect.proto.AnalyzePlanRequest.Explain.ExplainMode.EXPLAIN_MODE_UNSPECIFIED.getNumber()) {
        size += com.google.protobuf.CodedOutputStream
          .computeEnumSize(2, explainMode_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @Override
    public boolean equals(final Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.connect.proto.AnalyzePlanRequest.Explain)) {
        return super.equals(obj);
      }
      org.apache.spark.connect.proto.AnalyzePlanRequest.Explain other = (org.apache.spark.connect.proto.AnalyzePlanRequest.Explain) obj;

      if (hasPlan() != other.hasPlan()) return false;
      if (hasPlan()) {
        if (!getPlan()
            .equals(other.getPlan())) return false;
      }
      if (explainMode_ != other.explainMode_) return false;
      if (!getUnknownFields().equals(other.getUnknownFields())) return false;
      return true;
    }

    @Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasPlan()) {
        hash = (37 * hash) + PLAN_FIELD_NUMBER;
        hash = (53 * hash) + getPlan().hashCode();
      }
      hash = (37 * hash) + EXPLAIN_MODE_FIELD_NUMBER;
      hash = (53 * hash) + explainMode_;
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.connect.proto.AnalyzePlanRequest.Explain parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.Explain parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.Explain parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.Explain parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.Explain parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.Explain parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.Explain parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.Explain parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    public static org.apache.spark.connect.proto.AnalyzePlanRequest.Explain parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }

    public static org.apache.spark.connect.proto.AnalyzePlanRequest.Explain parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.Explain parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.Explain parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.connect.proto.AnalyzePlanRequest.Explain prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @Override
    protected Builder newBuilderForType(
        BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     * Explains the input plan based on a configurable mode.
     * </pre>
     *
     * Protobuf type {@code spark.connect.AnalyzePlanRequest.Explain}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:spark.connect.AnalyzePlanRequest.Explain)
        org.apache.spark.connect.proto.AnalyzePlanRequest.ExplainOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.connect.proto.Base.internal_static_spark_connect_AnalyzePlanRequest_Explain_descriptor;
      }

      @Override
      protected FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.connect.proto.Base.internal_static_spark_connect_AnalyzePlanRequest_Explain_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.connect.proto.AnalyzePlanRequest.Explain.class, org.apache.spark.connect.proto.AnalyzePlanRequest.Explain.Builder.class);
      }

      // Construct using org.apache.spark.connect.proto.AnalyzePlanRequest.Explain.newBuilder()
      private Builder() {

      }

      private Builder(
          BuilderParent parent) {
        super(parent);

      }
      @Override
      public Builder clear() {
        super.clear();
        bitField0_ = 0;
        plan_ = null;
        if (planBuilder_ != null) {
          planBuilder_.dispose();
          planBuilder_ = null;
        }
        explainMode_ = 0;
        return this;
      }

      @Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.connect.proto.Base.internal_static_spark_connect_AnalyzePlanRequest_Explain_descriptor;
      }

      @Override
      public org.apache.spark.connect.proto.AnalyzePlanRequest.Explain getDefaultInstanceForType() {
        return org.apache.spark.connect.proto.AnalyzePlanRequest.Explain.getDefaultInstance();
      }

      @Override
      public org.apache.spark.connect.proto.AnalyzePlanRequest.Explain build() {
        org.apache.spark.connect.proto.AnalyzePlanRequest.Explain result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @Override
      public org.apache.spark.connect.proto.AnalyzePlanRequest.Explain buildPartial() {
        org.apache.spark.connect.proto.AnalyzePlanRequest.Explain result = new org.apache.spark.connect.proto.AnalyzePlanRequest.Explain(this);
        if (bitField0_ != 0) { buildPartial0(result); }
        onBuilt();
        return result;
      }

      private void buildPartial0(org.apache.spark.connect.proto.AnalyzePlanRequest.Explain result) {
        int from_bitField0_ = bitField0_;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          result.plan_ = planBuilder_ == null
              ? plan_
              : planBuilder_.build();
        }
        if (((from_bitField0_ & 0x00000002) != 0)) {
          result.explainMode_ = explainMode_;
        }
      }

      @Override
      public Builder clone() {
        return super.clone();
      }
      @Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          Object value) {
        return super.setField(field, value);
      }
      @Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          Object value) {
        return super.addRepeatedField(field, value);
      }
      @Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.connect.proto.AnalyzePlanRequest.Explain) {
          return mergeFrom((org.apache.spark.connect.proto.AnalyzePlanRequest.Explain)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.connect.proto.AnalyzePlanRequest.Explain other) {
        if (other == org.apache.spark.connect.proto.AnalyzePlanRequest.Explain.getDefaultInstance()) return this;
        if (other.hasPlan()) {
          mergePlan(other.getPlan());
        }
        if (other.explainMode_ != 0) {
          setExplainModeValue(other.getExplainModeValue());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        onChanged();
        return this;
      }

      @Override
      public final boolean isInitialized() {
        return true;
      }

      @Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        if (extensionRegistry == null) {
          throw new NullPointerException();
        }
        try {
          boolean done = false;
          while (!done) {
            int tag = input.readTag();
            switch (tag) {
              case 0:
                done = true;
                break;
              case 10: {
                input.readMessage(
                    getPlanFieldBuilder().getBuilder(),
                    extensionRegistry);
                bitField0_ |= 0x00000001;
                break;
              } // case 10
              case 16: {
                explainMode_ = input.readEnum();
                bitField0_ |= 0x00000002;
                break;
              } // case 16
              default: {
                if (!super.parseUnknownField(input, extensionRegistry, tag)) {
                  done = true; // was an endgroup tag
                }
                break;
              } // default:
            } // switch (tag)
          } // while (!done)
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.unwrapIOException();
        } finally {
          onChanged();
        } // finally
        return this;
      }
      private int bitField0_;

      private org.apache.spark.connect.proto.Plan plan_;
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.connect.proto.Plan, org.apache.spark.connect.proto.Plan.Builder, org.apache.spark.connect.proto.PlanOrBuilder> planBuilder_;
      /**
       * <pre>
       * (Required) The logical plan to be analyzed.
       * </pre>
       *
       * <code>.spark.connect.Plan plan = 1;</code>
       * @return Whether the plan field is set.
       */
      public boolean hasPlan() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <pre>
       * (Required) The logical plan to be analyzed.
       * </pre>
       *
       * <code>.spark.connect.Plan plan = 1;</code>
       * @return The plan.
       */
      public org.apache.spark.connect.proto.Plan getPlan() {
        if (planBuilder_ == null) {
          return plan_ == null ? org.apache.spark.connect.proto.Plan.getDefaultInstance() : plan_;
        } else {
          return planBuilder_.getMessage();
        }
      }
      /**
       * <pre>
       * (Required) The logical plan to be analyzed.
       * </pre>
       *
       * <code>.spark.connect.Plan plan = 1;</code>
       */
      public Builder setPlan(org.apache.spark.connect.proto.Plan value) {
        if (planBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          plan_ = value;
        } else {
          planBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * (Required) The logical plan to be analyzed.
       * </pre>
       *
       * <code>.spark.connect.Plan plan = 1;</code>
       */
      public Builder setPlan(
          org.apache.spark.connect.proto.Plan.Builder builderForValue) {
        if (planBuilder_ == null) {
          plan_ = builderForValue.build();
        } else {
          planBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * (Required) The logical plan to be analyzed.
       * </pre>
       *
       * <code>.spark.connect.Plan plan = 1;</code>
       */
      public Builder mergePlan(org.apache.spark.connect.proto.Plan value) {
        if (planBuilder_ == null) {
          if (((bitField0_ & 0x00000001) != 0) &&
            plan_ != null &&
            plan_ != org.apache.spark.connect.proto.Plan.getDefaultInstance()) {
            getPlanBuilder().mergeFrom(value);
          } else {
            plan_ = value;
          }
        } else {
          planBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * (Required) The logical plan to be analyzed.
       * </pre>
       *
       * <code>.spark.connect.Plan plan = 1;</code>
       */
      public Builder clearPlan() {
        bitField0_ = (bitField0_ & ~0x00000001);
        plan_ = null;
        if (planBuilder_ != null) {
          planBuilder_.dispose();
          planBuilder_ = null;
        }
        onChanged();
        return this;
      }
      /**
       * <pre>
       * (Required) The logical plan to be analyzed.
       * </pre>
       *
       * <code>.spark.connect.Plan plan = 1;</code>
       */
      public org.apache.spark.connect.proto.Plan.Builder getPlanBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getPlanFieldBuilder().getBuilder();
      }
      /**
       * <pre>
       * (Required) The logical plan to be analyzed.
       * </pre>
       *
       * <code>.spark.connect.Plan plan = 1;</code>
       */
      public org.apache.spark.connect.proto.PlanOrBuilder getPlanOrBuilder() {
        if (planBuilder_ != null) {
          return planBuilder_.getMessageOrBuilder();
        } else {
          return plan_ == null ?
              org.apache.spark.connect.proto.Plan.getDefaultInstance() : plan_;
        }
      }
      /**
       * <pre>
       * (Required) The logical plan to be analyzed.
       * </pre>
       *
       * <code>.spark.connect.Plan plan = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.connect.proto.Plan, org.apache.spark.connect.proto.Plan.Builder, org.apache.spark.connect.proto.PlanOrBuilder> 
          getPlanFieldBuilder() {
        if (planBuilder_ == null) {
          planBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.apache.spark.connect.proto.Plan, org.apache.spark.connect.proto.Plan.Builder, org.apache.spark.connect.proto.PlanOrBuilder>(
                  getPlan(),
                  getParentForChildren(),
                  isClean());
          plan_ = null;
        }
        return planBuilder_;
      }

      private int explainMode_ = 0;
      /**
       * <pre>
       * (Required) For analyzePlan rpc calls, configure the mode to explain plan in strings.
       * </pre>
       *
       * <code>.spark.connect.AnalyzePlanRequest.Explain.ExplainMode explain_mode = 2;</code>
       * @return The enum numeric value on the wire for explainMode.
       */
      @Override public int getExplainModeValue() {
        return explainMode_;
      }
      /**
       * <pre>
       * (Required) For analyzePlan rpc calls, configure the mode to explain plan in strings.
       * </pre>
       *
       * <code>.spark.connect.AnalyzePlanRequest.Explain.ExplainMode explain_mode = 2;</code>
       * @param value The enum numeric value on the wire for explainMode to set.
       * @return This builder for chaining.
       */
      public Builder setExplainModeValue(int value) {
        explainMode_ = value;
        bitField0_ |= 0x00000002;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * (Required) For analyzePlan rpc calls, configure the mode to explain plan in strings.
       * </pre>
       *
       * <code>.spark.connect.AnalyzePlanRequest.Explain.ExplainMode explain_mode = 2;</code>
       * @return The explainMode.
       */
      @Override
      public org.apache.spark.connect.proto.AnalyzePlanRequest.Explain.ExplainMode getExplainMode() {
        org.apache.spark.connect.proto.AnalyzePlanRequest.Explain.ExplainMode result = org.apache.spark.connect.proto.AnalyzePlanRequest.Explain.ExplainMode.forNumber(explainMode_);
        return result == null ? org.apache.spark.connect.proto.AnalyzePlanRequest.Explain.ExplainMode.UNRECOGNIZED : result;
      }
      /**
       * <pre>
       * (Required) For analyzePlan rpc calls, configure the mode to explain plan in strings.
       * </pre>
       *
       * <code>.spark.connect.AnalyzePlanRequest.Explain.ExplainMode explain_mode = 2;</code>
       * @param value The explainMode to set.
       * @return This builder for chaining.
       */
      public Builder setExplainMode(org.apache.spark.connect.proto.AnalyzePlanRequest.Explain.ExplainMode value) {
        if (value == null) {
          throw new NullPointerException();
        }
        bitField0_ |= 0x00000002;
        explainMode_ = value.getNumber();
        onChanged();
        return this;
      }
      /**
       * <pre>
       * (Required) For analyzePlan rpc calls, configure the mode to explain plan in strings.
       * </pre>
       *
       * <code>.spark.connect.AnalyzePlanRequest.Explain.ExplainMode explain_mode = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearExplainMode() {
        bitField0_ = (bitField0_ & ~0x00000002);
        explainMode_ = 0;
        onChanged();
        return this;
      }
      @Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:spark.connect.AnalyzePlanRequest.Explain)
    }

    // @@protoc_insertion_point(class_scope:spark.connect.AnalyzePlanRequest.Explain)
    private static final org.apache.spark.connect.proto.AnalyzePlanRequest.Explain DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.connect.proto.AnalyzePlanRequest.Explain();
    }

    public static org.apache.spark.connect.proto.AnalyzePlanRequest.Explain getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<Explain>
        PARSER = new com.google.protobuf.AbstractParser<Explain>() {
      @Override
      public Explain parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        Builder builder = newBuilder();
        try {
          builder.mergeFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.setUnfinishedMessage(builder.buildPartial());
        } catch (com.google.protobuf.UninitializedMessageException e) {
          throw e.asInvalidProtocolBufferException().setUnfinishedMessage(builder.buildPartial());
        } catch (java.io.IOException e) {
          throw new com.google.protobuf.InvalidProtocolBufferException(e)
              .setUnfinishedMessage(builder.buildPartial());
        }
        return builder.buildPartial();
      }
    };

    public static com.google.protobuf.Parser<Explain> parser() {
      return PARSER;
    }

    @Override
    public com.google.protobuf.Parser<Explain> getParserForType() {
      return PARSER;
    }

    @Override
    public org.apache.spark.connect.proto.AnalyzePlanRequest.Explain getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface TreeStringOrBuilder extends
      // @@protoc_insertion_point(interface_extends:spark.connect.AnalyzePlanRequest.TreeString)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <pre>
     * (Required) The logical plan to be analyzed.
     * </pre>
     *
     * <code>.spark.connect.Plan plan = 1;</code>
     * @return Whether the plan field is set.
     */
    boolean hasPlan();
    /**
     * <pre>
     * (Required) The logical plan to be analyzed.
     * </pre>
     *
     * <code>.spark.connect.Plan plan = 1;</code>
     * @return The plan.
     */
    org.apache.spark.connect.proto.Plan getPlan();
    /**
     * <pre>
     * (Required) The logical plan to be analyzed.
     * </pre>
     *
     * <code>.spark.connect.Plan plan = 1;</code>
     */
    org.apache.spark.connect.proto.PlanOrBuilder getPlanOrBuilder();

    /**
     * <pre>
     * (Optional) Max level of the schema.
     * </pre>
     *
     * <code>optional int32 level = 2;</code>
     * @return Whether the level field is set.
     */
    boolean hasLevel();
    /**
     * <pre>
     * (Optional) Max level of the schema.
     * </pre>
     *
     * <code>optional int32 level = 2;</code>
     * @return The level.
     */
    int getLevel();
  }
  /**
   * Protobuf type {@code spark.connect.AnalyzePlanRequest.TreeString}
   */
  public static final class TreeString extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:spark.connect.AnalyzePlanRequest.TreeString)
      TreeStringOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use TreeString.newBuilder() to construct.
    private TreeString(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private TreeString() {
    }

    @Override
    @SuppressWarnings({"unused"})
    protected Object newInstance(
        UnusedPrivateParameter unused) {
      return new TreeString();
    }

    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.connect.proto.Base.internal_static_spark_connect_AnalyzePlanRequest_TreeString_descriptor;
    }

    @Override
    protected FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.connect.proto.Base.internal_static_spark_connect_AnalyzePlanRequest_TreeString_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.connect.proto.AnalyzePlanRequest.TreeString.class, org.apache.spark.connect.proto.AnalyzePlanRequest.TreeString.Builder.class);
    }

    private int bitField0_;
    public static final int PLAN_FIELD_NUMBER = 1;
    private org.apache.spark.connect.proto.Plan plan_;
    /**
     * <pre>
     * (Required) The logical plan to be analyzed.
     * </pre>
     *
     * <code>.spark.connect.Plan plan = 1;</code>
     * @return Whether the plan field is set.
     */
    @Override
    public boolean hasPlan() {
      return plan_ != null;
    }
    /**
     * <pre>
     * (Required) The logical plan to be analyzed.
     * </pre>
     *
     * <code>.spark.connect.Plan plan = 1;</code>
     * @return The plan.
     */
    @Override
    public org.apache.spark.connect.proto.Plan getPlan() {
      return plan_ == null ? org.apache.spark.connect.proto.Plan.getDefaultInstance() : plan_;
    }
    /**
     * <pre>
     * (Required) The logical plan to be analyzed.
     * </pre>
     *
     * <code>.spark.connect.Plan plan = 1;</code>
     */
    @Override
    public org.apache.spark.connect.proto.PlanOrBuilder getPlanOrBuilder() {
      return plan_ == null ? org.apache.spark.connect.proto.Plan.getDefaultInstance() : plan_;
    }

    public static final int LEVEL_FIELD_NUMBER = 2;
    private int level_ = 0;
    /**
     * <pre>
     * (Optional) Max level of the schema.
     * </pre>
     *
     * <code>optional int32 level = 2;</code>
     * @return Whether the level field is set.
     */
    @Override
    public boolean hasLevel() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <pre>
     * (Optional) Max level of the schema.
     * </pre>
     *
     * <code>optional int32 level = 2;</code>
     * @return The level.
     */
    @Override
    public int getLevel() {
      return level_;
    }

    private byte memoizedIsInitialized = -1;
    @Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (plan_ != null) {
        output.writeMessage(1, getPlan());
      }
      if (((bitField0_ & 0x00000001) != 0)) {
        output.writeInt32(2, level_);
      }
      getUnknownFields().writeTo(output);
    }

    @Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (plan_ != null) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, getPlan());
      }
      if (((bitField0_ & 0x00000001) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt32Size(2, level_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @Override
    public boolean equals(final Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.connect.proto.AnalyzePlanRequest.TreeString)) {
        return super.equals(obj);
      }
      org.apache.spark.connect.proto.AnalyzePlanRequest.TreeString other = (org.apache.spark.connect.proto.AnalyzePlanRequest.TreeString) obj;

      if (hasPlan() != other.hasPlan()) return false;
      if (hasPlan()) {
        if (!getPlan()
            .equals(other.getPlan())) return false;
      }
      if (hasLevel() != other.hasLevel()) return false;
      if (hasLevel()) {
        if (getLevel()
            != other.getLevel()) return false;
      }
      if (!getUnknownFields().equals(other.getUnknownFields())) return false;
      return true;
    }

    @Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasPlan()) {
        hash = (37 * hash) + PLAN_FIELD_NUMBER;
        hash = (53 * hash) + getPlan().hashCode();
      }
      if (hasLevel()) {
        hash = (37 * hash) + LEVEL_FIELD_NUMBER;
        hash = (53 * hash) + getLevel();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.connect.proto.AnalyzePlanRequest.TreeString parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.TreeString parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.TreeString parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.TreeString parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.TreeString parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.TreeString parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.TreeString parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.TreeString parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    public static org.apache.spark.connect.proto.AnalyzePlanRequest.TreeString parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }

    public static org.apache.spark.connect.proto.AnalyzePlanRequest.TreeString parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.TreeString parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.TreeString parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.connect.proto.AnalyzePlanRequest.TreeString prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @Override
    protected Builder newBuilderForType(
        BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code spark.connect.AnalyzePlanRequest.TreeString}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:spark.connect.AnalyzePlanRequest.TreeString)
        org.apache.spark.connect.proto.AnalyzePlanRequest.TreeStringOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.connect.proto.Base.internal_static_spark_connect_AnalyzePlanRequest_TreeString_descriptor;
      }

      @Override
      protected FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.connect.proto.Base.internal_static_spark_connect_AnalyzePlanRequest_TreeString_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.connect.proto.AnalyzePlanRequest.TreeString.class, org.apache.spark.connect.proto.AnalyzePlanRequest.TreeString.Builder.class);
      }

      // Construct using org.apache.spark.connect.proto.AnalyzePlanRequest.TreeString.newBuilder()
      private Builder() {

      }

      private Builder(
          BuilderParent parent) {
        super(parent);

      }
      @Override
      public Builder clear() {
        super.clear();
        bitField0_ = 0;
        plan_ = null;
        if (planBuilder_ != null) {
          planBuilder_.dispose();
          planBuilder_ = null;
        }
        level_ = 0;
        return this;
      }

      @Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.connect.proto.Base.internal_static_spark_connect_AnalyzePlanRequest_TreeString_descriptor;
      }

      @Override
      public org.apache.spark.connect.proto.AnalyzePlanRequest.TreeString getDefaultInstanceForType() {
        return org.apache.spark.connect.proto.AnalyzePlanRequest.TreeString.getDefaultInstance();
      }

      @Override
      public org.apache.spark.connect.proto.AnalyzePlanRequest.TreeString build() {
        org.apache.spark.connect.proto.AnalyzePlanRequest.TreeString result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @Override
      public org.apache.spark.connect.proto.AnalyzePlanRequest.TreeString buildPartial() {
        org.apache.spark.connect.proto.AnalyzePlanRequest.TreeString result = new org.apache.spark.connect.proto.AnalyzePlanRequest.TreeString(this);
        if (bitField0_ != 0) { buildPartial0(result); }
        onBuilt();
        return result;
      }

      private void buildPartial0(org.apache.spark.connect.proto.AnalyzePlanRequest.TreeString result) {
        int from_bitField0_ = bitField0_;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          result.plan_ = planBuilder_ == null
              ? plan_
              : planBuilder_.build();
        }
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000002) != 0)) {
          result.level_ = level_;
          to_bitField0_ |= 0x00000001;
        }
        result.bitField0_ |= to_bitField0_;
      }

      @Override
      public Builder clone() {
        return super.clone();
      }
      @Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          Object value) {
        return super.setField(field, value);
      }
      @Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          Object value) {
        return super.addRepeatedField(field, value);
      }
      @Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.connect.proto.AnalyzePlanRequest.TreeString) {
          return mergeFrom((org.apache.spark.connect.proto.AnalyzePlanRequest.TreeString)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.connect.proto.AnalyzePlanRequest.TreeString other) {
        if (other == org.apache.spark.connect.proto.AnalyzePlanRequest.TreeString.getDefaultInstance()) return this;
        if (other.hasPlan()) {
          mergePlan(other.getPlan());
        }
        if (other.hasLevel()) {
          setLevel(other.getLevel());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        onChanged();
        return this;
      }

      @Override
      public final boolean isInitialized() {
        return true;
      }

      @Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        if (extensionRegistry == null) {
          throw new NullPointerException();
        }
        try {
          boolean done = false;
          while (!done) {
            int tag = input.readTag();
            switch (tag) {
              case 0:
                done = true;
                break;
              case 10: {
                input.readMessage(
                    getPlanFieldBuilder().getBuilder(),
                    extensionRegistry);
                bitField0_ |= 0x00000001;
                break;
              } // case 10
              case 16: {
                level_ = input.readInt32();
                bitField0_ |= 0x00000002;
                break;
              } // case 16
              default: {
                if (!super.parseUnknownField(input, extensionRegistry, tag)) {
                  done = true; // was an endgroup tag
                }
                break;
              } // default:
            } // switch (tag)
          } // while (!done)
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.unwrapIOException();
        } finally {
          onChanged();
        } // finally
        return this;
      }
      private int bitField0_;

      private org.apache.spark.connect.proto.Plan plan_;
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.connect.proto.Plan, org.apache.spark.connect.proto.Plan.Builder, org.apache.spark.connect.proto.PlanOrBuilder> planBuilder_;
      /**
       * <pre>
       * (Required) The logical plan to be analyzed.
       * </pre>
       *
       * <code>.spark.connect.Plan plan = 1;</code>
       * @return Whether the plan field is set.
       */
      public boolean hasPlan() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <pre>
       * (Required) The logical plan to be analyzed.
       * </pre>
       *
       * <code>.spark.connect.Plan plan = 1;</code>
       * @return The plan.
       */
      public org.apache.spark.connect.proto.Plan getPlan() {
        if (planBuilder_ == null) {
          return plan_ == null ? org.apache.spark.connect.proto.Plan.getDefaultInstance() : plan_;
        } else {
          return planBuilder_.getMessage();
        }
      }
      /**
       * <pre>
       * (Required) The logical plan to be analyzed.
       * </pre>
       *
       * <code>.spark.connect.Plan plan = 1;</code>
       */
      public Builder setPlan(org.apache.spark.connect.proto.Plan value) {
        if (planBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          plan_ = value;
        } else {
          planBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * (Required) The logical plan to be analyzed.
       * </pre>
       *
       * <code>.spark.connect.Plan plan = 1;</code>
       */
      public Builder setPlan(
          org.apache.spark.connect.proto.Plan.Builder builderForValue) {
        if (planBuilder_ == null) {
          plan_ = builderForValue.build();
        } else {
          planBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * (Required) The logical plan to be analyzed.
       * </pre>
       *
       * <code>.spark.connect.Plan plan = 1;</code>
       */
      public Builder mergePlan(org.apache.spark.connect.proto.Plan value) {
        if (planBuilder_ == null) {
          if (((bitField0_ & 0x00000001) != 0) &&
            plan_ != null &&
            plan_ != org.apache.spark.connect.proto.Plan.getDefaultInstance()) {
            getPlanBuilder().mergeFrom(value);
          } else {
            plan_ = value;
          }
        } else {
          planBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * (Required) The logical plan to be analyzed.
       * </pre>
       *
       * <code>.spark.connect.Plan plan = 1;</code>
       */
      public Builder clearPlan() {
        bitField0_ = (bitField0_ & ~0x00000001);
        plan_ = null;
        if (planBuilder_ != null) {
          planBuilder_.dispose();
          planBuilder_ = null;
        }
        onChanged();
        return this;
      }
      /**
       * <pre>
       * (Required) The logical plan to be analyzed.
       * </pre>
       *
       * <code>.spark.connect.Plan plan = 1;</code>
       */
      public org.apache.spark.connect.proto.Plan.Builder getPlanBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getPlanFieldBuilder().getBuilder();
      }
      /**
       * <pre>
       * (Required) The logical plan to be analyzed.
       * </pre>
       *
       * <code>.spark.connect.Plan plan = 1;</code>
       */
      public org.apache.spark.connect.proto.PlanOrBuilder getPlanOrBuilder() {
        if (planBuilder_ != null) {
          return planBuilder_.getMessageOrBuilder();
        } else {
          return plan_ == null ?
              org.apache.spark.connect.proto.Plan.getDefaultInstance() : plan_;
        }
      }
      /**
       * <pre>
       * (Required) The logical plan to be analyzed.
       * </pre>
       *
       * <code>.spark.connect.Plan plan = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.connect.proto.Plan, org.apache.spark.connect.proto.Plan.Builder, org.apache.spark.connect.proto.PlanOrBuilder> 
          getPlanFieldBuilder() {
        if (planBuilder_ == null) {
          planBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.apache.spark.connect.proto.Plan, org.apache.spark.connect.proto.Plan.Builder, org.apache.spark.connect.proto.PlanOrBuilder>(
                  getPlan(),
                  getParentForChildren(),
                  isClean());
          plan_ = null;
        }
        return planBuilder_;
      }

      private int level_ ;
      /**
       * <pre>
       * (Optional) Max level of the schema.
       * </pre>
       *
       * <code>optional int32 level = 2;</code>
       * @return Whether the level field is set.
       */
      @Override
      public boolean hasLevel() {
        return ((bitField0_ & 0x00000002) != 0);
      }
      /**
       * <pre>
       * (Optional) Max level of the schema.
       * </pre>
       *
       * <code>optional int32 level = 2;</code>
       * @return The level.
       */
      @Override
      public int getLevel() {
        return level_;
      }
      /**
       * <pre>
       * (Optional) Max level of the schema.
       * </pre>
       *
       * <code>optional int32 level = 2;</code>
       * @param value The level to set.
       * @return This builder for chaining.
       */
      public Builder setLevel(int value) {

        level_ = value;
        bitField0_ |= 0x00000002;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * (Optional) Max level of the schema.
       * </pre>
       *
       * <code>optional int32 level = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearLevel() {
        bitField0_ = (bitField0_ & ~0x00000002);
        level_ = 0;
        onChanged();
        return this;
      }
      @Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:spark.connect.AnalyzePlanRequest.TreeString)
    }

    // @@protoc_insertion_point(class_scope:spark.connect.AnalyzePlanRequest.TreeString)
    private static final org.apache.spark.connect.proto.AnalyzePlanRequest.TreeString DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.connect.proto.AnalyzePlanRequest.TreeString();
    }

    public static org.apache.spark.connect.proto.AnalyzePlanRequest.TreeString getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<TreeString>
        PARSER = new com.google.protobuf.AbstractParser<TreeString>() {
      @Override
      public TreeString parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        Builder builder = newBuilder();
        try {
          builder.mergeFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.setUnfinishedMessage(builder.buildPartial());
        } catch (com.google.protobuf.UninitializedMessageException e) {
          throw e.asInvalidProtocolBufferException().setUnfinishedMessage(builder.buildPartial());
        } catch (java.io.IOException e) {
          throw new com.google.protobuf.InvalidProtocolBufferException(e)
              .setUnfinishedMessage(builder.buildPartial());
        }
        return builder.buildPartial();
      }
    };

    public static com.google.protobuf.Parser<TreeString> parser() {
      return PARSER;
    }

    @Override
    public com.google.protobuf.Parser<TreeString> getParserForType() {
      return PARSER;
    }

    @Override
    public org.apache.spark.connect.proto.AnalyzePlanRequest.TreeString getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface IsLocalOrBuilder extends
      // @@protoc_insertion_point(interface_extends:spark.connect.AnalyzePlanRequest.IsLocal)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <pre>
     * (Required) The logical plan to be analyzed.
     * </pre>
     *
     * <code>.spark.connect.Plan plan = 1;</code>
     * @return Whether the plan field is set.
     */
    boolean hasPlan();
    /**
     * <pre>
     * (Required) The logical plan to be analyzed.
     * </pre>
     *
     * <code>.spark.connect.Plan plan = 1;</code>
     * @return The plan.
     */
    org.apache.spark.connect.proto.Plan getPlan();
    /**
     * <pre>
     * (Required) The logical plan to be analyzed.
     * </pre>
     *
     * <code>.spark.connect.Plan plan = 1;</code>
     */
    org.apache.spark.connect.proto.PlanOrBuilder getPlanOrBuilder();
  }
  /**
   * Protobuf type {@code spark.connect.AnalyzePlanRequest.IsLocal}
   */
  public static final class IsLocal extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:spark.connect.AnalyzePlanRequest.IsLocal)
      IsLocalOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use IsLocal.newBuilder() to construct.
    private IsLocal(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private IsLocal() {
    }

    @Override
    @SuppressWarnings({"unused"})
    protected Object newInstance(
        UnusedPrivateParameter unused) {
      return new IsLocal();
    }

    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.connect.proto.Base.internal_static_spark_connect_AnalyzePlanRequest_IsLocal_descriptor;
    }

    @Override
    protected FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.connect.proto.Base.internal_static_spark_connect_AnalyzePlanRequest_IsLocal_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.connect.proto.AnalyzePlanRequest.IsLocal.class, org.apache.spark.connect.proto.AnalyzePlanRequest.IsLocal.Builder.class);
    }

    public static final int PLAN_FIELD_NUMBER = 1;
    private org.apache.spark.connect.proto.Plan plan_;
    /**
     * <pre>
     * (Required) The logical plan to be analyzed.
     * </pre>
     *
     * <code>.spark.connect.Plan plan = 1;</code>
     * @return Whether the plan field is set.
     */
    @Override
    public boolean hasPlan() {
      return plan_ != null;
    }
    /**
     * <pre>
     * (Required) The logical plan to be analyzed.
     * </pre>
     *
     * <code>.spark.connect.Plan plan = 1;</code>
     * @return The plan.
     */
    @Override
    public org.apache.spark.connect.proto.Plan getPlan() {
      return plan_ == null ? org.apache.spark.connect.proto.Plan.getDefaultInstance() : plan_;
    }
    /**
     * <pre>
     * (Required) The logical plan to be analyzed.
     * </pre>
     *
     * <code>.spark.connect.Plan plan = 1;</code>
     */
    @Override
    public org.apache.spark.connect.proto.PlanOrBuilder getPlanOrBuilder() {
      return plan_ == null ? org.apache.spark.connect.proto.Plan.getDefaultInstance() : plan_;
    }

    private byte memoizedIsInitialized = -1;
    @Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (plan_ != null) {
        output.writeMessage(1, getPlan());
      }
      getUnknownFields().writeTo(output);
    }

    @Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (plan_ != null) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, getPlan());
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @Override
    public boolean equals(final Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.connect.proto.AnalyzePlanRequest.IsLocal)) {
        return super.equals(obj);
      }
      org.apache.spark.connect.proto.AnalyzePlanRequest.IsLocal other = (org.apache.spark.connect.proto.AnalyzePlanRequest.IsLocal) obj;

      if (hasPlan() != other.hasPlan()) return false;
      if (hasPlan()) {
        if (!getPlan()
            .equals(other.getPlan())) return false;
      }
      if (!getUnknownFields().equals(other.getUnknownFields())) return false;
      return true;
    }

    @Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasPlan()) {
        hash = (37 * hash) + PLAN_FIELD_NUMBER;
        hash = (53 * hash) + getPlan().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.connect.proto.AnalyzePlanRequest.IsLocal parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.IsLocal parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.IsLocal parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.IsLocal parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.IsLocal parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.IsLocal parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.IsLocal parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.IsLocal parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    public static org.apache.spark.connect.proto.AnalyzePlanRequest.IsLocal parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }

    public static org.apache.spark.connect.proto.AnalyzePlanRequest.IsLocal parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.IsLocal parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.IsLocal parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.connect.proto.AnalyzePlanRequest.IsLocal prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @Override
    protected Builder newBuilderForType(
        BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code spark.connect.AnalyzePlanRequest.IsLocal}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:spark.connect.AnalyzePlanRequest.IsLocal)
        org.apache.spark.connect.proto.AnalyzePlanRequest.IsLocalOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.connect.proto.Base.internal_static_spark_connect_AnalyzePlanRequest_IsLocal_descriptor;
      }

      @Override
      protected FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.connect.proto.Base.internal_static_spark_connect_AnalyzePlanRequest_IsLocal_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.connect.proto.AnalyzePlanRequest.IsLocal.class, org.apache.spark.connect.proto.AnalyzePlanRequest.IsLocal.Builder.class);
      }

      // Construct using org.apache.spark.connect.proto.AnalyzePlanRequest.IsLocal.newBuilder()
      private Builder() {

      }

      private Builder(
          BuilderParent parent) {
        super(parent);

      }
      @Override
      public Builder clear() {
        super.clear();
        bitField0_ = 0;
        plan_ = null;
        if (planBuilder_ != null) {
          planBuilder_.dispose();
          planBuilder_ = null;
        }
        return this;
      }

      @Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.connect.proto.Base.internal_static_spark_connect_AnalyzePlanRequest_IsLocal_descriptor;
      }

      @Override
      public org.apache.spark.connect.proto.AnalyzePlanRequest.IsLocal getDefaultInstanceForType() {
        return org.apache.spark.connect.proto.AnalyzePlanRequest.IsLocal.getDefaultInstance();
      }

      @Override
      public org.apache.spark.connect.proto.AnalyzePlanRequest.IsLocal build() {
        org.apache.spark.connect.proto.AnalyzePlanRequest.IsLocal result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @Override
      public org.apache.spark.connect.proto.AnalyzePlanRequest.IsLocal buildPartial() {
        org.apache.spark.connect.proto.AnalyzePlanRequest.IsLocal result = new org.apache.spark.connect.proto.AnalyzePlanRequest.IsLocal(this);
        if (bitField0_ != 0) { buildPartial0(result); }
        onBuilt();
        return result;
      }

      private void buildPartial0(org.apache.spark.connect.proto.AnalyzePlanRequest.IsLocal result) {
        int from_bitField0_ = bitField0_;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          result.plan_ = planBuilder_ == null
              ? plan_
              : planBuilder_.build();
        }
      }

      @Override
      public Builder clone() {
        return super.clone();
      }
      @Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          Object value) {
        return super.setField(field, value);
      }
      @Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          Object value) {
        return super.addRepeatedField(field, value);
      }
      @Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.connect.proto.AnalyzePlanRequest.IsLocal) {
          return mergeFrom((org.apache.spark.connect.proto.AnalyzePlanRequest.IsLocal)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.connect.proto.AnalyzePlanRequest.IsLocal other) {
        if (other == org.apache.spark.connect.proto.AnalyzePlanRequest.IsLocal.getDefaultInstance()) return this;
        if (other.hasPlan()) {
          mergePlan(other.getPlan());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        onChanged();
        return this;
      }

      @Override
      public final boolean isInitialized() {
        return true;
      }

      @Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        if (extensionRegistry == null) {
          throw new NullPointerException();
        }
        try {
          boolean done = false;
          while (!done) {
            int tag = input.readTag();
            switch (tag) {
              case 0:
                done = true;
                break;
              case 10: {
                input.readMessage(
                    getPlanFieldBuilder().getBuilder(),
                    extensionRegistry);
                bitField0_ |= 0x00000001;
                break;
              } // case 10
              default: {
                if (!super.parseUnknownField(input, extensionRegistry, tag)) {
                  done = true; // was an endgroup tag
                }
                break;
              } // default:
            } // switch (tag)
          } // while (!done)
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.unwrapIOException();
        } finally {
          onChanged();
        } // finally
        return this;
      }
      private int bitField0_;

      private org.apache.spark.connect.proto.Plan plan_;
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.connect.proto.Plan, org.apache.spark.connect.proto.Plan.Builder, org.apache.spark.connect.proto.PlanOrBuilder> planBuilder_;
      /**
       * <pre>
       * (Required) The logical plan to be analyzed.
       * </pre>
       *
       * <code>.spark.connect.Plan plan = 1;</code>
       * @return Whether the plan field is set.
       */
      public boolean hasPlan() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <pre>
       * (Required) The logical plan to be analyzed.
       * </pre>
       *
       * <code>.spark.connect.Plan plan = 1;</code>
       * @return The plan.
       */
      public org.apache.spark.connect.proto.Plan getPlan() {
        if (planBuilder_ == null) {
          return plan_ == null ? org.apache.spark.connect.proto.Plan.getDefaultInstance() : plan_;
        } else {
          return planBuilder_.getMessage();
        }
      }
      /**
       * <pre>
       * (Required) The logical plan to be analyzed.
       * </pre>
       *
       * <code>.spark.connect.Plan plan = 1;</code>
       */
      public Builder setPlan(org.apache.spark.connect.proto.Plan value) {
        if (planBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          plan_ = value;
        } else {
          planBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * (Required) The logical plan to be analyzed.
       * </pre>
       *
       * <code>.spark.connect.Plan plan = 1;</code>
       */
      public Builder setPlan(
          org.apache.spark.connect.proto.Plan.Builder builderForValue) {
        if (planBuilder_ == null) {
          plan_ = builderForValue.build();
        } else {
          planBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * (Required) The logical plan to be analyzed.
       * </pre>
       *
       * <code>.spark.connect.Plan plan = 1;</code>
       */
      public Builder mergePlan(org.apache.spark.connect.proto.Plan value) {
        if (planBuilder_ == null) {
          if (((bitField0_ & 0x00000001) != 0) &&
            plan_ != null &&
            plan_ != org.apache.spark.connect.proto.Plan.getDefaultInstance()) {
            getPlanBuilder().mergeFrom(value);
          } else {
            plan_ = value;
          }
        } else {
          planBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * (Required) The logical plan to be analyzed.
       * </pre>
       *
       * <code>.spark.connect.Plan plan = 1;</code>
       */
      public Builder clearPlan() {
        bitField0_ = (bitField0_ & ~0x00000001);
        plan_ = null;
        if (planBuilder_ != null) {
          planBuilder_.dispose();
          planBuilder_ = null;
        }
        onChanged();
        return this;
      }
      /**
       * <pre>
       * (Required) The logical plan to be analyzed.
       * </pre>
       *
       * <code>.spark.connect.Plan plan = 1;</code>
       */
      public org.apache.spark.connect.proto.Plan.Builder getPlanBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getPlanFieldBuilder().getBuilder();
      }
      /**
       * <pre>
       * (Required) The logical plan to be analyzed.
       * </pre>
       *
       * <code>.spark.connect.Plan plan = 1;</code>
       */
      public org.apache.spark.connect.proto.PlanOrBuilder getPlanOrBuilder() {
        if (planBuilder_ != null) {
          return planBuilder_.getMessageOrBuilder();
        } else {
          return plan_ == null ?
              org.apache.spark.connect.proto.Plan.getDefaultInstance() : plan_;
        }
      }
      /**
       * <pre>
       * (Required) The logical plan to be analyzed.
       * </pre>
       *
       * <code>.spark.connect.Plan plan = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.connect.proto.Plan, org.apache.spark.connect.proto.Plan.Builder, org.apache.spark.connect.proto.PlanOrBuilder> 
          getPlanFieldBuilder() {
        if (planBuilder_ == null) {
          planBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.apache.spark.connect.proto.Plan, org.apache.spark.connect.proto.Plan.Builder, org.apache.spark.connect.proto.PlanOrBuilder>(
                  getPlan(),
                  getParentForChildren(),
                  isClean());
          plan_ = null;
        }
        return planBuilder_;
      }
      @Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:spark.connect.AnalyzePlanRequest.IsLocal)
    }

    // @@protoc_insertion_point(class_scope:spark.connect.AnalyzePlanRequest.IsLocal)
    private static final org.apache.spark.connect.proto.AnalyzePlanRequest.IsLocal DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.connect.proto.AnalyzePlanRequest.IsLocal();
    }

    public static org.apache.spark.connect.proto.AnalyzePlanRequest.IsLocal getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<IsLocal>
        PARSER = new com.google.protobuf.AbstractParser<IsLocal>() {
      @Override
      public IsLocal parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        Builder builder = newBuilder();
        try {
          builder.mergeFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.setUnfinishedMessage(builder.buildPartial());
        } catch (com.google.protobuf.UninitializedMessageException e) {
          throw e.asInvalidProtocolBufferException().setUnfinishedMessage(builder.buildPartial());
        } catch (java.io.IOException e) {
          throw new com.google.protobuf.InvalidProtocolBufferException(e)
              .setUnfinishedMessage(builder.buildPartial());
        }
        return builder.buildPartial();
      }
    };

    public static com.google.protobuf.Parser<IsLocal> parser() {
      return PARSER;
    }

    @Override
    public com.google.protobuf.Parser<IsLocal> getParserForType() {
      return PARSER;
    }

    @Override
    public org.apache.spark.connect.proto.AnalyzePlanRequest.IsLocal getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface IsStreamingOrBuilder extends
      // @@protoc_insertion_point(interface_extends:spark.connect.AnalyzePlanRequest.IsStreaming)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <pre>
     * (Required) The logical plan to be analyzed.
     * </pre>
     *
     * <code>.spark.connect.Plan plan = 1;</code>
     * @return Whether the plan field is set.
     */
    boolean hasPlan();
    /**
     * <pre>
     * (Required) The logical plan to be analyzed.
     * </pre>
     *
     * <code>.spark.connect.Plan plan = 1;</code>
     * @return The plan.
     */
    org.apache.spark.connect.proto.Plan getPlan();
    /**
     * <pre>
     * (Required) The logical plan to be analyzed.
     * </pre>
     *
     * <code>.spark.connect.Plan plan = 1;</code>
     */
    org.apache.spark.connect.proto.PlanOrBuilder getPlanOrBuilder();
  }
  /**
   * Protobuf type {@code spark.connect.AnalyzePlanRequest.IsStreaming}
   */
  public static final class IsStreaming extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:spark.connect.AnalyzePlanRequest.IsStreaming)
      IsStreamingOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use IsStreaming.newBuilder() to construct.
    private IsStreaming(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private IsStreaming() {
    }

    @Override
    @SuppressWarnings({"unused"})
    protected Object newInstance(
        UnusedPrivateParameter unused) {
      return new IsStreaming();
    }

    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.connect.proto.Base.internal_static_spark_connect_AnalyzePlanRequest_IsStreaming_descriptor;
    }

    @Override
    protected FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.connect.proto.Base.internal_static_spark_connect_AnalyzePlanRequest_IsStreaming_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.connect.proto.AnalyzePlanRequest.IsStreaming.class, org.apache.spark.connect.proto.AnalyzePlanRequest.IsStreaming.Builder.class);
    }

    public static final int PLAN_FIELD_NUMBER = 1;
    private org.apache.spark.connect.proto.Plan plan_;
    /**
     * <pre>
     * (Required) The logical plan to be analyzed.
     * </pre>
     *
     * <code>.spark.connect.Plan plan = 1;</code>
     * @return Whether the plan field is set.
     */
    @Override
    public boolean hasPlan() {
      return plan_ != null;
    }
    /**
     * <pre>
     * (Required) The logical plan to be analyzed.
     * </pre>
     *
     * <code>.spark.connect.Plan plan = 1;</code>
     * @return The plan.
     */
    @Override
    public org.apache.spark.connect.proto.Plan getPlan() {
      return plan_ == null ? org.apache.spark.connect.proto.Plan.getDefaultInstance() : plan_;
    }
    /**
     * <pre>
     * (Required) The logical plan to be analyzed.
     * </pre>
     *
     * <code>.spark.connect.Plan plan = 1;</code>
     */
    @Override
    public org.apache.spark.connect.proto.PlanOrBuilder getPlanOrBuilder() {
      return plan_ == null ? org.apache.spark.connect.proto.Plan.getDefaultInstance() : plan_;
    }

    private byte memoizedIsInitialized = -1;
    @Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (plan_ != null) {
        output.writeMessage(1, getPlan());
      }
      getUnknownFields().writeTo(output);
    }

    @Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (plan_ != null) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, getPlan());
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @Override
    public boolean equals(final Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.connect.proto.AnalyzePlanRequest.IsStreaming)) {
        return super.equals(obj);
      }
      org.apache.spark.connect.proto.AnalyzePlanRequest.IsStreaming other = (org.apache.spark.connect.proto.AnalyzePlanRequest.IsStreaming) obj;

      if (hasPlan() != other.hasPlan()) return false;
      if (hasPlan()) {
        if (!getPlan()
            .equals(other.getPlan())) return false;
      }
      if (!getUnknownFields().equals(other.getUnknownFields())) return false;
      return true;
    }

    @Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasPlan()) {
        hash = (37 * hash) + PLAN_FIELD_NUMBER;
        hash = (53 * hash) + getPlan().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.connect.proto.AnalyzePlanRequest.IsStreaming parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.IsStreaming parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.IsStreaming parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.IsStreaming parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.IsStreaming parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.IsStreaming parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.IsStreaming parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.IsStreaming parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    public static org.apache.spark.connect.proto.AnalyzePlanRequest.IsStreaming parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }

    public static org.apache.spark.connect.proto.AnalyzePlanRequest.IsStreaming parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.IsStreaming parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.IsStreaming parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.connect.proto.AnalyzePlanRequest.IsStreaming prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @Override
    protected Builder newBuilderForType(
        BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code spark.connect.AnalyzePlanRequest.IsStreaming}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:spark.connect.AnalyzePlanRequest.IsStreaming)
        org.apache.spark.connect.proto.AnalyzePlanRequest.IsStreamingOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.connect.proto.Base.internal_static_spark_connect_AnalyzePlanRequest_IsStreaming_descriptor;
      }

      @Override
      protected FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.connect.proto.Base.internal_static_spark_connect_AnalyzePlanRequest_IsStreaming_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.connect.proto.AnalyzePlanRequest.IsStreaming.class, org.apache.spark.connect.proto.AnalyzePlanRequest.IsStreaming.Builder.class);
      }

      // Construct using org.apache.spark.connect.proto.AnalyzePlanRequest.IsStreaming.newBuilder()
      private Builder() {

      }

      private Builder(
          BuilderParent parent) {
        super(parent);

      }
      @Override
      public Builder clear() {
        super.clear();
        bitField0_ = 0;
        plan_ = null;
        if (planBuilder_ != null) {
          planBuilder_.dispose();
          planBuilder_ = null;
        }
        return this;
      }

      @Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.connect.proto.Base.internal_static_spark_connect_AnalyzePlanRequest_IsStreaming_descriptor;
      }

      @Override
      public org.apache.spark.connect.proto.AnalyzePlanRequest.IsStreaming getDefaultInstanceForType() {
        return org.apache.spark.connect.proto.AnalyzePlanRequest.IsStreaming.getDefaultInstance();
      }

      @Override
      public org.apache.spark.connect.proto.AnalyzePlanRequest.IsStreaming build() {
        org.apache.spark.connect.proto.AnalyzePlanRequest.IsStreaming result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @Override
      public org.apache.spark.connect.proto.AnalyzePlanRequest.IsStreaming buildPartial() {
        org.apache.spark.connect.proto.AnalyzePlanRequest.IsStreaming result = new org.apache.spark.connect.proto.AnalyzePlanRequest.IsStreaming(this);
        if (bitField0_ != 0) { buildPartial0(result); }
        onBuilt();
        return result;
      }

      private void buildPartial0(org.apache.spark.connect.proto.AnalyzePlanRequest.IsStreaming result) {
        int from_bitField0_ = bitField0_;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          result.plan_ = planBuilder_ == null
              ? plan_
              : planBuilder_.build();
        }
      }

      @Override
      public Builder clone() {
        return super.clone();
      }
      @Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          Object value) {
        return super.setField(field, value);
      }
      @Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          Object value) {
        return super.addRepeatedField(field, value);
      }
      @Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.connect.proto.AnalyzePlanRequest.IsStreaming) {
          return mergeFrom((org.apache.spark.connect.proto.AnalyzePlanRequest.IsStreaming)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.connect.proto.AnalyzePlanRequest.IsStreaming other) {
        if (other == org.apache.spark.connect.proto.AnalyzePlanRequest.IsStreaming.getDefaultInstance()) return this;
        if (other.hasPlan()) {
          mergePlan(other.getPlan());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        onChanged();
        return this;
      }

      @Override
      public final boolean isInitialized() {
        return true;
      }

      @Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        if (extensionRegistry == null) {
          throw new NullPointerException();
        }
        try {
          boolean done = false;
          while (!done) {
            int tag = input.readTag();
            switch (tag) {
              case 0:
                done = true;
                break;
              case 10: {
                input.readMessage(
                    getPlanFieldBuilder().getBuilder(),
                    extensionRegistry);
                bitField0_ |= 0x00000001;
                break;
              } // case 10
              default: {
                if (!super.parseUnknownField(input, extensionRegistry, tag)) {
                  done = true; // was an endgroup tag
                }
                break;
              } // default:
            } // switch (tag)
          } // while (!done)
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.unwrapIOException();
        } finally {
          onChanged();
        } // finally
        return this;
      }
      private int bitField0_;

      private org.apache.spark.connect.proto.Plan plan_;
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.connect.proto.Plan, org.apache.spark.connect.proto.Plan.Builder, org.apache.spark.connect.proto.PlanOrBuilder> planBuilder_;
      /**
       * <pre>
       * (Required) The logical plan to be analyzed.
       * </pre>
       *
       * <code>.spark.connect.Plan plan = 1;</code>
       * @return Whether the plan field is set.
       */
      public boolean hasPlan() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <pre>
       * (Required) The logical plan to be analyzed.
       * </pre>
       *
       * <code>.spark.connect.Plan plan = 1;</code>
       * @return The plan.
       */
      public org.apache.spark.connect.proto.Plan getPlan() {
        if (planBuilder_ == null) {
          return plan_ == null ? org.apache.spark.connect.proto.Plan.getDefaultInstance() : plan_;
        } else {
          return planBuilder_.getMessage();
        }
      }
      /**
       * <pre>
       * (Required) The logical plan to be analyzed.
       * </pre>
       *
       * <code>.spark.connect.Plan plan = 1;</code>
       */
      public Builder setPlan(org.apache.spark.connect.proto.Plan value) {
        if (planBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          plan_ = value;
        } else {
          planBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * (Required) The logical plan to be analyzed.
       * </pre>
       *
       * <code>.spark.connect.Plan plan = 1;</code>
       */
      public Builder setPlan(
          org.apache.spark.connect.proto.Plan.Builder builderForValue) {
        if (planBuilder_ == null) {
          plan_ = builderForValue.build();
        } else {
          planBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * (Required) The logical plan to be analyzed.
       * </pre>
       *
       * <code>.spark.connect.Plan plan = 1;</code>
       */
      public Builder mergePlan(org.apache.spark.connect.proto.Plan value) {
        if (planBuilder_ == null) {
          if (((bitField0_ & 0x00000001) != 0) &&
            plan_ != null &&
            plan_ != org.apache.spark.connect.proto.Plan.getDefaultInstance()) {
            getPlanBuilder().mergeFrom(value);
          } else {
            plan_ = value;
          }
        } else {
          planBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * (Required) The logical plan to be analyzed.
       * </pre>
       *
       * <code>.spark.connect.Plan plan = 1;</code>
       */
      public Builder clearPlan() {
        bitField0_ = (bitField0_ & ~0x00000001);
        plan_ = null;
        if (planBuilder_ != null) {
          planBuilder_.dispose();
          planBuilder_ = null;
        }
        onChanged();
        return this;
      }
      /**
       * <pre>
       * (Required) The logical plan to be analyzed.
       * </pre>
       *
       * <code>.spark.connect.Plan plan = 1;</code>
       */
      public org.apache.spark.connect.proto.Plan.Builder getPlanBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getPlanFieldBuilder().getBuilder();
      }
      /**
       * <pre>
       * (Required) The logical plan to be analyzed.
       * </pre>
       *
       * <code>.spark.connect.Plan plan = 1;</code>
       */
      public org.apache.spark.connect.proto.PlanOrBuilder getPlanOrBuilder() {
        if (planBuilder_ != null) {
          return planBuilder_.getMessageOrBuilder();
        } else {
          return plan_ == null ?
              org.apache.spark.connect.proto.Plan.getDefaultInstance() : plan_;
        }
      }
      /**
       * <pre>
       * (Required) The logical plan to be analyzed.
       * </pre>
       *
       * <code>.spark.connect.Plan plan = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.connect.proto.Plan, org.apache.spark.connect.proto.Plan.Builder, org.apache.spark.connect.proto.PlanOrBuilder> 
          getPlanFieldBuilder() {
        if (planBuilder_ == null) {
          planBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.apache.spark.connect.proto.Plan, org.apache.spark.connect.proto.Plan.Builder, org.apache.spark.connect.proto.PlanOrBuilder>(
                  getPlan(),
                  getParentForChildren(),
                  isClean());
          plan_ = null;
        }
        return planBuilder_;
      }
      @Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:spark.connect.AnalyzePlanRequest.IsStreaming)
    }

    // @@protoc_insertion_point(class_scope:spark.connect.AnalyzePlanRequest.IsStreaming)
    private static final org.apache.spark.connect.proto.AnalyzePlanRequest.IsStreaming DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.connect.proto.AnalyzePlanRequest.IsStreaming();
    }

    public static org.apache.spark.connect.proto.AnalyzePlanRequest.IsStreaming getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<IsStreaming>
        PARSER = new com.google.protobuf.AbstractParser<IsStreaming>() {
      @Override
      public IsStreaming parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        Builder builder = newBuilder();
        try {
          builder.mergeFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.setUnfinishedMessage(builder.buildPartial());
        } catch (com.google.protobuf.UninitializedMessageException e) {
          throw e.asInvalidProtocolBufferException().setUnfinishedMessage(builder.buildPartial());
        } catch (java.io.IOException e) {
          throw new com.google.protobuf.InvalidProtocolBufferException(e)
              .setUnfinishedMessage(builder.buildPartial());
        }
        return builder.buildPartial();
      }
    };

    public static com.google.protobuf.Parser<IsStreaming> parser() {
      return PARSER;
    }

    @Override
    public com.google.protobuf.Parser<IsStreaming> getParserForType() {
      return PARSER;
    }

    @Override
    public org.apache.spark.connect.proto.AnalyzePlanRequest.IsStreaming getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface InputFilesOrBuilder extends
      // @@protoc_insertion_point(interface_extends:spark.connect.AnalyzePlanRequest.InputFiles)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <pre>
     * (Required) The logical plan to be analyzed.
     * </pre>
     *
     * <code>.spark.connect.Plan plan = 1;</code>
     * @return Whether the plan field is set.
     */
    boolean hasPlan();
    /**
     * <pre>
     * (Required) The logical plan to be analyzed.
     * </pre>
     *
     * <code>.spark.connect.Plan plan = 1;</code>
     * @return The plan.
     */
    org.apache.spark.connect.proto.Plan getPlan();
    /**
     * <pre>
     * (Required) The logical plan to be analyzed.
     * </pre>
     *
     * <code>.spark.connect.Plan plan = 1;</code>
     */
    org.apache.spark.connect.proto.PlanOrBuilder getPlanOrBuilder();
  }
  /**
   * Protobuf type {@code spark.connect.AnalyzePlanRequest.InputFiles}
   */
  public static final class InputFiles extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:spark.connect.AnalyzePlanRequest.InputFiles)
      InputFilesOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use InputFiles.newBuilder() to construct.
    private InputFiles(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private InputFiles() {
    }

    @Override
    @SuppressWarnings({"unused"})
    protected Object newInstance(
        UnusedPrivateParameter unused) {
      return new InputFiles();
    }

    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.connect.proto.Base.internal_static_spark_connect_AnalyzePlanRequest_InputFiles_descriptor;
    }

    @Override
    protected FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.connect.proto.Base.internal_static_spark_connect_AnalyzePlanRequest_InputFiles_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.connect.proto.AnalyzePlanRequest.InputFiles.class, org.apache.spark.connect.proto.AnalyzePlanRequest.InputFiles.Builder.class);
    }

    public static final int PLAN_FIELD_NUMBER = 1;
    private org.apache.spark.connect.proto.Plan plan_;
    /**
     * <pre>
     * (Required) The logical plan to be analyzed.
     * </pre>
     *
     * <code>.spark.connect.Plan plan = 1;</code>
     * @return Whether the plan field is set.
     */
    @Override
    public boolean hasPlan() {
      return plan_ != null;
    }
    /**
     * <pre>
     * (Required) The logical plan to be analyzed.
     * </pre>
     *
     * <code>.spark.connect.Plan plan = 1;</code>
     * @return The plan.
     */
    @Override
    public org.apache.spark.connect.proto.Plan getPlan() {
      return plan_ == null ? org.apache.spark.connect.proto.Plan.getDefaultInstance() : plan_;
    }
    /**
     * <pre>
     * (Required) The logical plan to be analyzed.
     * </pre>
     *
     * <code>.spark.connect.Plan plan = 1;</code>
     */
    @Override
    public org.apache.spark.connect.proto.PlanOrBuilder getPlanOrBuilder() {
      return plan_ == null ? org.apache.spark.connect.proto.Plan.getDefaultInstance() : plan_;
    }

    private byte memoizedIsInitialized = -1;
    @Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (plan_ != null) {
        output.writeMessage(1, getPlan());
      }
      getUnknownFields().writeTo(output);
    }

    @Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (plan_ != null) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, getPlan());
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @Override
    public boolean equals(final Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.connect.proto.AnalyzePlanRequest.InputFiles)) {
        return super.equals(obj);
      }
      org.apache.spark.connect.proto.AnalyzePlanRequest.InputFiles other = (org.apache.spark.connect.proto.AnalyzePlanRequest.InputFiles) obj;

      if (hasPlan() != other.hasPlan()) return false;
      if (hasPlan()) {
        if (!getPlan()
            .equals(other.getPlan())) return false;
      }
      if (!getUnknownFields().equals(other.getUnknownFields())) return false;
      return true;
    }

    @Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasPlan()) {
        hash = (37 * hash) + PLAN_FIELD_NUMBER;
        hash = (53 * hash) + getPlan().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.connect.proto.AnalyzePlanRequest.InputFiles parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.InputFiles parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.InputFiles parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.InputFiles parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.InputFiles parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.InputFiles parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.InputFiles parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.InputFiles parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    public static org.apache.spark.connect.proto.AnalyzePlanRequest.InputFiles parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }

    public static org.apache.spark.connect.proto.AnalyzePlanRequest.InputFiles parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.InputFiles parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.InputFiles parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.connect.proto.AnalyzePlanRequest.InputFiles prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @Override
    protected Builder newBuilderForType(
        BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code spark.connect.AnalyzePlanRequest.InputFiles}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:spark.connect.AnalyzePlanRequest.InputFiles)
        org.apache.spark.connect.proto.AnalyzePlanRequest.InputFilesOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.connect.proto.Base.internal_static_spark_connect_AnalyzePlanRequest_InputFiles_descriptor;
      }

      @Override
      protected FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.connect.proto.Base.internal_static_spark_connect_AnalyzePlanRequest_InputFiles_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.connect.proto.AnalyzePlanRequest.InputFiles.class, org.apache.spark.connect.proto.AnalyzePlanRequest.InputFiles.Builder.class);
      }

      // Construct using org.apache.spark.connect.proto.AnalyzePlanRequest.InputFiles.newBuilder()
      private Builder() {

      }

      private Builder(
          BuilderParent parent) {
        super(parent);

      }
      @Override
      public Builder clear() {
        super.clear();
        bitField0_ = 0;
        plan_ = null;
        if (planBuilder_ != null) {
          planBuilder_.dispose();
          planBuilder_ = null;
        }
        return this;
      }

      @Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.connect.proto.Base.internal_static_spark_connect_AnalyzePlanRequest_InputFiles_descriptor;
      }

      @Override
      public org.apache.spark.connect.proto.AnalyzePlanRequest.InputFiles getDefaultInstanceForType() {
        return org.apache.spark.connect.proto.AnalyzePlanRequest.InputFiles.getDefaultInstance();
      }

      @Override
      public org.apache.spark.connect.proto.AnalyzePlanRequest.InputFiles build() {
        org.apache.spark.connect.proto.AnalyzePlanRequest.InputFiles result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @Override
      public org.apache.spark.connect.proto.AnalyzePlanRequest.InputFiles buildPartial() {
        org.apache.spark.connect.proto.AnalyzePlanRequest.InputFiles result = new org.apache.spark.connect.proto.AnalyzePlanRequest.InputFiles(this);
        if (bitField0_ != 0) { buildPartial0(result); }
        onBuilt();
        return result;
      }

      private void buildPartial0(org.apache.spark.connect.proto.AnalyzePlanRequest.InputFiles result) {
        int from_bitField0_ = bitField0_;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          result.plan_ = planBuilder_ == null
              ? plan_
              : planBuilder_.build();
        }
      }

      @Override
      public Builder clone() {
        return super.clone();
      }
      @Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          Object value) {
        return super.setField(field, value);
      }
      @Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          Object value) {
        return super.addRepeatedField(field, value);
      }
      @Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.connect.proto.AnalyzePlanRequest.InputFiles) {
          return mergeFrom((org.apache.spark.connect.proto.AnalyzePlanRequest.InputFiles)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.connect.proto.AnalyzePlanRequest.InputFiles other) {
        if (other == org.apache.spark.connect.proto.AnalyzePlanRequest.InputFiles.getDefaultInstance()) return this;
        if (other.hasPlan()) {
          mergePlan(other.getPlan());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        onChanged();
        return this;
      }

      @Override
      public final boolean isInitialized() {
        return true;
      }

      @Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        if (extensionRegistry == null) {
          throw new NullPointerException();
        }
        try {
          boolean done = false;
          while (!done) {
            int tag = input.readTag();
            switch (tag) {
              case 0:
                done = true;
                break;
              case 10: {
                input.readMessage(
                    getPlanFieldBuilder().getBuilder(),
                    extensionRegistry);
                bitField0_ |= 0x00000001;
                break;
              } // case 10
              default: {
                if (!super.parseUnknownField(input, extensionRegistry, tag)) {
                  done = true; // was an endgroup tag
                }
                break;
              } // default:
            } // switch (tag)
          } // while (!done)
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.unwrapIOException();
        } finally {
          onChanged();
        } // finally
        return this;
      }
      private int bitField0_;

      private org.apache.spark.connect.proto.Plan plan_;
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.connect.proto.Plan, org.apache.spark.connect.proto.Plan.Builder, org.apache.spark.connect.proto.PlanOrBuilder> planBuilder_;
      /**
       * <pre>
       * (Required) The logical plan to be analyzed.
       * </pre>
       *
       * <code>.spark.connect.Plan plan = 1;</code>
       * @return Whether the plan field is set.
       */
      public boolean hasPlan() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <pre>
       * (Required) The logical plan to be analyzed.
       * </pre>
       *
       * <code>.spark.connect.Plan plan = 1;</code>
       * @return The plan.
       */
      public org.apache.spark.connect.proto.Plan getPlan() {
        if (planBuilder_ == null) {
          return plan_ == null ? org.apache.spark.connect.proto.Plan.getDefaultInstance() : plan_;
        } else {
          return planBuilder_.getMessage();
        }
      }
      /**
       * <pre>
       * (Required) The logical plan to be analyzed.
       * </pre>
       *
       * <code>.spark.connect.Plan plan = 1;</code>
       */
      public Builder setPlan(org.apache.spark.connect.proto.Plan value) {
        if (planBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          plan_ = value;
        } else {
          planBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * (Required) The logical plan to be analyzed.
       * </pre>
       *
       * <code>.spark.connect.Plan plan = 1;</code>
       */
      public Builder setPlan(
          org.apache.spark.connect.proto.Plan.Builder builderForValue) {
        if (planBuilder_ == null) {
          plan_ = builderForValue.build();
        } else {
          planBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * (Required) The logical plan to be analyzed.
       * </pre>
       *
       * <code>.spark.connect.Plan plan = 1;</code>
       */
      public Builder mergePlan(org.apache.spark.connect.proto.Plan value) {
        if (planBuilder_ == null) {
          if (((bitField0_ & 0x00000001) != 0) &&
            plan_ != null &&
            plan_ != org.apache.spark.connect.proto.Plan.getDefaultInstance()) {
            getPlanBuilder().mergeFrom(value);
          } else {
            plan_ = value;
          }
        } else {
          planBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * (Required) The logical plan to be analyzed.
       * </pre>
       *
       * <code>.spark.connect.Plan plan = 1;</code>
       */
      public Builder clearPlan() {
        bitField0_ = (bitField0_ & ~0x00000001);
        plan_ = null;
        if (planBuilder_ != null) {
          planBuilder_.dispose();
          planBuilder_ = null;
        }
        onChanged();
        return this;
      }
      /**
       * <pre>
       * (Required) The logical plan to be analyzed.
       * </pre>
       *
       * <code>.spark.connect.Plan plan = 1;</code>
       */
      public org.apache.spark.connect.proto.Plan.Builder getPlanBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getPlanFieldBuilder().getBuilder();
      }
      /**
       * <pre>
       * (Required) The logical plan to be analyzed.
       * </pre>
       *
       * <code>.spark.connect.Plan plan = 1;</code>
       */
      public org.apache.spark.connect.proto.PlanOrBuilder getPlanOrBuilder() {
        if (planBuilder_ != null) {
          return planBuilder_.getMessageOrBuilder();
        } else {
          return plan_ == null ?
              org.apache.spark.connect.proto.Plan.getDefaultInstance() : plan_;
        }
      }
      /**
       * <pre>
       * (Required) The logical plan to be analyzed.
       * </pre>
       *
       * <code>.spark.connect.Plan plan = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.connect.proto.Plan, org.apache.spark.connect.proto.Plan.Builder, org.apache.spark.connect.proto.PlanOrBuilder> 
          getPlanFieldBuilder() {
        if (planBuilder_ == null) {
          planBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.apache.spark.connect.proto.Plan, org.apache.spark.connect.proto.Plan.Builder, org.apache.spark.connect.proto.PlanOrBuilder>(
                  getPlan(),
                  getParentForChildren(),
                  isClean());
          plan_ = null;
        }
        return planBuilder_;
      }
      @Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:spark.connect.AnalyzePlanRequest.InputFiles)
    }

    // @@protoc_insertion_point(class_scope:spark.connect.AnalyzePlanRequest.InputFiles)
    private static final org.apache.spark.connect.proto.AnalyzePlanRequest.InputFiles DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.connect.proto.AnalyzePlanRequest.InputFiles();
    }

    public static org.apache.spark.connect.proto.AnalyzePlanRequest.InputFiles getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<InputFiles>
        PARSER = new com.google.protobuf.AbstractParser<InputFiles>() {
      @Override
      public InputFiles parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        Builder builder = newBuilder();
        try {
          builder.mergeFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.setUnfinishedMessage(builder.buildPartial());
        } catch (com.google.protobuf.UninitializedMessageException e) {
          throw e.asInvalidProtocolBufferException().setUnfinishedMessage(builder.buildPartial());
        } catch (java.io.IOException e) {
          throw new com.google.protobuf.InvalidProtocolBufferException(e)
              .setUnfinishedMessage(builder.buildPartial());
        }
        return builder.buildPartial();
      }
    };

    public static com.google.protobuf.Parser<InputFiles> parser() {
      return PARSER;
    }

    @Override
    public com.google.protobuf.Parser<InputFiles> getParserForType() {
      return PARSER;
    }

    @Override
    public org.apache.spark.connect.proto.AnalyzePlanRequest.InputFiles getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface SparkVersionOrBuilder extends
      // @@protoc_insertion_point(interface_extends:spark.connect.AnalyzePlanRequest.SparkVersion)
      com.google.protobuf.MessageOrBuilder {
  }
  /**
   * Protobuf type {@code spark.connect.AnalyzePlanRequest.SparkVersion}
   */
  public static final class SparkVersion extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:spark.connect.AnalyzePlanRequest.SparkVersion)
      SparkVersionOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use SparkVersion.newBuilder() to construct.
    private SparkVersion(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private SparkVersion() {
    }

    @Override
    @SuppressWarnings({"unused"})
    protected Object newInstance(
        UnusedPrivateParameter unused) {
      return new SparkVersion();
    }

    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.connect.proto.Base.internal_static_spark_connect_AnalyzePlanRequest_SparkVersion_descriptor;
    }

    @Override
    protected FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.connect.proto.Base.internal_static_spark_connect_AnalyzePlanRequest_SparkVersion_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.connect.proto.AnalyzePlanRequest.SparkVersion.class, org.apache.spark.connect.proto.AnalyzePlanRequest.SparkVersion.Builder.class);
    }

    private byte memoizedIsInitialized = -1;
    @Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getUnknownFields().writeTo(output);
    }

    @Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      size += getUnknownFields().getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @Override
    public boolean equals(final Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.connect.proto.AnalyzePlanRequest.SparkVersion)) {
        return super.equals(obj);
      }
      org.apache.spark.connect.proto.AnalyzePlanRequest.SparkVersion other = (org.apache.spark.connect.proto.AnalyzePlanRequest.SparkVersion) obj;

      if (!getUnknownFields().equals(other.getUnknownFields())) return false;
      return true;
    }

    @Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.connect.proto.AnalyzePlanRequest.SparkVersion parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.SparkVersion parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.SparkVersion parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.SparkVersion parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.SparkVersion parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.SparkVersion parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.SparkVersion parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.SparkVersion parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    public static org.apache.spark.connect.proto.AnalyzePlanRequest.SparkVersion parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }

    public static org.apache.spark.connect.proto.AnalyzePlanRequest.SparkVersion parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.SparkVersion parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.SparkVersion parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.connect.proto.AnalyzePlanRequest.SparkVersion prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @Override
    protected Builder newBuilderForType(
        BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code spark.connect.AnalyzePlanRequest.SparkVersion}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:spark.connect.AnalyzePlanRequest.SparkVersion)
        org.apache.spark.connect.proto.AnalyzePlanRequest.SparkVersionOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.connect.proto.Base.internal_static_spark_connect_AnalyzePlanRequest_SparkVersion_descriptor;
      }

      @Override
      protected FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.connect.proto.Base.internal_static_spark_connect_AnalyzePlanRequest_SparkVersion_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.connect.proto.AnalyzePlanRequest.SparkVersion.class, org.apache.spark.connect.proto.AnalyzePlanRequest.SparkVersion.Builder.class);
      }

      // Construct using org.apache.spark.connect.proto.AnalyzePlanRequest.SparkVersion.newBuilder()
      private Builder() {

      }

      private Builder(
          BuilderParent parent) {
        super(parent);

      }
      @Override
      public Builder clear() {
        super.clear();
        return this;
      }

      @Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.connect.proto.Base.internal_static_spark_connect_AnalyzePlanRequest_SparkVersion_descriptor;
      }

      @Override
      public org.apache.spark.connect.proto.AnalyzePlanRequest.SparkVersion getDefaultInstanceForType() {
        return org.apache.spark.connect.proto.AnalyzePlanRequest.SparkVersion.getDefaultInstance();
      }

      @Override
      public org.apache.spark.connect.proto.AnalyzePlanRequest.SparkVersion build() {
        org.apache.spark.connect.proto.AnalyzePlanRequest.SparkVersion result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @Override
      public org.apache.spark.connect.proto.AnalyzePlanRequest.SparkVersion buildPartial() {
        org.apache.spark.connect.proto.AnalyzePlanRequest.SparkVersion result = new org.apache.spark.connect.proto.AnalyzePlanRequest.SparkVersion(this);
        onBuilt();
        return result;
      }

      @Override
      public Builder clone() {
        return super.clone();
      }
      @Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          Object value) {
        return super.setField(field, value);
      }
      @Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          Object value) {
        return super.addRepeatedField(field, value);
      }
      @Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.connect.proto.AnalyzePlanRequest.SparkVersion) {
          return mergeFrom((org.apache.spark.connect.proto.AnalyzePlanRequest.SparkVersion)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.connect.proto.AnalyzePlanRequest.SparkVersion other) {
        if (other == org.apache.spark.connect.proto.AnalyzePlanRequest.SparkVersion.getDefaultInstance()) return this;
        this.mergeUnknownFields(other.getUnknownFields());
        onChanged();
        return this;
      }

      @Override
      public final boolean isInitialized() {
        return true;
      }

      @Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        if (extensionRegistry == null) {
          throw new NullPointerException();
        }
        try {
          boolean done = false;
          while (!done) {
            int tag = input.readTag();
            switch (tag) {
              case 0:
                done = true;
                break;
              default: {
                if (!super.parseUnknownField(input, extensionRegistry, tag)) {
                  done = true; // was an endgroup tag
                }
                break;
              } // default:
            } // switch (tag)
          } // while (!done)
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.unwrapIOException();
        } finally {
          onChanged();
        } // finally
        return this;
      }
      @Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:spark.connect.AnalyzePlanRequest.SparkVersion)
    }

    // @@protoc_insertion_point(class_scope:spark.connect.AnalyzePlanRequest.SparkVersion)
    private static final org.apache.spark.connect.proto.AnalyzePlanRequest.SparkVersion DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.connect.proto.AnalyzePlanRequest.SparkVersion();
    }

    public static org.apache.spark.connect.proto.AnalyzePlanRequest.SparkVersion getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<SparkVersion>
        PARSER = new com.google.protobuf.AbstractParser<SparkVersion>() {
      @Override
      public SparkVersion parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        Builder builder = newBuilder();
        try {
          builder.mergeFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.setUnfinishedMessage(builder.buildPartial());
        } catch (com.google.protobuf.UninitializedMessageException e) {
          throw e.asInvalidProtocolBufferException().setUnfinishedMessage(builder.buildPartial());
        } catch (java.io.IOException e) {
          throw new com.google.protobuf.InvalidProtocolBufferException(e)
              .setUnfinishedMessage(builder.buildPartial());
        }
        return builder.buildPartial();
      }
    };

    public static com.google.protobuf.Parser<SparkVersion> parser() {
      return PARSER;
    }

    @Override
    public com.google.protobuf.Parser<SparkVersion> getParserForType() {
      return PARSER;
    }

    @Override
    public org.apache.spark.connect.proto.AnalyzePlanRequest.SparkVersion getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface DDLParseOrBuilder extends
      // @@protoc_insertion_point(interface_extends:spark.connect.AnalyzePlanRequest.DDLParse)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <pre>
     * (Required) The DDL formatted string to be parsed.
     * </pre>
     *
     * <code>string ddl_string = 1;</code>
     * @return The ddlString.
     */
    String getDdlString();
    /**
     * <pre>
     * (Required) The DDL formatted string to be parsed.
     * </pre>
     *
     * <code>string ddl_string = 1;</code>
     * @return The bytes for ddlString.
     */
    com.google.protobuf.ByteString
        getDdlStringBytes();
  }
  /**
   * Protobuf type {@code spark.connect.AnalyzePlanRequest.DDLParse}
   */
  public static final class DDLParse extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:spark.connect.AnalyzePlanRequest.DDLParse)
      DDLParseOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use DDLParse.newBuilder() to construct.
    private DDLParse(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private DDLParse() {
      ddlString_ = "";
    }

    @Override
    @SuppressWarnings({"unused"})
    protected Object newInstance(
        UnusedPrivateParameter unused) {
      return new DDLParse();
    }

    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.connect.proto.Base.internal_static_spark_connect_AnalyzePlanRequest_DDLParse_descriptor;
    }

    @Override
    protected FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.connect.proto.Base.internal_static_spark_connect_AnalyzePlanRequest_DDLParse_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.connect.proto.AnalyzePlanRequest.DDLParse.class, org.apache.spark.connect.proto.AnalyzePlanRequest.DDLParse.Builder.class);
    }

    public static final int DDL_STRING_FIELD_NUMBER = 1;
    @SuppressWarnings("serial")
    private volatile Object ddlString_ = "";
    /**
     * <pre>
     * (Required) The DDL formatted string to be parsed.
     * </pre>
     *
     * <code>string ddl_string = 1;</code>
     * @return The ddlString.
     */
    @Override
    public String getDdlString() {
      Object ref = ddlString_;
      if (ref instanceof String) {
        return (String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        String s = bs.toStringUtf8();
        ddlString_ = s;
        return s;
      }
    }
    /**
     * <pre>
     * (Required) The DDL formatted string to be parsed.
     * </pre>
     *
     * <code>string ddl_string = 1;</code>
     * @return The bytes for ddlString.
     */
    @Override
    public com.google.protobuf.ByteString
        getDdlStringBytes() {
      Object ref = ddlString_;
      if (ref instanceof String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (String) ref);
        ddlString_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    private byte memoizedIsInitialized = -1;
    @Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (!com.google.protobuf.GeneratedMessageV3.isStringEmpty(ddlString_)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 1, ddlString_);
      }
      getUnknownFields().writeTo(output);
    }

    @Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (!com.google.protobuf.GeneratedMessageV3.isStringEmpty(ddlString_)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(1, ddlString_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @Override
    public boolean equals(final Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.connect.proto.AnalyzePlanRequest.DDLParse)) {
        return super.equals(obj);
      }
      org.apache.spark.connect.proto.AnalyzePlanRequest.DDLParse other = (org.apache.spark.connect.proto.AnalyzePlanRequest.DDLParse) obj;

      if (!getDdlString()
          .equals(other.getDdlString())) return false;
      if (!getUnknownFields().equals(other.getUnknownFields())) return false;
      return true;
    }

    @Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      hash = (37 * hash) + DDL_STRING_FIELD_NUMBER;
      hash = (53 * hash) + getDdlString().hashCode();
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.connect.proto.AnalyzePlanRequest.DDLParse parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.DDLParse parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.DDLParse parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.DDLParse parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.DDLParse parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.DDLParse parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.DDLParse parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.DDLParse parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    public static org.apache.spark.connect.proto.AnalyzePlanRequest.DDLParse parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }

    public static org.apache.spark.connect.proto.AnalyzePlanRequest.DDLParse parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.DDLParse parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.DDLParse parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.connect.proto.AnalyzePlanRequest.DDLParse prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @Override
    protected Builder newBuilderForType(
        BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code spark.connect.AnalyzePlanRequest.DDLParse}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:spark.connect.AnalyzePlanRequest.DDLParse)
        org.apache.spark.connect.proto.AnalyzePlanRequest.DDLParseOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.connect.proto.Base.internal_static_spark_connect_AnalyzePlanRequest_DDLParse_descriptor;
      }

      @Override
      protected FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.connect.proto.Base.internal_static_spark_connect_AnalyzePlanRequest_DDLParse_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.connect.proto.AnalyzePlanRequest.DDLParse.class, org.apache.spark.connect.proto.AnalyzePlanRequest.DDLParse.Builder.class);
      }

      // Construct using org.apache.spark.connect.proto.AnalyzePlanRequest.DDLParse.newBuilder()
      private Builder() {

      }

      private Builder(
          BuilderParent parent) {
        super(parent);

      }
      @Override
      public Builder clear() {
        super.clear();
        bitField0_ = 0;
        ddlString_ = "";
        return this;
      }

      @Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.connect.proto.Base.internal_static_spark_connect_AnalyzePlanRequest_DDLParse_descriptor;
      }

      @Override
      public org.apache.spark.connect.proto.AnalyzePlanRequest.DDLParse getDefaultInstanceForType() {
        return org.apache.spark.connect.proto.AnalyzePlanRequest.DDLParse.getDefaultInstance();
      }

      @Override
      public org.apache.spark.connect.proto.AnalyzePlanRequest.DDLParse build() {
        org.apache.spark.connect.proto.AnalyzePlanRequest.DDLParse result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @Override
      public org.apache.spark.connect.proto.AnalyzePlanRequest.DDLParse buildPartial() {
        org.apache.spark.connect.proto.AnalyzePlanRequest.DDLParse result = new org.apache.spark.connect.proto.AnalyzePlanRequest.DDLParse(this);
        if (bitField0_ != 0) { buildPartial0(result); }
        onBuilt();
        return result;
      }

      private void buildPartial0(org.apache.spark.connect.proto.AnalyzePlanRequest.DDLParse result) {
        int from_bitField0_ = bitField0_;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          result.ddlString_ = ddlString_;
        }
      }

      @Override
      public Builder clone() {
        return super.clone();
      }
      @Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          Object value) {
        return super.setField(field, value);
      }
      @Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          Object value) {
        return super.addRepeatedField(field, value);
      }
      @Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.connect.proto.AnalyzePlanRequest.DDLParse) {
          return mergeFrom((org.apache.spark.connect.proto.AnalyzePlanRequest.DDLParse)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.connect.proto.AnalyzePlanRequest.DDLParse other) {
        if (other == org.apache.spark.connect.proto.AnalyzePlanRequest.DDLParse.getDefaultInstance()) return this;
        if (!other.getDdlString().isEmpty()) {
          ddlString_ = other.ddlString_;
          bitField0_ |= 0x00000001;
          onChanged();
        }
        this.mergeUnknownFields(other.getUnknownFields());
        onChanged();
        return this;
      }

      @Override
      public final boolean isInitialized() {
        return true;
      }

      @Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        if (extensionRegistry == null) {
          throw new NullPointerException();
        }
        try {
          boolean done = false;
          while (!done) {
            int tag = input.readTag();
            switch (tag) {
              case 0:
                done = true;
                break;
              case 10: {
                ddlString_ = input.readStringRequireUtf8();
                bitField0_ |= 0x00000001;
                break;
              } // case 10
              default: {
                if (!super.parseUnknownField(input, extensionRegistry, tag)) {
                  done = true; // was an endgroup tag
                }
                break;
              } // default:
            } // switch (tag)
          } // while (!done)
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.unwrapIOException();
        } finally {
          onChanged();
        } // finally
        return this;
      }
      private int bitField0_;

      private Object ddlString_ = "";
      /**
       * <pre>
       * (Required) The DDL formatted string to be parsed.
       * </pre>
       *
       * <code>string ddl_string = 1;</code>
       * @return The ddlString.
       */
      public String getDdlString() {
        Object ref = ddlString_;
        if (!(ref instanceof String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          String s = bs.toStringUtf8();
          ddlString_ = s;
          return s;
        } else {
          return (String) ref;
        }
      }
      /**
       * <pre>
       * (Required) The DDL formatted string to be parsed.
       * </pre>
       *
       * <code>string ddl_string = 1;</code>
       * @return The bytes for ddlString.
       */
      public com.google.protobuf.ByteString
          getDdlStringBytes() {
        Object ref = ddlString_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (String) ref);
          ddlString_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <pre>
       * (Required) The DDL formatted string to be parsed.
       * </pre>
       *
       * <code>string ddl_string = 1;</code>
       * @param value The ddlString to set.
       * @return This builder for chaining.
       */
      public Builder setDdlString(
          String value) {
        if (value == null) { throw new NullPointerException(); }
        ddlString_ = value;
        bitField0_ |= 0x00000001;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * (Required) The DDL formatted string to be parsed.
       * </pre>
       *
       * <code>string ddl_string = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearDdlString() {
        ddlString_ = getDefaultInstance().getDdlString();
        bitField0_ = (bitField0_ & ~0x00000001);
        onChanged();
        return this;
      }
      /**
       * <pre>
       * (Required) The DDL formatted string to be parsed.
       * </pre>
       *
       * <code>string ddl_string = 1;</code>
       * @param value The bytes for ddlString to set.
       * @return This builder for chaining.
       */
      public Builder setDdlStringBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) { throw new NullPointerException(); }
        checkByteStringIsUtf8(value);
        ddlString_ = value;
        bitField0_ |= 0x00000001;
        onChanged();
        return this;
      }
      @Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:spark.connect.AnalyzePlanRequest.DDLParse)
    }

    // @@protoc_insertion_point(class_scope:spark.connect.AnalyzePlanRequest.DDLParse)
    private static final org.apache.spark.connect.proto.AnalyzePlanRequest.DDLParse DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.connect.proto.AnalyzePlanRequest.DDLParse();
    }

    public static org.apache.spark.connect.proto.AnalyzePlanRequest.DDLParse getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<DDLParse>
        PARSER = new com.google.protobuf.AbstractParser<DDLParse>() {
      @Override
      public DDLParse parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        Builder builder = newBuilder();
        try {
          builder.mergeFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.setUnfinishedMessage(builder.buildPartial());
        } catch (com.google.protobuf.UninitializedMessageException e) {
          throw e.asInvalidProtocolBufferException().setUnfinishedMessage(builder.buildPartial());
        } catch (java.io.IOException e) {
          throw new com.google.protobuf.InvalidProtocolBufferException(e)
              .setUnfinishedMessage(builder.buildPartial());
        }
        return builder.buildPartial();
      }
    };

    public static com.google.protobuf.Parser<DDLParse> parser() {
      return PARSER;
    }

    @Override
    public com.google.protobuf.Parser<DDLParse> getParserForType() {
      return PARSER;
    }

    @Override
    public org.apache.spark.connect.proto.AnalyzePlanRequest.DDLParse getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface SameSemanticsOrBuilder extends
      // @@protoc_insertion_point(interface_extends:spark.connect.AnalyzePlanRequest.SameSemantics)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <pre>
     * (Required) The plan to be compared.
     * </pre>
     *
     * <code>.spark.connect.Plan target_plan = 1;</code>
     * @return Whether the targetPlan field is set.
     */
    boolean hasTargetPlan();
    /**
     * <pre>
     * (Required) The plan to be compared.
     * </pre>
     *
     * <code>.spark.connect.Plan target_plan = 1;</code>
     * @return The targetPlan.
     */
    org.apache.spark.connect.proto.Plan getTargetPlan();
    /**
     * <pre>
     * (Required) The plan to be compared.
     * </pre>
     *
     * <code>.spark.connect.Plan target_plan = 1;</code>
     */
    org.apache.spark.connect.proto.PlanOrBuilder getTargetPlanOrBuilder();

    /**
     * <pre>
     * (Required) The other plan to be compared.
     * </pre>
     *
     * <code>.spark.connect.Plan other_plan = 2;</code>
     * @return Whether the otherPlan field is set.
     */
    boolean hasOtherPlan();
    /**
     * <pre>
     * (Required) The other plan to be compared.
     * </pre>
     *
     * <code>.spark.connect.Plan other_plan = 2;</code>
     * @return The otherPlan.
     */
    org.apache.spark.connect.proto.Plan getOtherPlan();
    /**
     * <pre>
     * (Required) The other plan to be compared.
     * </pre>
     *
     * <code>.spark.connect.Plan other_plan = 2;</code>
     */
    org.apache.spark.connect.proto.PlanOrBuilder getOtherPlanOrBuilder();
  }
  /**
   * <pre>
   * Returns `true` when the logical query plans  are equal and therefore return same results.
   * </pre>
   *
   * Protobuf type {@code spark.connect.AnalyzePlanRequest.SameSemantics}
   */
  public static final class SameSemantics extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:spark.connect.AnalyzePlanRequest.SameSemantics)
      SameSemanticsOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use SameSemantics.newBuilder() to construct.
    private SameSemantics(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private SameSemantics() {
    }

    @Override
    @SuppressWarnings({"unused"})
    protected Object newInstance(
        UnusedPrivateParameter unused) {
      return new SameSemantics();
    }

    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.connect.proto.Base.internal_static_spark_connect_AnalyzePlanRequest_SameSemantics_descriptor;
    }

    @Override
    protected FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.connect.proto.Base.internal_static_spark_connect_AnalyzePlanRequest_SameSemantics_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.connect.proto.AnalyzePlanRequest.SameSemantics.class, org.apache.spark.connect.proto.AnalyzePlanRequest.SameSemantics.Builder.class);
    }

    public static final int TARGET_PLAN_FIELD_NUMBER = 1;
    private org.apache.spark.connect.proto.Plan targetPlan_;
    /**
     * <pre>
     * (Required) The plan to be compared.
     * </pre>
     *
     * <code>.spark.connect.Plan target_plan = 1;</code>
     * @return Whether the targetPlan field is set.
     */
    @Override
    public boolean hasTargetPlan() {
      return targetPlan_ != null;
    }
    /**
     * <pre>
     * (Required) The plan to be compared.
     * </pre>
     *
     * <code>.spark.connect.Plan target_plan = 1;</code>
     * @return The targetPlan.
     */
    @Override
    public org.apache.spark.connect.proto.Plan getTargetPlan() {
      return targetPlan_ == null ? org.apache.spark.connect.proto.Plan.getDefaultInstance() : targetPlan_;
    }
    /**
     * <pre>
     * (Required) The plan to be compared.
     * </pre>
     *
     * <code>.spark.connect.Plan target_plan = 1;</code>
     */
    @Override
    public org.apache.spark.connect.proto.PlanOrBuilder getTargetPlanOrBuilder() {
      return targetPlan_ == null ? org.apache.spark.connect.proto.Plan.getDefaultInstance() : targetPlan_;
    }

    public static final int OTHER_PLAN_FIELD_NUMBER = 2;
    private org.apache.spark.connect.proto.Plan otherPlan_;
    /**
     * <pre>
     * (Required) The other plan to be compared.
     * </pre>
     *
     * <code>.spark.connect.Plan other_plan = 2;</code>
     * @return Whether the otherPlan field is set.
     */
    @Override
    public boolean hasOtherPlan() {
      return otherPlan_ != null;
    }
    /**
     * <pre>
     * (Required) The other plan to be compared.
     * </pre>
     *
     * <code>.spark.connect.Plan other_plan = 2;</code>
     * @return The otherPlan.
     */
    @Override
    public org.apache.spark.connect.proto.Plan getOtherPlan() {
      return otherPlan_ == null ? org.apache.spark.connect.proto.Plan.getDefaultInstance() : otherPlan_;
    }
    /**
     * <pre>
     * (Required) The other plan to be compared.
     * </pre>
     *
     * <code>.spark.connect.Plan other_plan = 2;</code>
     */
    @Override
    public org.apache.spark.connect.proto.PlanOrBuilder getOtherPlanOrBuilder() {
      return otherPlan_ == null ? org.apache.spark.connect.proto.Plan.getDefaultInstance() : otherPlan_;
    }

    private byte memoizedIsInitialized = -1;
    @Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (targetPlan_ != null) {
        output.writeMessage(1, getTargetPlan());
      }
      if (otherPlan_ != null) {
        output.writeMessage(2, getOtherPlan());
      }
      getUnknownFields().writeTo(output);
    }

    @Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (targetPlan_ != null) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, getTargetPlan());
      }
      if (otherPlan_ != null) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(2, getOtherPlan());
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @Override
    public boolean equals(final Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.connect.proto.AnalyzePlanRequest.SameSemantics)) {
        return super.equals(obj);
      }
      org.apache.spark.connect.proto.AnalyzePlanRequest.SameSemantics other = (org.apache.spark.connect.proto.AnalyzePlanRequest.SameSemantics) obj;

      if (hasTargetPlan() != other.hasTargetPlan()) return false;
      if (hasTargetPlan()) {
        if (!getTargetPlan()
            .equals(other.getTargetPlan())) return false;
      }
      if (hasOtherPlan() != other.hasOtherPlan()) return false;
      if (hasOtherPlan()) {
        if (!getOtherPlan()
            .equals(other.getOtherPlan())) return false;
      }
      if (!getUnknownFields().equals(other.getUnknownFields())) return false;
      return true;
    }

    @Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasTargetPlan()) {
        hash = (37 * hash) + TARGET_PLAN_FIELD_NUMBER;
        hash = (53 * hash) + getTargetPlan().hashCode();
      }
      if (hasOtherPlan()) {
        hash = (37 * hash) + OTHER_PLAN_FIELD_NUMBER;
        hash = (53 * hash) + getOtherPlan().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.connect.proto.AnalyzePlanRequest.SameSemantics parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.SameSemantics parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.SameSemantics parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.SameSemantics parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.SameSemantics parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.SameSemantics parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.SameSemantics parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.SameSemantics parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    public static org.apache.spark.connect.proto.AnalyzePlanRequest.SameSemantics parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }

    public static org.apache.spark.connect.proto.AnalyzePlanRequest.SameSemantics parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.SameSemantics parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.SameSemantics parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.connect.proto.AnalyzePlanRequest.SameSemantics prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @Override
    protected Builder newBuilderForType(
        BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     * Returns `true` when the logical query plans  are equal and therefore return same results.
     * </pre>
     *
     * Protobuf type {@code spark.connect.AnalyzePlanRequest.SameSemantics}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:spark.connect.AnalyzePlanRequest.SameSemantics)
        org.apache.spark.connect.proto.AnalyzePlanRequest.SameSemanticsOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.connect.proto.Base.internal_static_spark_connect_AnalyzePlanRequest_SameSemantics_descriptor;
      }

      @Override
      protected FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.connect.proto.Base.internal_static_spark_connect_AnalyzePlanRequest_SameSemantics_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.connect.proto.AnalyzePlanRequest.SameSemantics.class, org.apache.spark.connect.proto.AnalyzePlanRequest.SameSemantics.Builder.class);
      }

      // Construct using org.apache.spark.connect.proto.AnalyzePlanRequest.SameSemantics.newBuilder()
      private Builder() {

      }

      private Builder(
          BuilderParent parent) {
        super(parent);

      }
      @Override
      public Builder clear() {
        super.clear();
        bitField0_ = 0;
        targetPlan_ = null;
        if (targetPlanBuilder_ != null) {
          targetPlanBuilder_.dispose();
          targetPlanBuilder_ = null;
        }
        otherPlan_ = null;
        if (otherPlanBuilder_ != null) {
          otherPlanBuilder_.dispose();
          otherPlanBuilder_ = null;
        }
        return this;
      }

      @Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.connect.proto.Base.internal_static_spark_connect_AnalyzePlanRequest_SameSemantics_descriptor;
      }

      @Override
      public org.apache.spark.connect.proto.AnalyzePlanRequest.SameSemantics getDefaultInstanceForType() {
        return org.apache.spark.connect.proto.AnalyzePlanRequest.SameSemantics.getDefaultInstance();
      }

      @Override
      public org.apache.spark.connect.proto.AnalyzePlanRequest.SameSemantics build() {
        org.apache.spark.connect.proto.AnalyzePlanRequest.SameSemantics result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @Override
      public org.apache.spark.connect.proto.AnalyzePlanRequest.SameSemantics buildPartial() {
        org.apache.spark.connect.proto.AnalyzePlanRequest.SameSemantics result = new org.apache.spark.connect.proto.AnalyzePlanRequest.SameSemantics(this);
        if (bitField0_ != 0) { buildPartial0(result); }
        onBuilt();
        return result;
      }

      private void buildPartial0(org.apache.spark.connect.proto.AnalyzePlanRequest.SameSemantics result) {
        int from_bitField0_ = bitField0_;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          result.targetPlan_ = targetPlanBuilder_ == null
              ? targetPlan_
              : targetPlanBuilder_.build();
        }
        if (((from_bitField0_ & 0x00000002) != 0)) {
          result.otherPlan_ = otherPlanBuilder_ == null
              ? otherPlan_
              : otherPlanBuilder_.build();
        }
      }

      @Override
      public Builder clone() {
        return super.clone();
      }
      @Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          Object value) {
        return super.setField(field, value);
      }
      @Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          Object value) {
        return super.addRepeatedField(field, value);
      }
      @Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.connect.proto.AnalyzePlanRequest.SameSemantics) {
          return mergeFrom((org.apache.spark.connect.proto.AnalyzePlanRequest.SameSemantics)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.connect.proto.AnalyzePlanRequest.SameSemantics other) {
        if (other == org.apache.spark.connect.proto.AnalyzePlanRequest.SameSemantics.getDefaultInstance()) return this;
        if (other.hasTargetPlan()) {
          mergeTargetPlan(other.getTargetPlan());
        }
        if (other.hasOtherPlan()) {
          mergeOtherPlan(other.getOtherPlan());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        onChanged();
        return this;
      }

      @Override
      public final boolean isInitialized() {
        return true;
      }

      @Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        if (extensionRegistry == null) {
          throw new NullPointerException();
        }
        try {
          boolean done = false;
          while (!done) {
            int tag = input.readTag();
            switch (tag) {
              case 0:
                done = true;
                break;
              case 10: {
                input.readMessage(
                    getTargetPlanFieldBuilder().getBuilder(),
                    extensionRegistry);
                bitField0_ |= 0x00000001;
                break;
              } // case 10
              case 18: {
                input.readMessage(
                    getOtherPlanFieldBuilder().getBuilder(),
                    extensionRegistry);
                bitField0_ |= 0x00000002;
                break;
              } // case 18
              default: {
                if (!super.parseUnknownField(input, extensionRegistry, tag)) {
                  done = true; // was an endgroup tag
                }
                break;
              } // default:
            } // switch (tag)
          } // while (!done)
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.unwrapIOException();
        } finally {
          onChanged();
        } // finally
        return this;
      }
      private int bitField0_;

      private org.apache.spark.connect.proto.Plan targetPlan_;
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.connect.proto.Plan, org.apache.spark.connect.proto.Plan.Builder, org.apache.spark.connect.proto.PlanOrBuilder> targetPlanBuilder_;
      /**
       * <pre>
       * (Required) The plan to be compared.
       * </pre>
       *
       * <code>.spark.connect.Plan target_plan = 1;</code>
       * @return Whether the targetPlan field is set.
       */
      public boolean hasTargetPlan() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <pre>
       * (Required) The plan to be compared.
       * </pre>
       *
       * <code>.spark.connect.Plan target_plan = 1;</code>
       * @return The targetPlan.
       */
      public org.apache.spark.connect.proto.Plan getTargetPlan() {
        if (targetPlanBuilder_ == null) {
          return targetPlan_ == null ? org.apache.spark.connect.proto.Plan.getDefaultInstance() : targetPlan_;
        } else {
          return targetPlanBuilder_.getMessage();
        }
      }
      /**
       * <pre>
       * (Required) The plan to be compared.
       * </pre>
       *
       * <code>.spark.connect.Plan target_plan = 1;</code>
       */
      public Builder setTargetPlan(org.apache.spark.connect.proto.Plan value) {
        if (targetPlanBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          targetPlan_ = value;
        } else {
          targetPlanBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * (Required) The plan to be compared.
       * </pre>
       *
       * <code>.spark.connect.Plan target_plan = 1;</code>
       */
      public Builder setTargetPlan(
          org.apache.spark.connect.proto.Plan.Builder builderForValue) {
        if (targetPlanBuilder_ == null) {
          targetPlan_ = builderForValue.build();
        } else {
          targetPlanBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * (Required) The plan to be compared.
       * </pre>
       *
       * <code>.spark.connect.Plan target_plan = 1;</code>
       */
      public Builder mergeTargetPlan(org.apache.spark.connect.proto.Plan value) {
        if (targetPlanBuilder_ == null) {
          if (((bitField0_ & 0x00000001) != 0) &&
            targetPlan_ != null &&
            targetPlan_ != org.apache.spark.connect.proto.Plan.getDefaultInstance()) {
            getTargetPlanBuilder().mergeFrom(value);
          } else {
            targetPlan_ = value;
          }
        } else {
          targetPlanBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * (Required) The plan to be compared.
       * </pre>
       *
       * <code>.spark.connect.Plan target_plan = 1;</code>
       */
      public Builder clearTargetPlan() {
        bitField0_ = (bitField0_ & ~0x00000001);
        targetPlan_ = null;
        if (targetPlanBuilder_ != null) {
          targetPlanBuilder_.dispose();
          targetPlanBuilder_ = null;
        }
        onChanged();
        return this;
      }
      /**
       * <pre>
       * (Required) The plan to be compared.
       * </pre>
       *
       * <code>.spark.connect.Plan target_plan = 1;</code>
       */
      public org.apache.spark.connect.proto.Plan.Builder getTargetPlanBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getTargetPlanFieldBuilder().getBuilder();
      }
      /**
       * <pre>
       * (Required) The plan to be compared.
       * </pre>
       *
       * <code>.spark.connect.Plan target_plan = 1;</code>
       */
      public org.apache.spark.connect.proto.PlanOrBuilder getTargetPlanOrBuilder() {
        if (targetPlanBuilder_ != null) {
          return targetPlanBuilder_.getMessageOrBuilder();
        } else {
          return targetPlan_ == null ?
              org.apache.spark.connect.proto.Plan.getDefaultInstance() : targetPlan_;
        }
      }
      /**
       * <pre>
       * (Required) The plan to be compared.
       * </pre>
       *
       * <code>.spark.connect.Plan target_plan = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.connect.proto.Plan, org.apache.spark.connect.proto.Plan.Builder, org.apache.spark.connect.proto.PlanOrBuilder> 
          getTargetPlanFieldBuilder() {
        if (targetPlanBuilder_ == null) {
          targetPlanBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.apache.spark.connect.proto.Plan, org.apache.spark.connect.proto.Plan.Builder, org.apache.spark.connect.proto.PlanOrBuilder>(
                  getTargetPlan(),
                  getParentForChildren(),
                  isClean());
          targetPlan_ = null;
        }
        return targetPlanBuilder_;
      }

      private org.apache.spark.connect.proto.Plan otherPlan_;
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.connect.proto.Plan, org.apache.spark.connect.proto.Plan.Builder, org.apache.spark.connect.proto.PlanOrBuilder> otherPlanBuilder_;
      /**
       * <pre>
       * (Required) The other plan to be compared.
       * </pre>
       *
       * <code>.spark.connect.Plan other_plan = 2;</code>
       * @return Whether the otherPlan field is set.
       */
      public boolean hasOtherPlan() {
        return ((bitField0_ & 0x00000002) != 0);
      }
      /**
       * <pre>
       * (Required) The other plan to be compared.
       * </pre>
       *
       * <code>.spark.connect.Plan other_plan = 2;</code>
       * @return The otherPlan.
       */
      public org.apache.spark.connect.proto.Plan getOtherPlan() {
        if (otherPlanBuilder_ == null) {
          return otherPlan_ == null ? org.apache.spark.connect.proto.Plan.getDefaultInstance() : otherPlan_;
        } else {
          return otherPlanBuilder_.getMessage();
        }
      }
      /**
       * <pre>
       * (Required) The other plan to be compared.
       * </pre>
       *
       * <code>.spark.connect.Plan other_plan = 2;</code>
       */
      public Builder setOtherPlan(org.apache.spark.connect.proto.Plan value) {
        if (otherPlanBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          otherPlan_ = value;
        } else {
          otherPlanBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000002;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * (Required) The other plan to be compared.
       * </pre>
       *
       * <code>.spark.connect.Plan other_plan = 2;</code>
       */
      public Builder setOtherPlan(
          org.apache.spark.connect.proto.Plan.Builder builderForValue) {
        if (otherPlanBuilder_ == null) {
          otherPlan_ = builderForValue.build();
        } else {
          otherPlanBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000002;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * (Required) The other plan to be compared.
       * </pre>
       *
       * <code>.spark.connect.Plan other_plan = 2;</code>
       */
      public Builder mergeOtherPlan(org.apache.spark.connect.proto.Plan value) {
        if (otherPlanBuilder_ == null) {
          if (((bitField0_ & 0x00000002) != 0) &&
            otherPlan_ != null &&
            otherPlan_ != org.apache.spark.connect.proto.Plan.getDefaultInstance()) {
            getOtherPlanBuilder().mergeFrom(value);
          } else {
            otherPlan_ = value;
          }
        } else {
          otherPlanBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000002;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * (Required) The other plan to be compared.
       * </pre>
       *
       * <code>.spark.connect.Plan other_plan = 2;</code>
       */
      public Builder clearOtherPlan() {
        bitField0_ = (bitField0_ & ~0x00000002);
        otherPlan_ = null;
        if (otherPlanBuilder_ != null) {
          otherPlanBuilder_.dispose();
          otherPlanBuilder_ = null;
        }
        onChanged();
        return this;
      }
      /**
       * <pre>
       * (Required) The other plan to be compared.
       * </pre>
       *
       * <code>.spark.connect.Plan other_plan = 2;</code>
       */
      public org.apache.spark.connect.proto.Plan.Builder getOtherPlanBuilder() {
        bitField0_ |= 0x00000002;
        onChanged();
        return getOtherPlanFieldBuilder().getBuilder();
      }
      /**
       * <pre>
       * (Required) The other plan to be compared.
       * </pre>
       *
       * <code>.spark.connect.Plan other_plan = 2;</code>
       */
      public org.apache.spark.connect.proto.PlanOrBuilder getOtherPlanOrBuilder() {
        if (otherPlanBuilder_ != null) {
          return otherPlanBuilder_.getMessageOrBuilder();
        } else {
          return otherPlan_ == null ?
              org.apache.spark.connect.proto.Plan.getDefaultInstance() : otherPlan_;
        }
      }
      /**
       * <pre>
       * (Required) The other plan to be compared.
       * </pre>
       *
       * <code>.spark.connect.Plan other_plan = 2;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.connect.proto.Plan, org.apache.spark.connect.proto.Plan.Builder, org.apache.spark.connect.proto.PlanOrBuilder> 
          getOtherPlanFieldBuilder() {
        if (otherPlanBuilder_ == null) {
          otherPlanBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.apache.spark.connect.proto.Plan, org.apache.spark.connect.proto.Plan.Builder, org.apache.spark.connect.proto.PlanOrBuilder>(
                  getOtherPlan(),
                  getParentForChildren(),
                  isClean());
          otherPlan_ = null;
        }
        return otherPlanBuilder_;
      }
      @Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:spark.connect.AnalyzePlanRequest.SameSemantics)
    }

    // @@protoc_insertion_point(class_scope:spark.connect.AnalyzePlanRequest.SameSemantics)
    private static final org.apache.spark.connect.proto.AnalyzePlanRequest.SameSemantics DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.connect.proto.AnalyzePlanRequest.SameSemantics();
    }

    public static org.apache.spark.connect.proto.AnalyzePlanRequest.SameSemantics getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<SameSemantics>
        PARSER = new com.google.protobuf.AbstractParser<SameSemantics>() {
      @Override
      public SameSemantics parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        Builder builder = newBuilder();
        try {
          builder.mergeFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.setUnfinishedMessage(builder.buildPartial());
        } catch (com.google.protobuf.UninitializedMessageException e) {
          throw e.asInvalidProtocolBufferException().setUnfinishedMessage(builder.buildPartial());
        } catch (java.io.IOException e) {
          throw new com.google.protobuf.InvalidProtocolBufferException(e)
              .setUnfinishedMessage(builder.buildPartial());
        }
        return builder.buildPartial();
      }
    };

    public static com.google.protobuf.Parser<SameSemantics> parser() {
      return PARSER;
    }

    @Override
    public com.google.protobuf.Parser<SameSemantics> getParserForType() {
      return PARSER;
    }

    @Override
    public org.apache.spark.connect.proto.AnalyzePlanRequest.SameSemantics getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface SemanticHashOrBuilder extends
      // @@protoc_insertion_point(interface_extends:spark.connect.AnalyzePlanRequest.SemanticHash)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <pre>
     * (Required) The logical plan to get a hashCode.
     * </pre>
     *
     * <code>.spark.connect.Plan plan = 1;</code>
     * @return Whether the plan field is set.
     */
    boolean hasPlan();
    /**
     * <pre>
     * (Required) The logical plan to get a hashCode.
     * </pre>
     *
     * <code>.spark.connect.Plan plan = 1;</code>
     * @return The plan.
     */
    org.apache.spark.connect.proto.Plan getPlan();
    /**
     * <pre>
     * (Required) The logical plan to get a hashCode.
     * </pre>
     *
     * <code>.spark.connect.Plan plan = 1;</code>
     */
    org.apache.spark.connect.proto.PlanOrBuilder getPlanOrBuilder();
  }
  /**
   * Protobuf type {@code spark.connect.AnalyzePlanRequest.SemanticHash}
   */
  public static final class SemanticHash extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:spark.connect.AnalyzePlanRequest.SemanticHash)
      SemanticHashOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use SemanticHash.newBuilder() to construct.
    private SemanticHash(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private SemanticHash() {
    }

    @Override
    @SuppressWarnings({"unused"})
    protected Object newInstance(
        UnusedPrivateParameter unused) {
      return new SemanticHash();
    }

    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.connect.proto.Base.internal_static_spark_connect_AnalyzePlanRequest_SemanticHash_descriptor;
    }

    @Override
    protected FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.connect.proto.Base.internal_static_spark_connect_AnalyzePlanRequest_SemanticHash_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.connect.proto.AnalyzePlanRequest.SemanticHash.class, org.apache.spark.connect.proto.AnalyzePlanRequest.SemanticHash.Builder.class);
    }

    public static final int PLAN_FIELD_NUMBER = 1;
    private org.apache.spark.connect.proto.Plan plan_;
    /**
     * <pre>
     * (Required) The logical plan to get a hashCode.
     * </pre>
     *
     * <code>.spark.connect.Plan plan = 1;</code>
     * @return Whether the plan field is set.
     */
    @Override
    public boolean hasPlan() {
      return plan_ != null;
    }
    /**
     * <pre>
     * (Required) The logical plan to get a hashCode.
     * </pre>
     *
     * <code>.spark.connect.Plan plan = 1;</code>
     * @return The plan.
     */
    @Override
    public org.apache.spark.connect.proto.Plan getPlan() {
      return plan_ == null ? org.apache.spark.connect.proto.Plan.getDefaultInstance() : plan_;
    }
    /**
     * <pre>
     * (Required) The logical plan to get a hashCode.
     * </pre>
     *
     * <code>.spark.connect.Plan plan = 1;</code>
     */
    @Override
    public org.apache.spark.connect.proto.PlanOrBuilder getPlanOrBuilder() {
      return plan_ == null ? org.apache.spark.connect.proto.Plan.getDefaultInstance() : plan_;
    }

    private byte memoizedIsInitialized = -1;
    @Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (plan_ != null) {
        output.writeMessage(1, getPlan());
      }
      getUnknownFields().writeTo(output);
    }

    @Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (plan_ != null) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, getPlan());
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @Override
    public boolean equals(final Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.connect.proto.AnalyzePlanRequest.SemanticHash)) {
        return super.equals(obj);
      }
      org.apache.spark.connect.proto.AnalyzePlanRequest.SemanticHash other = (org.apache.spark.connect.proto.AnalyzePlanRequest.SemanticHash) obj;

      if (hasPlan() != other.hasPlan()) return false;
      if (hasPlan()) {
        if (!getPlan()
            .equals(other.getPlan())) return false;
      }
      if (!getUnknownFields().equals(other.getUnknownFields())) return false;
      return true;
    }

    @Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasPlan()) {
        hash = (37 * hash) + PLAN_FIELD_NUMBER;
        hash = (53 * hash) + getPlan().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.connect.proto.AnalyzePlanRequest.SemanticHash parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.SemanticHash parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.SemanticHash parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.SemanticHash parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.SemanticHash parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.SemanticHash parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.SemanticHash parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.SemanticHash parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    public static org.apache.spark.connect.proto.AnalyzePlanRequest.SemanticHash parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }

    public static org.apache.spark.connect.proto.AnalyzePlanRequest.SemanticHash parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.SemanticHash parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.SemanticHash parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.connect.proto.AnalyzePlanRequest.SemanticHash prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @Override
    protected Builder newBuilderForType(
        BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code spark.connect.AnalyzePlanRequest.SemanticHash}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:spark.connect.AnalyzePlanRequest.SemanticHash)
        org.apache.spark.connect.proto.AnalyzePlanRequest.SemanticHashOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.connect.proto.Base.internal_static_spark_connect_AnalyzePlanRequest_SemanticHash_descriptor;
      }

      @Override
      protected FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.connect.proto.Base.internal_static_spark_connect_AnalyzePlanRequest_SemanticHash_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.connect.proto.AnalyzePlanRequest.SemanticHash.class, org.apache.spark.connect.proto.AnalyzePlanRequest.SemanticHash.Builder.class);
      }

      // Construct using org.apache.spark.connect.proto.AnalyzePlanRequest.SemanticHash.newBuilder()
      private Builder() {

      }

      private Builder(
          BuilderParent parent) {
        super(parent);

      }
      @Override
      public Builder clear() {
        super.clear();
        bitField0_ = 0;
        plan_ = null;
        if (planBuilder_ != null) {
          planBuilder_.dispose();
          planBuilder_ = null;
        }
        return this;
      }

      @Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.connect.proto.Base.internal_static_spark_connect_AnalyzePlanRequest_SemanticHash_descriptor;
      }

      @Override
      public org.apache.spark.connect.proto.AnalyzePlanRequest.SemanticHash getDefaultInstanceForType() {
        return org.apache.spark.connect.proto.AnalyzePlanRequest.SemanticHash.getDefaultInstance();
      }

      @Override
      public org.apache.spark.connect.proto.AnalyzePlanRequest.SemanticHash build() {
        org.apache.spark.connect.proto.AnalyzePlanRequest.SemanticHash result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @Override
      public org.apache.spark.connect.proto.AnalyzePlanRequest.SemanticHash buildPartial() {
        org.apache.spark.connect.proto.AnalyzePlanRequest.SemanticHash result = new org.apache.spark.connect.proto.AnalyzePlanRequest.SemanticHash(this);
        if (bitField0_ != 0) { buildPartial0(result); }
        onBuilt();
        return result;
      }

      private void buildPartial0(org.apache.spark.connect.proto.AnalyzePlanRequest.SemanticHash result) {
        int from_bitField0_ = bitField0_;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          result.plan_ = planBuilder_ == null
              ? plan_
              : planBuilder_.build();
        }
      }

      @Override
      public Builder clone() {
        return super.clone();
      }
      @Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          Object value) {
        return super.setField(field, value);
      }
      @Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          Object value) {
        return super.addRepeatedField(field, value);
      }
      @Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.connect.proto.AnalyzePlanRequest.SemanticHash) {
          return mergeFrom((org.apache.spark.connect.proto.AnalyzePlanRequest.SemanticHash)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.connect.proto.AnalyzePlanRequest.SemanticHash other) {
        if (other == org.apache.spark.connect.proto.AnalyzePlanRequest.SemanticHash.getDefaultInstance()) return this;
        if (other.hasPlan()) {
          mergePlan(other.getPlan());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        onChanged();
        return this;
      }

      @Override
      public final boolean isInitialized() {
        return true;
      }

      @Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        if (extensionRegistry == null) {
          throw new NullPointerException();
        }
        try {
          boolean done = false;
          while (!done) {
            int tag = input.readTag();
            switch (tag) {
              case 0:
                done = true;
                break;
              case 10: {
                input.readMessage(
                    getPlanFieldBuilder().getBuilder(),
                    extensionRegistry);
                bitField0_ |= 0x00000001;
                break;
              } // case 10
              default: {
                if (!super.parseUnknownField(input, extensionRegistry, tag)) {
                  done = true; // was an endgroup tag
                }
                break;
              } // default:
            } // switch (tag)
          } // while (!done)
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.unwrapIOException();
        } finally {
          onChanged();
        } // finally
        return this;
      }
      private int bitField0_;

      private org.apache.spark.connect.proto.Plan plan_;
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.connect.proto.Plan, org.apache.spark.connect.proto.Plan.Builder, org.apache.spark.connect.proto.PlanOrBuilder> planBuilder_;
      /**
       * <pre>
       * (Required) The logical plan to get a hashCode.
       * </pre>
       *
       * <code>.spark.connect.Plan plan = 1;</code>
       * @return Whether the plan field is set.
       */
      public boolean hasPlan() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <pre>
       * (Required) The logical plan to get a hashCode.
       * </pre>
       *
       * <code>.spark.connect.Plan plan = 1;</code>
       * @return The plan.
       */
      public org.apache.spark.connect.proto.Plan getPlan() {
        if (planBuilder_ == null) {
          return plan_ == null ? org.apache.spark.connect.proto.Plan.getDefaultInstance() : plan_;
        } else {
          return planBuilder_.getMessage();
        }
      }
      /**
       * <pre>
       * (Required) The logical plan to get a hashCode.
       * </pre>
       *
       * <code>.spark.connect.Plan plan = 1;</code>
       */
      public Builder setPlan(org.apache.spark.connect.proto.Plan value) {
        if (planBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          plan_ = value;
        } else {
          planBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * (Required) The logical plan to get a hashCode.
       * </pre>
       *
       * <code>.spark.connect.Plan plan = 1;</code>
       */
      public Builder setPlan(
          org.apache.spark.connect.proto.Plan.Builder builderForValue) {
        if (planBuilder_ == null) {
          plan_ = builderForValue.build();
        } else {
          planBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * (Required) The logical plan to get a hashCode.
       * </pre>
       *
       * <code>.spark.connect.Plan plan = 1;</code>
       */
      public Builder mergePlan(org.apache.spark.connect.proto.Plan value) {
        if (planBuilder_ == null) {
          if (((bitField0_ & 0x00000001) != 0) &&
            plan_ != null &&
            plan_ != org.apache.spark.connect.proto.Plan.getDefaultInstance()) {
            getPlanBuilder().mergeFrom(value);
          } else {
            plan_ = value;
          }
        } else {
          planBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * (Required) The logical plan to get a hashCode.
       * </pre>
       *
       * <code>.spark.connect.Plan plan = 1;</code>
       */
      public Builder clearPlan() {
        bitField0_ = (bitField0_ & ~0x00000001);
        plan_ = null;
        if (planBuilder_ != null) {
          planBuilder_.dispose();
          planBuilder_ = null;
        }
        onChanged();
        return this;
      }
      /**
       * <pre>
       * (Required) The logical plan to get a hashCode.
       * </pre>
       *
       * <code>.spark.connect.Plan plan = 1;</code>
       */
      public org.apache.spark.connect.proto.Plan.Builder getPlanBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getPlanFieldBuilder().getBuilder();
      }
      /**
       * <pre>
       * (Required) The logical plan to get a hashCode.
       * </pre>
       *
       * <code>.spark.connect.Plan plan = 1;</code>
       */
      public org.apache.spark.connect.proto.PlanOrBuilder getPlanOrBuilder() {
        if (planBuilder_ != null) {
          return planBuilder_.getMessageOrBuilder();
        } else {
          return plan_ == null ?
              org.apache.spark.connect.proto.Plan.getDefaultInstance() : plan_;
        }
      }
      /**
       * <pre>
       * (Required) The logical plan to get a hashCode.
       * </pre>
       *
       * <code>.spark.connect.Plan plan = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.connect.proto.Plan, org.apache.spark.connect.proto.Plan.Builder, org.apache.spark.connect.proto.PlanOrBuilder> 
          getPlanFieldBuilder() {
        if (planBuilder_ == null) {
          planBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.apache.spark.connect.proto.Plan, org.apache.spark.connect.proto.Plan.Builder, org.apache.spark.connect.proto.PlanOrBuilder>(
                  getPlan(),
                  getParentForChildren(),
                  isClean());
          plan_ = null;
        }
        return planBuilder_;
      }
      @Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:spark.connect.AnalyzePlanRequest.SemanticHash)
    }

    // @@protoc_insertion_point(class_scope:spark.connect.AnalyzePlanRequest.SemanticHash)
    private static final org.apache.spark.connect.proto.AnalyzePlanRequest.SemanticHash DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.connect.proto.AnalyzePlanRequest.SemanticHash();
    }

    public static org.apache.spark.connect.proto.AnalyzePlanRequest.SemanticHash getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<SemanticHash>
        PARSER = new com.google.protobuf.AbstractParser<SemanticHash>() {
      @Override
      public SemanticHash parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        Builder builder = newBuilder();
        try {
          builder.mergeFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.setUnfinishedMessage(builder.buildPartial());
        } catch (com.google.protobuf.UninitializedMessageException e) {
          throw e.asInvalidProtocolBufferException().setUnfinishedMessage(builder.buildPartial());
        } catch (java.io.IOException e) {
          throw new com.google.protobuf.InvalidProtocolBufferException(e)
              .setUnfinishedMessage(builder.buildPartial());
        }
        return builder.buildPartial();
      }
    };

    public static com.google.protobuf.Parser<SemanticHash> parser() {
      return PARSER;
    }

    @Override
    public com.google.protobuf.Parser<SemanticHash> getParserForType() {
      return PARSER;
    }

    @Override
    public org.apache.spark.connect.proto.AnalyzePlanRequest.SemanticHash getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface PersistOrBuilder extends
      // @@protoc_insertion_point(interface_extends:spark.connect.AnalyzePlanRequest.Persist)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <pre>
     * (Required) The logical plan to persist.
     * </pre>
     *
     * <code>.spark.connect.Relation relation = 1;</code>
     * @return Whether the relation field is set.
     */
    boolean hasRelation();
    /**
     * <pre>
     * (Required) The logical plan to persist.
     * </pre>
     *
     * <code>.spark.connect.Relation relation = 1;</code>
     * @return The relation.
     */
    org.apache.spark.connect.proto.Relation getRelation();
    /**
     * <pre>
     * (Required) The logical plan to persist.
     * </pre>
     *
     * <code>.spark.connect.Relation relation = 1;</code>
     */
    org.apache.spark.connect.proto.RelationOrBuilder getRelationOrBuilder();

    /**
     * <pre>
     * (Optional) The storage level.
     * </pre>
     *
     * <code>optional .spark.connect.StorageLevel storage_level = 2;</code>
     * @return Whether the storageLevel field is set.
     */
    boolean hasStorageLevel();
    /**
     * <pre>
     * (Optional) The storage level.
     * </pre>
     *
     * <code>optional .spark.connect.StorageLevel storage_level = 2;</code>
     * @return The storageLevel.
     */
    org.apache.spark.connect.proto.StorageLevel getStorageLevel();
    /**
     * <pre>
     * (Optional) The storage level.
     * </pre>
     *
     * <code>optional .spark.connect.StorageLevel storage_level = 2;</code>
     */
    org.apache.spark.connect.proto.StorageLevelOrBuilder getStorageLevelOrBuilder();
  }
  /**
   * Protobuf type {@code spark.connect.AnalyzePlanRequest.Persist}
   */
  public static final class Persist extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:spark.connect.AnalyzePlanRequest.Persist)
      PersistOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use Persist.newBuilder() to construct.
    private Persist(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private Persist() {
    }

    @Override
    @SuppressWarnings({"unused"})
    protected Object newInstance(
        UnusedPrivateParameter unused) {
      return new Persist();
    }

    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.connect.proto.Base.internal_static_spark_connect_AnalyzePlanRequest_Persist_descriptor;
    }

    @Override
    protected FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.connect.proto.Base.internal_static_spark_connect_AnalyzePlanRequest_Persist_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.connect.proto.AnalyzePlanRequest.Persist.class, org.apache.spark.connect.proto.AnalyzePlanRequest.Persist.Builder.class);
    }

    private int bitField0_;
    public static final int RELATION_FIELD_NUMBER = 1;
    private org.apache.spark.connect.proto.Relation relation_;
    /**
     * <pre>
     * (Required) The logical plan to persist.
     * </pre>
     *
     * <code>.spark.connect.Relation relation = 1;</code>
     * @return Whether the relation field is set.
     */
    @Override
    public boolean hasRelation() {
      return relation_ != null;
    }
    /**
     * <pre>
     * (Required) The logical plan to persist.
     * </pre>
     *
     * <code>.spark.connect.Relation relation = 1;</code>
     * @return The relation.
     */
    @Override
    public org.apache.spark.connect.proto.Relation getRelation() {
      return relation_ == null ? org.apache.spark.connect.proto.Relation.getDefaultInstance() : relation_;
    }
    /**
     * <pre>
     * (Required) The logical plan to persist.
     * </pre>
     *
     * <code>.spark.connect.Relation relation = 1;</code>
     */
    @Override
    public org.apache.spark.connect.proto.RelationOrBuilder getRelationOrBuilder() {
      return relation_ == null ? org.apache.spark.connect.proto.Relation.getDefaultInstance() : relation_;
    }

    public static final int STORAGE_LEVEL_FIELD_NUMBER = 2;
    private org.apache.spark.connect.proto.StorageLevel storageLevel_;
    /**
     * <pre>
     * (Optional) The storage level.
     * </pre>
     *
     * <code>optional .spark.connect.StorageLevel storage_level = 2;</code>
     * @return Whether the storageLevel field is set.
     */
    @Override
    public boolean hasStorageLevel() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <pre>
     * (Optional) The storage level.
     * </pre>
     *
     * <code>optional .spark.connect.StorageLevel storage_level = 2;</code>
     * @return The storageLevel.
     */
    @Override
    public org.apache.spark.connect.proto.StorageLevel getStorageLevel() {
      return storageLevel_ == null ? org.apache.spark.connect.proto.StorageLevel.getDefaultInstance() : storageLevel_;
    }
    /**
     * <pre>
     * (Optional) The storage level.
     * </pre>
     *
     * <code>optional .spark.connect.StorageLevel storage_level = 2;</code>
     */
    @Override
    public org.apache.spark.connect.proto.StorageLevelOrBuilder getStorageLevelOrBuilder() {
      return storageLevel_ == null ? org.apache.spark.connect.proto.StorageLevel.getDefaultInstance() : storageLevel_;
    }

    private byte memoizedIsInitialized = -1;
    @Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (relation_ != null) {
        output.writeMessage(1, getRelation());
      }
      if (((bitField0_ & 0x00000001) != 0)) {
        output.writeMessage(2, getStorageLevel());
      }
      getUnknownFields().writeTo(output);
    }

    @Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (relation_ != null) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, getRelation());
      }
      if (((bitField0_ & 0x00000001) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(2, getStorageLevel());
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @Override
    public boolean equals(final Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.connect.proto.AnalyzePlanRequest.Persist)) {
        return super.equals(obj);
      }
      org.apache.spark.connect.proto.AnalyzePlanRequest.Persist other = (org.apache.spark.connect.proto.AnalyzePlanRequest.Persist) obj;

      if (hasRelation() != other.hasRelation()) return false;
      if (hasRelation()) {
        if (!getRelation()
            .equals(other.getRelation())) return false;
      }
      if (hasStorageLevel() != other.hasStorageLevel()) return false;
      if (hasStorageLevel()) {
        if (!getStorageLevel()
            .equals(other.getStorageLevel())) return false;
      }
      if (!getUnknownFields().equals(other.getUnknownFields())) return false;
      return true;
    }

    @Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasRelation()) {
        hash = (37 * hash) + RELATION_FIELD_NUMBER;
        hash = (53 * hash) + getRelation().hashCode();
      }
      if (hasStorageLevel()) {
        hash = (37 * hash) + STORAGE_LEVEL_FIELD_NUMBER;
        hash = (53 * hash) + getStorageLevel().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.connect.proto.AnalyzePlanRequest.Persist parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.Persist parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.Persist parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.Persist parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.Persist parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.Persist parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.Persist parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.Persist parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    public static org.apache.spark.connect.proto.AnalyzePlanRequest.Persist parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }

    public static org.apache.spark.connect.proto.AnalyzePlanRequest.Persist parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.Persist parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.Persist parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.connect.proto.AnalyzePlanRequest.Persist prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @Override
    protected Builder newBuilderForType(
        BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code spark.connect.AnalyzePlanRequest.Persist}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:spark.connect.AnalyzePlanRequest.Persist)
        org.apache.spark.connect.proto.AnalyzePlanRequest.PersistOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.connect.proto.Base.internal_static_spark_connect_AnalyzePlanRequest_Persist_descriptor;
      }

      @Override
      protected FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.connect.proto.Base.internal_static_spark_connect_AnalyzePlanRequest_Persist_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.connect.proto.AnalyzePlanRequest.Persist.class, org.apache.spark.connect.proto.AnalyzePlanRequest.Persist.Builder.class);
      }

      // Construct using org.apache.spark.connect.proto.AnalyzePlanRequest.Persist.newBuilder()
      private Builder() {
        maybeForceBuilderInitialization();
      }

      private Builder(
          BuilderParent parent) {
        super(parent);
        maybeForceBuilderInitialization();
      }
      private void maybeForceBuilderInitialization() {
        if (com.google.protobuf.GeneratedMessageV3
                .alwaysUseFieldBuilders) {
          getRelationFieldBuilder();
          getStorageLevelFieldBuilder();
        }
      }
      @Override
      public Builder clear() {
        super.clear();
        bitField0_ = 0;
        relation_ = null;
        if (relationBuilder_ != null) {
          relationBuilder_.dispose();
          relationBuilder_ = null;
        }
        storageLevel_ = null;
        if (storageLevelBuilder_ != null) {
          storageLevelBuilder_.dispose();
          storageLevelBuilder_ = null;
        }
        return this;
      }

      @Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.connect.proto.Base.internal_static_spark_connect_AnalyzePlanRequest_Persist_descriptor;
      }

      @Override
      public org.apache.spark.connect.proto.AnalyzePlanRequest.Persist getDefaultInstanceForType() {
        return org.apache.spark.connect.proto.AnalyzePlanRequest.Persist.getDefaultInstance();
      }

      @Override
      public org.apache.spark.connect.proto.AnalyzePlanRequest.Persist build() {
        org.apache.spark.connect.proto.AnalyzePlanRequest.Persist result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @Override
      public org.apache.spark.connect.proto.AnalyzePlanRequest.Persist buildPartial() {
        org.apache.spark.connect.proto.AnalyzePlanRequest.Persist result = new org.apache.spark.connect.proto.AnalyzePlanRequest.Persist(this);
        if (bitField0_ != 0) { buildPartial0(result); }
        onBuilt();
        return result;
      }

      private void buildPartial0(org.apache.spark.connect.proto.AnalyzePlanRequest.Persist result) {
        int from_bitField0_ = bitField0_;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          result.relation_ = relationBuilder_ == null
              ? relation_
              : relationBuilder_.build();
        }
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000002) != 0)) {
          result.storageLevel_ = storageLevelBuilder_ == null
              ? storageLevel_
              : storageLevelBuilder_.build();
          to_bitField0_ |= 0x00000001;
        }
        result.bitField0_ |= to_bitField0_;
      }

      @Override
      public Builder clone() {
        return super.clone();
      }
      @Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          Object value) {
        return super.setField(field, value);
      }
      @Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          Object value) {
        return super.addRepeatedField(field, value);
      }
      @Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.connect.proto.AnalyzePlanRequest.Persist) {
          return mergeFrom((org.apache.spark.connect.proto.AnalyzePlanRequest.Persist)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.connect.proto.AnalyzePlanRequest.Persist other) {
        if (other == org.apache.spark.connect.proto.AnalyzePlanRequest.Persist.getDefaultInstance()) return this;
        if (other.hasRelation()) {
          mergeRelation(other.getRelation());
        }
        if (other.hasStorageLevel()) {
          mergeStorageLevel(other.getStorageLevel());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        onChanged();
        return this;
      }

      @Override
      public final boolean isInitialized() {
        return true;
      }

      @Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        if (extensionRegistry == null) {
          throw new NullPointerException();
        }
        try {
          boolean done = false;
          while (!done) {
            int tag = input.readTag();
            switch (tag) {
              case 0:
                done = true;
                break;
              case 10: {
                input.readMessage(
                    getRelationFieldBuilder().getBuilder(),
                    extensionRegistry);
                bitField0_ |= 0x00000001;
                break;
              } // case 10
              case 18: {
                input.readMessage(
                    getStorageLevelFieldBuilder().getBuilder(),
                    extensionRegistry);
                bitField0_ |= 0x00000002;
                break;
              } // case 18
              default: {
                if (!super.parseUnknownField(input, extensionRegistry, tag)) {
                  done = true; // was an endgroup tag
                }
                break;
              } // default:
            } // switch (tag)
          } // while (!done)
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.unwrapIOException();
        } finally {
          onChanged();
        } // finally
        return this;
      }
      private int bitField0_;

      private org.apache.spark.connect.proto.Relation relation_;
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.connect.proto.Relation, org.apache.spark.connect.proto.Relation.Builder, org.apache.spark.connect.proto.RelationOrBuilder> relationBuilder_;
      /**
       * <pre>
       * (Required) The logical plan to persist.
       * </pre>
       *
       * <code>.spark.connect.Relation relation = 1;</code>
       * @return Whether the relation field is set.
       */
      public boolean hasRelation() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <pre>
       * (Required) The logical plan to persist.
       * </pre>
       *
       * <code>.spark.connect.Relation relation = 1;</code>
       * @return The relation.
       */
      public org.apache.spark.connect.proto.Relation getRelation() {
        if (relationBuilder_ == null) {
          return relation_ == null ? org.apache.spark.connect.proto.Relation.getDefaultInstance() : relation_;
        } else {
          return relationBuilder_.getMessage();
        }
      }
      /**
       * <pre>
       * (Required) The logical plan to persist.
       * </pre>
       *
       * <code>.spark.connect.Relation relation = 1;</code>
       */
      public Builder setRelation(org.apache.spark.connect.proto.Relation value) {
        if (relationBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          relation_ = value;
        } else {
          relationBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * (Required) The logical plan to persist.
       * </pre>
       *
       * <code>.spark.connect.Relation relation = 1;</code>
       */
      public Builder setRelation(
          org.apache.spark.connect.proto.Relation.Builder builderForValue) {
        if (relationBuilder_ == null) {
          relation_ = builderForValue.build();
        } else {
          relationBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * (Required) The logical plan to persist.
       * </pre>
       *
       * <code>.spark.connect.Relation relation = 1;</code>
       */
      public Builder mergeRelation(org.apache.spark.connect.proto.Relation value) {
        if (relationBuilder_ == null) {
          if (((bitField0_ & 0x00000001) != 0) &&
            relation_ != null &&
            relation_ != org.apache.spark.connect.proto.Relation.getDefaultInstance()) {
            getRelationBuilder().mergeFrom(value);
          } else {
            relation_ = value;
          }
        } else {
          relationBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * (Required) The logical plan to persist.
       * </pre>
       *
       * <code>.spark.connect.Relation relation = 1;</code>
       */
      public Builder clearRelation() {
        bitField0_ = (bitField0_ & ~0x00000001);
        relation_ = null;
        if (relationBuilder_ != null) {
          relationBuilder_.dispose();
          relationBuilder_ = null;
        }
        onChanged();
        return this;
      }
      /**
       * <pre>
       * (Required) The logical plan to persist.
       * </pre>
       *
       * <code>.spark.connect.Relation relation = 1;</code>
       */
      public org.apache.spark.connect.proto.Relation.Builder getRelationBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getRelationFieldBuilder().getBuilder();
      }
      /**
       * <pre>
       * (Required) The logical plan to persist.
       * </pre>
       *
       * <code>.spark.connect.Relation relation = 1;</code>
       */
      public org.apache.spark.connect.proto.RelationOrBuilder getRelationOrBuilder() {
        if (relationBuilder_ != null) {
          return relationBuilder_.getMessageOrBuilder();
        } else {
          return relation_ == null ?
              org.apache.spark.connect.proto.Relation.getDefaultInstance() : relation_;
        }
      }
      /**
       * <pre>
       * (Required) The logical plan to persist.
       * </pre>
       *
       * <code>.spark.connect.Relation relation = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.connect.proto.Relation, org.apache.spark.connect.proto.Relation.Builder, org.apache.spark.connect.proto.RelationOrBuilder> 
          getRelationFieldBuilder() {
        if (relationBuilder_ == null) {
          relationBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.apache.spark.connect.proto.Relation, org.apache.spark.connect.proto.Relation.Builder, org.apache.spark.connect.proto.RelationOrBuilder>(
                  getRelation(),
                  getParentForChildren(),
                  isClean());
          relation_ = null;
        }
        return relationBuilder_;
      }

      private org.apache.spark.connect.proto.StorageLevel storageLevel_;
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.connect.proto.StorageLevel, org.apache.spark.connect.proto.StorageLevel.Builder, org.apache.spark.connect.proto.StorageLevelOrBuilder> storageLevelBuilder_;
      /**
       * <pre>
       * (Optional) The storage level.
       * </pre>
       *
       * <code>optional .spark.connect.StorageLevel storage_level = 2;</code>
       * @return Whether the storageLevel field is set.
       */
      public boolean hasStorageLevel() {
        return ((bitField0_ & 0x00000002) != 0);
      }
      /**
       * <pre>
       * (Optional) The storage level.
       * </pre>
       *
       * <code>optional .spark.connect.StorageLevel storage_level = 2;</code>
       * @return The storageLevel.
       */
      public org.apache.spark.connect.proto.StorageLevel getStorageLevel() {
        if (storageLevelBuilder_ == null) {
          return storageLevel_ == null ? org.apache.spark.connect.proto.StorageLevel.getDefaultInstance() : storageLevel_;
        } else {
          return storageLevelBuilder_.getMessage();
        }
      }
      /**
       * <pre>
       * (Optional) The storage level.
       * </pre>
       *
       * <code>optional .spark.connect.StorageLevel storage_level = 2;</code>
       */
      public Builder setStorageLevel(org.apache.spark.connect.proto.StorageLevel value) {
        if (storageLevelBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          storageLevel_ = value;
        } else {
          storageLevelBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000002;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * (Optional) The storage level.
       * </pre>
       *
       * <code>optional .spark.connect.StorageLevel storage_level = 2;</code>
       */
      public Builder setStorageLevel(
          org.apache.spark.connect.proto.StorageLevel.Builder builderForValue) {
        if (storageLevelBuilder_ == null) {
          storageLevel_ = builderForValue.build();
        } else {
          storageLevelBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000002;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * (Optional) The storage level.
       * </pre>
       *
       * <code>optional .spark.connect.StorageLevel storage_level = 2;</code>
       */
      public Builder mergeStorageLevel(org.apache.spark.connect.proto.StorageLevel value) {
        if (storageLevelBuilder_ == null) {
          if (((bitField0_ & 0x00000002) != 0) &&
            storageLevel_ != null &&
            storageLevel_ != org.apache.spark.connect.proto.StorageLevel.getDefaultInstance()) {
            getStorageLevelBuilder().mergeFrom(value);
          } else {
            storageLevel_ = value;
          }
        } else {
          storageLevelBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000002;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * (Optional) The storage level.
       * </pre>
       *
       * <code>optional .spark.connect.StorageLevel storage_level = 2;</code>
       */
      public Builder clearStorageLevel() {
        bitField0_ = (bitField0_ & ~0x00000002);
        storageLevel_ = null;
        if (storageLevelBuilder_ != null) {
          storageLevelBuilder_.dispose();
          storageLevelBuilder_ = null;
        }
        onChanged();
        return this;
      }
      /**
       * <pre>
       * (Optional) The storage level.
       * </pre>
       *
       * <code>optional .spark.connect.StorageLevel storage_level = 2;</code>
       */
      public org.apache.spark.connect.proto.StorageLevel.Builder getStorageLevelBuilder() {
        bitField0_ |= 0x00000002;
        onChanged();
        return getStorageLevelFieldBuilder().getBuilder();
      }
      /**
       * <pre>
       * (Optional) The storage level.
       * </pre>
       *
       * <code>optional .spark.connect.StorageLevel storage_level = 2;</code>
       */
      public org.apache.spark.connect.proto.StorageLevelOrBuilder getStorageLevelOrBuilder() {
        if (storageLevelBuilder_ != null) {
          return storageLevelBuilder_.getMessageOrBuilder();
        } else {
          return storageLevel_ == null ?
              org.apache.spark.connect.proto.StorageLevel.getDefaultInstance() : storageLevel_;
        }
      }
      /**
       * <pre>
       * (Optional) The storage level.
       * </pre>
       *
       * <code>optional .spark.connect.StorageLevel storage_level = 2;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.connect.proto.StorageLevel, org.apache.spark.connect.proto.StorageLevel.Builder, org.apache.spark.connect.proto.StorageLevelOrBuilder> 
          getStorageLevelFieldBuilder() {
        if (storageLevelBuilder_ == null) {
          storageLevelBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.apache.spark.connect.proto.StorageLevel, org.apache.spark.connect.proto.StorageLevel.Builder, org.apache.spark.connect.proto.StorageLevelOrBuilder>(
                  getStorageLevel(),
                  getParentForChildren(),
                  isClean());
          storageLevel_ = null;
        }
        return storageLevelBuilder_;
      }
      @Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:spark.connect.AnalyzePlanRequest.Persist)
    }

    // @@protoc_insertion_point(class_scope:spark.connect.AnalyzePlanRequest.Persist)
    private static final org.apache.spark.connect.proto.AnalyzePlanRequest.Persist DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.connect.proto.AnalyzePlanRequest.Persist();
    }

    public static org.apache.spark.connect.proto.AnalyzePlanRequest.Persist getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<Persist>
        PARSER = new com.google.protobuf.AbstractParser<Persist>() {
      @Override
      public Persist parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        Builder builder = newBuilder();
        try {
          builder.mergeFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.setUnfinishedMessage(builder.buildPartial());
        } catch (com.google.protobuf.UninitializedMessageException e) {
          throw e.asInvalidProtocolBufferException().setUnfinishedMessage(builder.buildPartial());
        } catch (java.io.IOException e) {
          throw new com.google.protobuf.InvalidProtocolBufferException(e)
              .setUnfinishedMessage(builder.buildPartial());
        }
        return builder.buildPartial();
      }
    };

    public static com.google.protobuf.Parser<Persist> parser() {
      return PARSER;
    }

    @Override
    public com.google.protobuf.Parser<Persist> getParserForType() {
      return PARSER;
    }

    @Override
    public org.apache.spark.connect.proto.AnalyzePlanRequest.Persist getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface UnpersistOrBuilder extends
      // @@protoc_insertion_point(interface_extends:spark.connect.AnalyzePlanRequest.Unpersist)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <pre>
     * (Required) The logical plan to unpersist.
     * </pre>
     *
     * <code>.spark.connect.Relation relation = 1;</code>
     * @return Whether the relation field is set.
     */
    boolean hasRelation();
    /**
     * <pre>
     * (Required) The logical plan to unpersist.
     * </pre>
     *
     * <code>.spark.connect.Relation relation = 1;</code>
     * @return The relation.
     */
    org.apache.spark.connect.proto.Relation getRelation();
    /**
     * <pre>
     * (Required) The logical plan to unpersist.
     * </pre>
     *
     * <code>.spark.connect.Relation relation = 1;</code>
     */
    org.apache.spark.connect.proto.RelationOrBuilder getRelationOrBuilder();

    /**
     * <pre>
     * (Optional) Whether to block until all blocks are deleted.
     * </pre>
     *
     * <code>optional bool blocking = 2;</code>
     * @return Whether the blocking field is set.
     */
    boolean hasBlocking();
    /**
     * <pre>
     * (Optional) Whether to block until all blocks are deleted.
     * </pre>
     *
     * <code>optional bool blocking = 2;</code>
     * @return The blocking.
     */
    boolean getBlocking();
  }
  /**
   * Protobuf type {@code spark.connect.AnalyzePlanRequest.Unpersist}
   */
  public static final class Unpersist extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:spark.connect.AnalyzePlanRequest.Unpersist)
      UnpersistOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use Unpersist.newBuilder() to construct.
    private Unpersist(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private Unpersist() {
    }

    @Override
    @SuppressWarnings({"unused"})
    protected Object newInstance(
        UnusedPrivateParameter unused) {
      return new Unpersist();
    }

    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.connect.proto.Base.internal_static_spark_connect_AnalyzePlanRequest_Unpersist_descriptor;
    }

    @Override
    protected FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.connect.proto.Base.internal_static_spark_connect_AnalyzePlanRequest_Unpersist_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.connect.proto.AnalyzePlanRequest.Unpersist.class, org.apache.spark.connect.proto.AnalyzePlanRequest.Unpersist.Builder.class);
    }

    private int bitField0_;
    public static final int RELATION_FIELD_NUMBER = 1;
    private org.apache.spark.connect.proto.Relation relation_;
    /**
     * <pre>
     * (Required) The logical plan to unpersist.
     * </pre>
     *
     * <code>.spark.connect.Relation relation = 1;</code>
     * @return Whether the relation field is set.
     */
    @Override
    public boolean hasRelation() {
      return relation_ != null;
    }
    /**
     * <pre>
     * (Required) The logical plan to unpersist.
     * </pre>
     *
     * <code>.spark.connect.Relation relation = 1;</code>
     * @return The relation.
     */
    @Override
    public org.apache.spark.connect.proto.Relation getRelation() {
      return relation_ == null ? org.apache.spark.connect.proto.Relation.getDefaultInstance() : relation_;
    }
    /**
     * <pre>
     * (Required) The logical plan to unpersist.
     * </pre>
     *
     * <code>.spark.connect.Relation relation = 1;</code>
     */
    @Override
    public org.apache.spark.connect.proto.RelationOrBuilder getRelationOrBuilder() {
      return relation_ == null ? org.apache.spark.connect.proto.Relation.getDefaultInstance() : relation_;
    }

    public static final int BLOCKING_FIELD_NUMBER = 2;
    private boolean blocking_ = false;
    /**
     * <pre>
     * (Optional) Whether to block until all blocks are deleted.
     * </pre>
     *
     * <code>optional bool blocking = 2;</code>
     * @return Whether the blocking field is set.
     */
    @Override
    public boolean hasBlocking() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <pre>
     * (Optional) Whether to block until all blocks are deleted.
     * </pre>
     *
     * <code>optional bool blocking = 2;</code>
     * @return The blocking.
     */
    @Override
    public boolean getBlocking() {
      return blocking_;
    }

    private byte memoizedIsInitialized = -1;
    @Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (relation_ != null) {
        output.writeMessage(1, getRelation());
      }
      if (((bitField0_ & 0x00000001) != 0)) {
        output.writeBool(2, blocking_);
      }
      getUnknownFields().writeTo(output);
    }

    @Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (relation_ != null) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, getRelation());
      }
      if (((bitField0_ & 0x00000001) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeBoolSize(2, blocking_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @Override
    public boolean equals(final Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.connect.proto.AnalyzePlanRequest.Unpersist)) {
        return super.equals(obj);
      }
      org.apache.spark.connect.proto.AnalyzePlanRequest.Unpersist other = (org.apache.spark.connect.proto.AnalyzePlanRequest.Unpersist) obj;

      if (hasRelation() != other.hasRelation()) return false;
      if (hasRelation()) {
        if (!getRelation()
            .equals(other.getRelation())) return false;
      }
      if (hasBlocking() != other.hasBlocking()) return false;
      if (hasBlocking()) {
        if (getBlocking()
            != other.getBlocking()) return false;
      }
      if (!getUnknownFields().equals(other.getUnknownFields())) return false;
      return true;
    }

    @Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasRelation()) {
        hash = (37 * hash) + RELATION_FIELD_NUMBER;
        hash = (53 * hash) + getRelation().hashCode();
      }
      if (hasBlocking()) {
        hash = (37 * hash) + BLOCKING_FIELD_NUMBER;
        hash = (53 * hash) + com.google.protobuf.Internal.hashBoolean(
            getBlocking());
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.connect.proto.AnalyzePlanRequest.Unpersist parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.Unpersist parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.Unpersist parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.Unpersist parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.Unpersist parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.Unpersist parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.Unpersist parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.Unpersist parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    public static org.apache.spark.connect.proto.AnalyzePlanRequest.Unpersist parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }

    public static org.apache.spark.connect.proto.AnalyzePlanRequest.Unpersist parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.Unpersist parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.Unpersist parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.connect.proto.AnalyzePlanRequest.Unpersist prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @Override
    protected Builder newBuilderForType(
        BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code spark.connect.AnalyzePlanRequest.Unpersist}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:spark.connect.AnalyzePlanRequest.Unpersist)
        org.apache.spark.connect.proto.AnalyzePlanRequest.UnpersistOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.connect.proto.Base.internal_static_spark_connect_AnalyzePlanRequest_Unpersist_descriptor;
      }

      @Override
      protected FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.connect.proto.Base.internal_static_spark_connect_AnalyzePlanRequest_Unpersist_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.connect.proto.AnalyzePlanRequest.Unpersist.class, org.apache.spark.connect.proto.AnalyzePlanRequest.Unpersist.Builder.class);
      }

      // Construct using org.apache.spark.connect.proto.AnalyzePlanRequest.Unpersist.newBuilder()
      private Builder() {

      }

      private Builder(
          BuilderParent parent) {
        super(parent);

      }
      @Override
      public Builder clear() {
        super.clear();
        bitField0_ = 0;
        relation_ = null;
        if (relationBuilder_ != null) {
          relationBuilder_.dispose();
          relationBuilder_ = null;
        }
        blocking_ = false;
        return this;
      }

      @Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.connect.proto.Base.internal_static_spark_connect_AnalyzePlanRequest_Unpersist_descriptor;
      }

      @Override
      public org.apache.spark.connect.proto.AnalyzePlanRequest.Unpersist getDefaultInstanceForType() {
        return org.apache.spark.connect.proto.AnalyzePlanRequest.Unpersist.getDefaultInstance();
      }

      @Override
      public org.apache.spark.connect.proto.AnalyzePlanRequest.Unpersist build() {
        org.apache.spark.connect.proto.AnalyzePlanRequest.Unpersist result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @Override
      public org.apache.spark.connect.proto.AnalyzePlanRequest.Unpersist buildPartial() {
        org.apache.spark.connect.proto.AnalyzePlanRequest.Unpersist result = new org.apache.spark.connect.proto.AnalyzePlanRequest.Unpersist(this);
        if (bitField0_ != 0) { buildPartial0(result); }
        onBuilt();
        return result;
      }

      private void buildPartial0(org.apache.spark.connect.proto.AnalyzePlanRequest.Unpersist result) {
        int from_bitField0_ = bitField0_;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          result.relation_ = relationBuilder_ == null
              ? relation_
              : relationBuilder_.build();
        }
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000002) != 0)) {
          result.blocking_ = blocking_;
          to_bitField0_ |= 0x00000001;
        }
        result.bitField0_ |= to_bitField0_;
      }

      @Override
      public Builder clone() {
        return super.clone();
      }
      @Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          Object value) {
        return super.setField(field, value);
      }
      @Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          Object value) {
        return super.addRepeatedField(field, value);
      }
      @Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.connect.proto.AnalyzePlanRequest.Unpersist) {
          return mergeFrom((org.apache.spark.connect.proto.AnalyzePlanRequest.Unpersist)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.connect.proto.AnalyzePlanRequest.Unpersist other) {
        if (other == org.apache.spark.connect.proto.AnalyzePlanRequest.Unpersist.getDefaultInstance()) return this;
        if (other.hasRelation()) {
          mergeRelation(other.getRelation());
        }
        if (other.hasBlocking()) {
          setBlocking(other.getBlocking());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        onChanged();
        return this;
      }

      @Override
      public final boolean isInitialized() {
        return true;
      }

      @Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        if (extensionRegistry == null) {
          throw new NullPointerException();
        }
        try {
          boolean done = false;
          while (!done) {
            int tag = input.readTag();
            switch (tag) {
              case 0:
                done = true;
                break;
              case 10: {
                input.readMessage(
                    getRelationFieldBuilder().getBuilder(),
                    extensionRegistry);
                bitField0_ |= 0x00000001;
                break;
              } // case 10
              case 16: {
                blocking_ = input.readBool();
                bitField0_ |= 0x00000002;
                break;
              } // case 16
              default: {
                if (!super.parseUnknownField(input, extensionRegistry, tag)) {
                  done = true; // was an endgroup tag
                }
                break;
              } // default:
            } // switch (tag)
          } // while (!done)
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.unwrapIOException();
        } finally {
          onChanged();
        } // finally
        return this;
      }
      private int bitField0_;

      private org.apache.spark.connect.proto.Relation relation_;
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.connect.proto.Relation, org.apache.spark.connect.proto.Relation.Builder, org.apache.spark.connect.proto.RelationOrBuilder> relationBuilder_;
      /**
       * <pre>
       * (Required) The logical plan to unpersist.
       * </pre>
       *
       * <code>.spark.connect.Relation relation = 1;</code>
       * @return Whether the relation field is set.
       */
      public boolean hasRelation() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <pre>
       * (Required) The logical plan to unpersist.
       * </pre>
       *
       * <code>.spark.connect.Relation relation = 1;</code>
       * @return The relation.
       */
      public org.apache.spark.connect.proto.Relation getRelation() {
        if (relationBuilder_ == null) {
          return relation_ == null ? org.apache.spark.connect.proto.Relation.getDefaultInstance() : relation_;
        } else {
          return relationBuilder_.getMessage();
        }
      }
      /**
       * <pre>
       * (Required) The logical plan to unpersist.
       * </pre>
       *
       * <code>.spark.connect.Relation relation = 1;</code>
       */
      public Builder setRelation(org.apache.spark.connect.proto.Relation value) {
        if (relationBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          relation_ = value;
        } else {
          relationBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * (Required) The logical plan to unpersist.
       * </pre>
       *
       * <code>.spark.connect.Relation relation = 1;</code>
       */
      public Builder setRelation(
          org.apache.spark.connect.proto.Relation.Builder builderForValue) {
        if (relationBuilder_ == null) {
          relation_ = builderForValue.build();
        } else {
          relationBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * (Required) The logical plan to unpersist.
       * </pre>
       *
       * <code>.spark.connect.Relation relation = 1;</code>
       */
      public Builder mergeRelation(org.apache.spark.connect.proto.Relation value) {
        if (relationBuilder_ == null) {
          if (((bitField0_ & 0x00000001) != 0) &&
            relation_ != null &&
            relation_ != org.apache.spark.connect.proto.Relation.getDefaultInstance()) {
            getRelationBuilder().mergeFrom(value);
          } else {
            relation_ = value;
          }
        } else {
          relationBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * (Required) The logical plan to unpersist.
       * </pre>
       *
       * <code>.spark.connect.Relation relation = 1;</code>
       */
      public Builder clearRelation() {
        bitField0_ = (bitField0_ & ~0x00000001);
        relation_ = null;
        if (relationBuilder_ != null) {
          relationBuilder_.dispose();
          relationBuilder_ = null;
        }
        onChanged();
        return this;
      }
      /**
       * <pre>
       * (Required) The logical plan to unpersist.
       * </pre>
       *
       * <code>.spark.connect.Relation relation = 1;</code>
       */
      public org.apache.spark.connect.proto.Relation.Builder getRelationBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getRelationFieldBuilder().getBuilder();
      }
      /**
       * <pre>
       * (Required) The logical plan to unpersist.
       * </pre>
       *
       * <code>.spark.connect.Relation relation = 1;</code>
       */
      public org.apache.spark.connect.proto.RelationOrBuilder getRelationOrBuilder() {
        if (relationBuilder_ != null) {
          return relationBuilder_.getMessageOrBuilder();
        } else {
          return relation_ == null ?
              org.apache.spark.connect.proto.Relation.getDefaultInstance() : relation_;
        }
      }
      /**
       * <pre>
       * (Required) The logical plan to unpersist.
       * </pre>
       *
       * <code>.spark.connect.Relation relation = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.connect.proto.Relation, org.apache.spark.connect.proto.Relation.Builder, org.apache.spark.connect.proto.RelationOrBuilder> 
          getRelationFieldBuilder() {
        if (relationBuilder_ == null) {
          relationBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.apache.spark.connect.proto.Relation, org.apache.spark.connect.proto.Relation.Builder, org.apache.spark.connect.proto.RelationOrBuilder>(
                  getRelation(),
                  getParentForChildren(),
                  isClean());
          relation_ = null;
        }
        return relationBuilder_;
      }

      private boolean blocking_ ;
      /**
       * <pre>
       * (Optional) Whether to block until all blocks are deleted.
       * </pre>
       *
       * <code>optional bool blocking = 2;</code>
       * @return Whether the blocking field is set.
       */
      @Override
      public boolean hasBlocking() {
        return ((bitField0_ & 0x00000002) != 0);
      }
      /**
       * <pre>
       * (Optional) Whether to block until all blocks are deleted.
       * </pre>
       *
       * <code>optional bool blocking = 2;</code>
       * @return The blocking.
       */
      @Override
      public boolean getBlocking() {
        return blocking_;
      }
      /**
       * <pre>
       * (Optional) Whether to block until all blocks are deleted.
       * </pre>
       *
       * <code>optional bool blocking = 2;</code>
       * @param value The blocking to set.
       * @return This builder for chaining.
       */
      public Builder setBlocking(boolean value) {

        blocking_ = value;
        bitField0_ |= 0x00000002;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * (Optional) Whether to block until all blocks are deleted.
       * </pre>
       *
       * <code>optional bool blocking = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearBlocking() {
        bitField0_ = (bitField0_ & ~0x00000002);
        blocking_ = false;
        onChanged();
        return this;
      }
      @Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:spark.connect.AnalyzePlanRequest.Unpersist)
    }

    // @@protoc_insertion_point(class_scope:spark.connect.AnalyzePlanRequest.Unpersist)
    private static final org.apache.spark.connect.proto.AnalyzePlanRequest.Unpersist DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.connect.proto.AnalyzePlanRequest.Unpersist();
    }

    public static org.apache.spark.connect.proto.AnalyzePlanRequest.Unpersist getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<Unpersist>
        PARSER = new com.google.protobuf.AbstractParser<Unpersist>() {
      @Override
      public Unpersist parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        Builder builder = newBuilder();
        try {
          builder.mergeFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.setUnfinishedMessage(builder.buildPartial());
        } catch (com.google.protobuf.UninitializedMessageException e) {
          throw e.asInvalidProtocolBufferException().setUnfinishedMessage(builder.buildPartial());
        } catch (java.io.IOException e) {
          throw new com.google.protobuf.InvalidProtocolBufferException(e)
              .setUnfinishedMessage(builder.buildPartial());
        }
        return builder.buildPartial();
      }
    };

    public static com.google.protobuf.Parser<Unpersist> parser() {
      return PARSER;
    }

    @Override
    public com.google.protobuf.Parser<Unpersist> getParserForType() {
      return PARSER;
    }

    @Override
    public org.apache.spark.connect.proto.AnalyzePlanRequest.Unpersist getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface GetStorageLevelOrBuilder extends
      // @@protoc_insertion_point(interface_extends:spark.connect.AnalyzePlanRequest.GetStorageLevel)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <pre>
     * (Required) The logical plan to get the storage level.
     * </pre>
     *
     * <code>.spark.connect.Relation relation = 1;</code>
     * @return Whether the relation field is set.
     */
    boolean hasRelation();
    /**
     * <pre>
     * (Required) The logical plan to get the storage level.
     * </pre>
     *
     * <code>.spark.connect.Relation relation = 1;</code>
     * @return The relation.
     */
    org.apache.spark.connect.proto.Relation getRelation();
    /**
     * <pre>
     * (Required) The logical plan to get the storage level.
     * </pre>
     *
     * <code>.spark.connect.Relation relation = 1;</code>
     */
    org.apache.spark.connect.proto.RelationOrBuilder getRelationOrBuilder();
  }
  /**
   * Protobuf type {@code spark.connect.AnalyzePlanRequest.GetStorageLevel}
   */
  public static final class GetStorageLevel extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:spark.connect.AnalyzePlanRequest.GetStorageLevel)
      GetStorageLevelOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use GetStorageLevel.newBuilder() to construct.
    private GetStorageLevel(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private GetStorageLevel() {
    }

    @Override
    @SuppressWarnings({"unused"})
    protected Object newInstance(
        UnusedPrivateParameter unused) {
      return new GetStorageLevel();
    }

    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.connect.proto.Base.internal_static_spark_connect_AnalyzePlanRequest_GetStorageLevel_descriptor;
    }

    @Override
    protected FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.connect.proto.Base.internal_static_spark_connect_AnalyzePlanRequest_GetStorageLevel_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.connect.proto.AnalyzePlanRequest.GetStorageLevel.class, org.apache.spark.connect.proto.AnalyzePlanRequest.GetStorageLevel.Builder.class);
    }

    public static final int RELATION_FIELD_NUMBER = 1;
    private org.apache.spark.connect.proto.Relation relation_;
    /**
     * <pre>
     * (Required) The logical plan to get the storage level.
     * </pre>
     *
     * <code>.spark.connect.Relation relation = 1;</code>
     * @return Whether the relation field is set.
     */
    @Override
    public boolean hasRelation() {
      return relation_ != null;
    }
    /**
     * <pre>
     * (Required) The logical plan to get the storage level.
     * </pre>
     *
     * <code>.spark.connect.Relation relation = 1;</code>
     * @return The relation.
     */
    @Override
    public org.apache.spark.connect.proto.Relation getRelation() {
      return relation_ == null ? org.apache.spark.connect.proto.Relation.getDefaultInstance() : relation_;
    }
    /**
     * <pre>
     * (Required) The logical plan to get the storage level.
     * </pre>
     *
     * <code>.spark.connect.Relation relation = 1;</code>
     */
    @Override
    public org.apache.spark.connect.proto.RelationOrBuilder getRelationOrBuilder() {
      return relation_ == null ? org.apache.spark.connect.proto.Relation.getDefaultInstance() : relation_;
    }

    private byte memoizedIsInitialized = -1;
    @Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (relation_ != null) {
        output.writeMessage(1, getRelation());
      }
      getUnknownFields().writeTo(output);
    }

    @Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (relation_ != null) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, getRelation());
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @Override
    public boolean equals(final Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.connect.proto.AnalyzePlanRequest.GetStorageLevel)) {
        return super.equals(obj);
      }
      org.apache.spark.connect.proto.AnalyzePlanRequest.GetStorageLevel other = (org.apache.spark.connect.proto.AnalyzePlanRequest.GetStorageLevel) obj;

      if (hasRelation() != other.hasRelation()) return false;
      if (hasRelation()) {
        if (!getRelation()
            .equals(other.getRelation())) return false;
      }
      if (!getUnknownFields().equals(other.getUnknownFields())) return false;
      return true;
    }

    @Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasRelation()) {
        hash = (37 * hash) + RELATION_FIELD_NUMBER;
        hash = (53 * hash) + getRelation().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.connect.proto.AnalyzePlanRequest.GetStorageLevel parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.GetStorageLevel parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.GetStorageLevel parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.GetStorageLevel parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.GetStorageLevel parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.GetStorageLevel parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.GetStorageLevel parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.GetStorageLevel parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    public static org.apache.spark.connect.proto.AnalyzePlanRequest.GetStorageLevel parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }

    public static org.apache.spark.connect.proto.AnalyzePlanRequest.GetStorageLevel parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.GetStorageLevel parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.connect.proto.AnalyzePlanRequest.GetStorageLevel parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.connect.proto.AnalyzePlanRequest.GetStorageLevel prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @Override
    protected Builder newBuilderForType(
        BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code spark.connect.AnalyzePlanRequest.GetStorageLevel}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:spark.connect.AnalyzePlanRequest.GetStorageLevel)
        org.apache.spark.connect.proto.AnalyzePlanRequest.GetStorageLevelOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.connect.proto.Base.internal_static_spark_connect_AnalyzePlanRequest_GetStorageLevel_descriptor;
      }

      @Override
      protected FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.connect.proto.Base.internal_static_spark_connect_AnalyzePlanRequest_GetStorageLevel_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.connect.proto.AnalyzePlanRequest.GetStorageLevel.class, org.apache.spark.connect.proto.AnalyzePlanRequest.GetStorageLevel.Builder.class);
      }

      // Construct using org.apache.spark.connect.proto.AnalyzePlanRequest.GetStorageLevel.newBuilder()
      private Builder() {

      }

      private Builder(
          BuilderParent parent) {
        super(parent);

      }
      @Override
      public Builder clear() {
        super.clear();
        bitField0_ = 0;
        relation_ = null;
        if (relationBuilder_ != null) {
          relationBuilder_.dispose();
          relationBuilder_ = null;
        }
        return this;
      }

      @Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.connect.proto.Base.internal_static_spark_connect_AnalyzePlanRequest_GetStorageLevel_descriptor;
      }

      @Override
      public org.apache.spark.connect.proto.AnalyzePlanRequest.GetStorageLevel getDefaultInstanceForType() {
        return org.apache.spark.connect.proto.AnalyzePlanRequest.GetStorageLevel.getDefaultInstance();
      }

      @Override
      public org.apache.spark.connect.proto.AnalyzePlanRequest.GetStorageLevel build() {
        org.apache.spark.connect.proto.AnalyzePlanRequest.GetStorageLevel result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @Override
      public org.apache.spark.connect.proto.AnalyzePlanRequest.GetStorageLevel buildPartial() {
        org.apache.spark.connect.proto.AnalyzePlanRequest.GetStorageLevel result = new org.apache.spark.connect.proto.AnalyzePlanRequest.GetStorageLevel(this);
        if (bitField0_ != 0) { buildPartial0(result); }
        onBuilt();
        return result;
      }

      private void buildPartial0(org.apache.spark.connect.proto.AnalyzePlanRequest.GetStorageLevel result) {
        int from_bitField0_ = bitField0_;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          result.relation_ = relationBuilder_ == null
              ? relation_
              : relationBuilder_.build();
        }
      }

      @Override
      public Builder clone() {
        return super.clone();
      }
      @Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          Object value) {
        return super.setField(field, value);
      }
      @Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          Object value) {
        return super.addRepeatedField(field, value);
      }
      @Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.connect.proto.AnalyzePlanRequest.GetStorageLevel) {
          return mergeFrom((org.apache.spark.connect.proto.AnalyzePlanRequest.GetStorageLevel)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.connect.proto.AnalyzePlanRequest.GetStorageLevel other) {
        if (other == org.apache.spark.connect.proto.AnalyzePlanRequest.GetStorageLevel.getDefaultInstance()) return this;
        if (other.hasRelation()) {
          mergeRelation(other.getRelation());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        onChanged();
        return this;
      }

      @Override
      public final boolean isInitialized() {
        return true;
      }

      @Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        if (extensionRegistry == null) {
          throw new NullPointerException();
        }
        try {
          boolean done = false;
          while (!done) {
            int tag = input.readTag();
            switch (tag) {
              case 0:
                done = true;
                break;
              case 10: {
                input.readMessage(
                    getRelationFieldBuilder().getBuilder(),
                    extensionRegistry);
                bitField0_ |= 0x00000001;
                break;
              } // case 10
              default: {
                if (!super.parseUnknownField(input, extensionRegistry, tag)) {
                  done = true; // was an endgroup tag
                }
                break;
              } // default:
            } // switch (tag)
          } // while (!done)
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.unwrapIOException();
        } finally {
          onChanged();
        } // finally
        return this;
      }
      private int bitField0_;

      private org.apache.spark.connect.proto.Relation relation_;
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.connect.proto.Relation, org.apache.spark.connect.proto.Relation.Builder, org.apache.spark.connect.proto.RelationOrBuilder> relationBuilder_;
      /**
       * <pre>
       * (Required) The logical plan to get the storage level.
       * </pre>
       *
       * <code>.spark.connect.Relation relation = 1;</code>
       * @return Whether the relation field is set.
       */
      public boolean hasRelation() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <pre>
       * (Required) The logical plan to get the storage level.
       * </pre>
       *
       * <code>.spark.connect.Relation relation = 1;</code>
       * @return The relation.
       */
      public org.apache.spark.connect.proto.Relation getRelation() {
        if (relationBuilder_ == null) {
          return relation_ == null ? org.apache.spark.connect.proto.Relation.getDefaultInstance() : relation_;
        } else {
          return relationBuilder_.getMessage();
        }
      }
      /**
       * <pre>
       * (Required) The logical plan to get the storage level.
       * </pre>
       *
       * <code>.spark.connect.Relation relation = 1;</code>
       */
      public Builder setRelation(org.apache.spark.connect.proto.Relation value) {
        if (relationBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          relation_ = value;
        } else {
          relationBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * (Required) The logical plan to get the storage level.
       * </pre>
       *
       * <code>.spark.connect.Relation relation = 1;</code>
       */
      public Builder setRelation(
          org.apache.spark.connect.proto.Relation.Builder builderForValue) {
        if (relationBuilder_ == null) {
          relation_ = builderForValue.build();
        } else {
          relationBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * (Required) The logical plan to get the storage level.
       * </pre>
       *
       * <code>.spark.connect.Relation relation = 1;</code>
       */
      public Builder mergeRelation(org.apache.spark.connect.proto.Relation value) {
        if (relationBuilder_ == null) {
          if (((bitField0_ & 0x00000001) != 0) &&
            relation_ != null &&
            relation_ != org.apache.spark.connect.proto.Relation.getDefaultInstance()) {
            getRelationBuilder().mergeFrom(value);
          } else {
            relation_ = value;
          }
        } else {
          relationBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * (Required) The logical plan to get the storage level.
       * </pre>
       *
       * <code>.spark.connect.Relation relation = 1;</code>
       */
      public Builder clearRelation() {
        bitField0_ = (bitField0_ & ~0x00000001);
        relation_ = null;
        if (relationBuilder_ != null) {
          relationBuilder_.dispose();
          relationBuilder_ = null;
        }
        onChanged();
        return this;
      }
      /**
       * <pre>
       * (Required) The logical plan to get the storage level.
       * </pre>
       *
       * <code>.spark.connect.Relation relation = 1;</code>
       */
      public org.apache.spark.connect.proto.Relation.Builder getRelationBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getRelationFieldBuilder().getBuilder();
      }
      /**
       * <pre>
       * (Required) The logical plan to get the storage level.
       * </pre>
       *
       * <code>.spark.connect.Relation relation = 1;</code>
       */
      public org.apache.spark.connect.proto.RelationOrBuilder getRelationOrBuilder() {
        if (relationBuilder_ != null) {
          return relationBuilder_.getMessageOrBuilder();
        } else {
          return relation_ == null ?
              org.apache.spark.connect.proto.Relation.getDefaultInstance() : relation_;
        }
      }
      /**
       * <pre>
       * (Required) The logical plan to get the storage level.
       * </pre>
       *
       * <code>.spark.connect.Relation relation = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.connect.proto.Relation, org.apache.spark.connect.proto.Relation.Builder, org.apache.spark.connect.proto.RelationOrBuilder> 
          getRelationFieldBuilder() {
        if (relationBuilder_ == null) {
          relationBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.apache.spark.connect.proto.Relation, org.apache.spark.connect.proto.Relation.Builder, org.apache.spark.connect.proto.RelationOrBuilder>(
                  getRelation(),
                  getParentForChildren(),
                  isClean());
          relation_ = null;
        }
        return relationBuilder_;
      }
      @Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:spark.connect.AnalyzePlanRequest.GetStorageLevel)
    }

    // @@protoc_insertion_point(class_scope:spark.connect.AnalyzePlanRequest.GetStorageLevel)
    private static final org.apache.spark.connect.proto.AnalyzePlanRequest.GetStorageLevel DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.connect.proto.AnalyzePlanRequest.GetStorageLevel();
    }

    public static org.apache.spark.connect.proto.AnalyzePlanRequest.GetStorageLevel getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<GetStorageLevel>
        PARSER = new com.google.protobuf.AbstractParser<GetStorageLevel>() {
      @Override
      public GetStorageLevel parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        Builder builder = newBuilder();
        try {
          builder.mergeFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.setUnfinishedMessage(builder.buildPartial());
        } catch (com.google.protobuf.UninitializedMessageException e) {
          throw e.asInvalidProtocolBufferException().setUnfinishedMessage(builder.buildPartial());
        } catch (java.io.IOException e) {
          throw new com.google.protobuf.InvalidProtocolBufferException(e)
              .setUnfinishedMessage(builder.buildPartial());
        }
        return builder.buildPartial();
      }
    };

    public static com.google.protobuf.Parser<GetStorageLevel> parser() {
      return PARSER;
    }

    @Override
    public com.google.protobuf.Parser<GetStorageLevel> getParserForType() {
      return PARSER;
    }

    @Override
    public org.apache.spark.connect.proto.AnalyzePlanRequest.GetStorageLevel getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  private int bitField0_;
  private int analyzeCase_ = 0;
  @SuppressWarnings("serial")
  private Object analyze_;
  public enum AnalyzeCase
      implements com.google.protobuf.Internal.EnumLite,
          InternalOneOfEnum {
    SCHEMA(4),
    EXPLAIN(5),
    TREE_STRING(6),
    IS_LOCAL(7),
    IS_STREAMING(8),
    INPUT_FILES(9),
    SPARK_VERSION(10),
    DDL_PARSE(11),
    SAME_SEMANTICS(12),
    SEMANTIC_HASH(13),
    PERSIST(14),
    UNPERSIST(15),
    GET_STORAGE_LEVEL(16),
    ANALYZE_NOT_SET(0);
    private final int value;
    private AnalyzeCase(int value) {
      this.value = value;
    }
    /**
     * @param value The number of the enum to look for.
     * @return The enum associated with the given number.
     * @deprecated Use {@link #forNumber(int)} instead.
     */
    @Deprecated
    public static AnalyzeCase valueOf(int value) {
      return forNumber(value);
    }

    public static AnalyzeCase forNumber(int value) {
      switch (value) {
        case 4: return SCHEMA;
        case 5: return EXPLAIN;
        case 6: return TREE_STRING;
        case 7: return IS_LOCAL;
        case 8: return IS_STREAMING;
        case 9: return INPUT_FILES;
        case 10: return SPARK_VERSION;
        case 11: return DDL_PARSE;
        case 12: return SAME_SEMANTICS;
        case 13: return SEMANTIC_HASH;
        case 14: return PERSIST;
        case 15: return UNPERSIST;
        case 16: return GET_STORAGE_LEVEL;
        case 0: return ANALYZE_NOT_SET;
        default: return null;
      }
    }
    public int getNumber() {
      return this.value;
    }
  };

  public AnalyzeCase
  getAnalyzeCase() {
    return AnalyzeCase.forNumber(
        analyzeCase_);
  }

  public static final int SESSION_ID_FIELD_NUMBER = 1;
  @SuppressWarnings("serial")
  private volatile Object sessionId_ = "";
  /**
   * <pre>
   * (Required)
   *
   * The session_id specifies a spark session for a user id (which is specified
   * by user_context.user_id). The session_id is set by the client to be able to
   * collate streaming responses from different queries within the dedicated session.
   * The id should be an UUID string of the format `00112233-4455-6677-8899-aabbccddeeff`
   * </pre>
   *
   * <code>string session_id = 1;</code>
   * @return The sessionId.
   */
  @Override
  public String getSessionId() {
    Object ref = sessionId_;
    if (ref instanceof String) {
      return (String) ref;
    } else {
      com.google.protobuf.ByteString bs = 
          (com.google.protobuf.ByteString) ref;
      String s = bs.toStringUtf8();
      sessionId_ = s;
      return s;
    }
  }
  /**
   * <pre>
   * (Required)
   *
   * The session_id specifies a spark session for a user id (which is specified
   * by user_context.user_id). The session_id is set by the client to be able to
   * collate streaming responses from different queries within the dedicated session.
   * The id should be an UUID string of the format `00112233-4455-6677-8899-aabbccddeeff`
   * </pre>
   *
   * <code>string session_id = 1;</code>
   * @return The bytes for sessionId.
   */
  @Override
  public com.google.protobuf.ByteString
      getSessionIdBytes() {
    Object ref = sessionId_;
    if (ref instanceof String) {
      com.google.protobuf.ByteString b = 
          com.google.protobuf.ByteString.copyFromUtf8(
              (String) ref);
      sessionId_ = b;
      return b;
    } else {
      return (com.google.protobuf.ByteString) ref;
    }
  }

  public static final int USER_CONTEXT_FIELD_NUMBER = 2;
  private org.apache.spark.connect.proto.UserContext userContext_;
  /**
   * <pre>
   * (Required) User context
   * </pre>
   *
   * <code>.spark.connect.UserContext user_context = 2;</code>
   * @return Whether the userContext field is set.
   */
  @Override
  public boolean hasUserContext() {
    return userContext_ != null;
  }
  /**
   * <pre>
   * (Required) User context
   * </pre>
   *
   * <code>.spark.connect.UserContext user_context = 2;</code>
   * @return The userContext.
   */
  @Override
  public org.apache.spark.connect.proto.UserContext getUserContext() {
    return userContext_ == null ? org.apache.spark.connect.proto.UserContext.getDefaultInstance() : userContext_;
  }
  /**
   * <pre>
   * (Required) User context
   * </pre>
   *
   * <code>.spark.connect.UserContext user_context = 2;</code>
   */
  @Override
  public org.apache.spark.connect.proto.UserContextOrBuilder getUserContextOrBuilder() {
    return userContext_ == null ? org.apache.spark.connect.proto.UserContext.getDefaultInstance() : userContext_;
  }

  public static final int CLIENT_TYPE_FIELD_NUMBER = 3;
  @SuppressWarnings("serial")
  private volatile Object clientType_ = "";
  /**
   * <pre>
   * Provides optional information about the client sending the request. This field
   * can be used for language or version specific information and is only intended for
   * logging purposes and will not be interpreted by the server.
   * </pre>
   *
   * <code>optional string client_type = 3;</code>
   * @return Whether the clientType field is set.
   */
  @Override
  public boolean hasClientType() {
    return ((bitField0_ & 0x00000001) != 0);
  }
  /**
   * <pre>
   * Provides optional information about the client sending the request. This field
   * can be used for language or version specific information and is only intended for
   * logging purposes and will not be interpreted by the server.
   * </pre>
   *
   * <code>optional string client_type = 3;</code>
   * @return The clientType.
   */
  @Override
  public String getClientType() {
    Object ref = clientType_;
    if (ref instanceof String) {
      return (String) ref;
    } else {
      com.google.protobuf.ByteString bs = 
          (com.google.protobuf.ByteString) ref;
      String s = bs.toStringUtf8();
      clientType_ = s;
      return s;
    }
  }
  /**
   * <pre>
   * Provides optional information about the client sending the request. This field
   * can be used for language or version specific information and is only intended for
   * logging purposes and will not be interpreted by the server.
   * </pre>
   *
   * <code>optional string client_type = 3;</code>
   * @return The bytes for clientType.
   */
  @Override
  public com.google.protobuf.ByteString
      getClientTypeBytes() {
    Object ref = clientType_;
    if (ref instanceof String) {
      com.google.protobuf.ByteString b = 
          com.google.protobuf.ByteString.copyFromUtf8(
              (String) ref);
      clientType_ = b;
      return b;
    } else {
      return (com.google.protobuf.ByteString) ref;
    }
  }

  public static final int SCHEMA_FIELD_NUMBER = 4;
  /**
   * <code>.spark.connect.AnalyzePlanRequest.Schema schema = 4;</code>
   * @return Whether the schema field is set.
   */
  @Override
  public boolean hasSchema() {
    return analyzeCase_ == 4;
  }
  /**
   * <code>.spark.connect.AnalyzePlanRequest.Schema schema = 4;</code>
   * @return The schema.
   */
  @Override
  public org.apache.spark.connect.proto.AnalyzePlanRequest.Schema getSchema() {
    if (analyzeCase_ == 4) {
       return (org.apache.spark.connect.proto.AnalyzePlanRequest.Schema) analyze_;
    }
    return org.apache.spark.connect.proto.AnalyzePlanRequest.Schema.getDefaultInstance();
  }
  /**
   * <code>.spark.connect.AnalyzePlanRequest.Schema schema = 4;</code>
   */
  @Override
  public org.apache.spark.connect.proto.AnalyzePlanRequest.SchemaOrBuilder getSchemaOrBuilder() {
    if (analyzeCase_ == 4) {
       return (org.apache.spark.connect.proto.AnalyzePlanRequest.Schema) analyze_;
    }
    return org.apache.spark.connect.proto.AnalyzePlanRequest.Schema.getDefaultInstance();
  }

  public static final int EXPLAIN_FIELD_NUMBER = 5;
  /**
   * <code>.spark.connect.AnalyzePlanRequest.Explain explain = 5;</code>
   * @return Whether the explain field is set.
   */
  @Override
  public boolean hasExplain() {
    return analyzeCase_ == 5;
  }
  /**
   * <code>.spark.connect.AnalyzePlanRequest.Explain explain = 5;</code>
   * @return The explain.
   */
  @Override
  public org.apache.spark.connect.proto.AnalyzePlanRequest.Explain getExplain() {
    if (analyzeCase_ == 5) {
       return (org.apache.spark.connect.proto.AnalyzePlanRequest.Explain) analyze_;
    }
    return org.apache.spark.connect.proto.AnalyzePlanRequest.Explain.getDefaultInstance();
  }
  /**
   * <code>.spark.connect.AnalyzePlanRequest.Explain explain = 5;</code>
   */
  @Override
  public org.apache.spark.connect.proto.AnalyzePlanRequest.ExplainOrBuilder getExplainOrBuilder() {
    if (analyzeCase_ == 5) {
       return (org.apache.spark.connect.proto.AnalyzePlanRequest.Explain) analyze_;
    }
    return org.apache.spark.connect.proto.AnalyzePlanRequest.Explain.getDefaultInstance();
  }

  public static final int TREE_STRING_FIELD_NUMBER = 6;
  /**
   * <code>.spark.connect.AnalyzePlanRequest.TreeString tree_string = 6;</code>
   * @return Whether the treeString field is set.
   */
  @Override
  public boolean hasTreeString() {
    return analyzeCase_ == 6;
  }
  /**
   * <code>.spark.connect.AnalyzePlanRequest.TreeString tree_string = 6;</code>
   * @return The treeString.
   */
  @Override
  public org.apache.spark.connect.proto.AnalyzePlanRequest.TreeString getTreeString() {
    if (analyzeCase_ == 6) {
       return (org.apache.spark.connect.proto.AnalyzePlanRequest.TreeString) analyze_;
    }
    return org.apache.spark.connect.proto.AnalyzePlanRequest.TreeString.getDefaultInstance();
  }
  /**
   * <code>.spark.connect.AnalyzePlanRequest.TreeString tree_string = 6;</code>
   */
  @Override
  public org.apache.spark.connect.proto.AnalyzePlanRequest.TreeStringOrBuilder getTreeStringOrBuilder() {
    if (analyzeCase_ == 6) {
       return (org.apache.spark.connect.proto.AnalyzePlanRequest.TreeString) analyze_;
    }
    return org.apache.spark.connect.proto.AnalyzePlanRequest.TreeString.getDefaultInstance();
  }

  public static final int IS_LOCAL_FIELD_NUMBER = 7;
  /**
   * <code>.spark.connect.AnalyzePlanRequest.IsLocal is_local = 7;</code>
   * @return Whether the isLocal field is set.
   */
  @Override
  public boolean hasIsLocal() {
    return analyzeCase_ == 7;
  }
  /**
   * <code>.spark.connect.AnalyzePlanRequest.IsLocal is_local = 7;</code>
   * @return The isLocal.
   */
  @Override
  public org.apache.spark.connect.proto.AnalyzePlanRequest.IsLocal getIsLocal() {
    if (analyzeCase_ == 7) {
       return (org.apache.spark.connect.proto.AnalyzePlanRequest.IsLocal) analyze_;
    }
    return org.apache.spark.connect.proto.AnalyzePlanRequest.IsLocal.getDefaultInstance();
  }
  /**
   * <code>.spark.connect.AnalyzePlanRequest.IsLocal is_local = 7;</code>
   */
  @Override
  public org.apache.spark.connect.proto.AnalyzePlanRequest.IsLocalOrBuilder getIsLocalOrBuilder() {
    if (analyzeCase_ == 7) {
       return (org.apache.spark.connect.proto.AnalyzePlanRequest.IsLocal) analyze_;
    }
    return org.apache.spark.connect.proto.AnalyzePlanRequest.IsLocal.getDefaultInstance();
  }

  public static final int IS_STREAMING_FIELD_NUMBER = 8;
  /**
   * <code>.spark.connect.AnalyzePlanRequest.IsStreaming is_streaming = 8;</code>
   * @return Whether the isStreaming field is set.
   */
  @Override
  public boolean hasIsStreaming() {
    return analyzeCase_ == 8;
  }
  /**
   * <code>.spark.connect.AnalyzePlanRequest.IsStreaming is_streaming = 8;</code>
   * @return The isStreaming.
   */
  @Override
  public org.apache.spark.connect.proto.AnalyzePlanRequest.IsStreaming getIsStreaming() {
    if (analyzeCase_ == 8) {
       return (org.apache.spark.connect.proto.AnalyzePlanRequest.IsStreaming) analyze_;
    }
    return org.apache.spark.connect.proto.AnalyzePlanRequest.IsStreaming.getDefaultInstance();
  }
  /**
   * <code>.spark.connect.AnalyzePlanRequest.IsStreaming is_streaming = 8;</code>
   */
  @Override
  public org.apache.spark.connect.proto.AnalyzePlanRequest.IsStreamingOrBuilder getIsStreamingOrBuilder() {
    if (analyzeCase_ == 8) {
       return (org.apache.spark.connect.proto.AnalyzePlanRequest.IsStreaming) analyze_;
    }
    return org.apache.spark.connect.proto.AnalyzePlanRequest.IsStreaming.getDefaultInstance();
  }

  public static final int INPUT_FILES_FIELD_NUMBER = 9;
  /**
   * <code>.spark.connect.AnalyzePlanRequest.InputFiles input_files = 9;</code>
   * @return Whether the inputFiles field is set.
   */
  @Override
  public boolean hasInputFiles() {
    return analyzeCase_ == 9;
  }
  /**
   * <code>.spark.connect.AnalyzePlanRequest.InputFiles input_files = 9;</code>
   * @return The inputFiles.
   */
  @Override
  public org.apache.spark.connect.proto.AnalyzePlanRequest.InputFiles getInputFiles() {
    if (analyzeCase_ == 9) {
       return (org.apache.spark.connect.proto.AnalyzePlanRequest.InputFiles) analyze_;
    }
    return org.apache.spark.connect.proto.AnalyzePlanRequest.InputFiles.getDefaultInstance();
  }
  /**
   * <code>.spark.connect.AnalyzePlanRequest.InputFiles input_files = 9;</code>
   */
  @Override
  public org.apache.spark.connect.proto.AnalyzePlanRequest.InputFilesOrBuilder getInputFilesOrBuilder() {
    if (analyzeCase_ == 9) {
       return (org.apache.spark.connect.proto.AnalyzePlanRequest.InputFiles) analyze_;
    }
    return org.apache.spark.connect.proto.AnalyzePlanRequest.InputFiles.getDefaultInstance();
  }

  public static final int SPARK_VERSION_FIELD_NUMBER = 10;
  /**
   * <code>.spark.connect.AnalyzePlanRequest.SparkVersion spark_version = 10;</code>
   * @return Whether the sparkVersion field is set.
   */
  @Override
  public boolean hasSparkVersion() {
    return analyzeCase_ == 10;
  }
  /**
   * <code>.spark.connect.AnalyzePlanRequest.SparkVersion spark_version = 10;</code>
   * @return The sparkVersion.
   */
  @Override
  public org.apache.spark.connect.proto.AnalyzePlanRequest.SparkVersion getSparkVersion() {
    if (analyzeCase_ == 10) {
       return (org.apache.spark.connect.proto.AnalyzePlanRequest.SparkVersion) analyze_;
    }
    return org.apache.spark.connect.proto.AnalyzePlanRequest.SparkVersion.getDefaultInstance();
  }
  /**
   * <code>.spark.connect.AnalyzePlanRequest.SparkVersion spark_version = 10;</code>
   */
  @Override
  public org.apache.spark.connect.proto.AnalyzePlanRequest.SparkVersionOrBuilder getSparkVersionOrBuilder() {
    if (analyzeCase_ == 10) {
       return (org.apache.spark.connect.proto.AnalyzePlanRequest.SparkVersion) analyze_;
    }
    return org.apache.spark.connect.proto.AnalyzePlanRequest.SparkVersion.getDefaultInstance();
  }

  public static final int DDL_PARSE_FIELD_NUMBER = 11;
  /**
   * <code>.spark.connect.AnalyzePlanRequest.DDLParse ddl_parse = 11;</code>
   * @return Whether the ddlParse field is set.
   */
  @Override
  public boolean hasDdlParse() {
    return analyzeCase_ == 11;
  }
  /**
   * <code>.spark.connect.AnalyzePlanRequest.DDLParse ddl_parse = 11;</code>
   * @return The ddlParse.
   */
  @Override
  public org.apache.spark.connect.proto.AnalyzePlanRequest.DDLParse getDdlParse() {
    if (analyzeCase_ == 11) {
       return (org.apache.spark.connect.proto.AnalyzePlanRequest.DDLParse) analyze_;
    }
    return org.apache.spark.connect.proto.AnalyzePlanRequest.DDLParse.getDefaultInstance();
  }
  /**
   * <code>.spark.connect.AnalyzePlanRequest.DDLParse ddl_parse = 11;</code>
   */
  @Override
  public org.apache.spark.connect.proto.AnalyzePlanRequest.DDLParseOrBuilder getDdlParseOrBuilder() {
    if (analyzeCase_ == 11) {
       return (org.apache.spark.connect.proto.AnalyzePlanRequest.DDLParse) analyze_;
    }
    return org.apache.spark.connect.proto.AnalyzePlanRequest.DDLParse.getDefaultInstance();
  }

  public static final int SAME_SEMANTICS_FIELD_NUMBER = 12;
  /**
   * <code>.spark.connect.AnalyzePlanRequest.SameSemantics same_semantics = 12;</code>
   * @return Whether the sameSemantics field is set.
   */
  @Override
  public boolean hasSameSemantics() {
    return analyzeCase_ == 12;
  }
  /**
   * <code>.spark.connect.AnalyzePlanRequest.SameSemantics same_semantics = 12;</code>
   * @return The sameSemantics.
   */
  @Override
  public org.apache.spark.connect.proto.AnalyzePlanRequest.SameSemantics getSameSemantics() {
    if (analyzeCase_ == 12) {
       return (org.apache.spark.connect.proto.AnalyzePlanRequest.SameSemantics) analyze_;
    }
    return org.apache.spark.connect.proto.AnalyzePlanRequest.SameSemantics.getDefaultInstance();
  }
  /**
   * <code>.spark.connect.AnalyzePlanRequest.SameSemantics same_semantics = 12;</code>
   */
  @Override
  public org.apache.spark.connect.proto.AnalyzePlanRequest.SameSemanticsOrBuilder getSameSemanticsOrBuilder() {
    if (analyzeCase_ == 12) {
       return (org.apache.spark.connect.proto.AnalyzePlanRequest.SameSemantics) analyze_;
    }
    return org.apache.spark.connect.proto.AnalyzePlanRequest.SameSemantics.getDefaultInstance();
  }

  public static final int SEMANTIC_HASH_FIELD_NUMBER = 13;
  /**
   * <code>.spark.connect.AnalyzePlanRequest.SemanticHash semantic_hash = 13;</code>
   * @return Whether the semanticHash field is set.
   */
  @Override
  public boolean hasSemanticHash() {
    return analyzeCase_ == 13;
  }
  /**
   * <code>.spark.connect.AnalyzePlanRequest.SemanticHash semantic_hash = 13;</code>
   * @return The semanticHash.
   */
  @Override
  public org.apache.spark.connect.proto.AnalyzePlanRequest.SemanticHash getSemanticHash() {
    if (analyzeCase_ == 13) {
       return (org.apache.spark.connect.proto.AnalyzePlanRequest.SemanticHash) analyze_;
    }
    return org.apache.spark.connect.proto.AnalyzePlanRequest.SemanticHash.getDefaultInstance();
  }
  /**
   * <code>.spark.connect.AnalyzePlanRequest.SemanticHash semantic_hash = 13;</code>
   */
  @Override
  public org.apache.spark.connect.proto.AnalyzePlanRequest.SemanticHashOrBuilder getSemanticHashOrBuilder() {
    if (analyzeCase_ == 13) {
       return (org.apache.spark.connect.proto.AnalyzePlanRequest.SemanticHash) analyze_;
    }
    return org.apache.spark.connect.proto.AnalyzePlanRequest.SemanticHash.getDefaultInstance();
  }

  public static final int PERSIST_FIELD_NUMBER = 14;
  /**
   * <code>.spark.connect.AnalyzePlanRequest.Persist persist = 14;</code>
   * @return Whether the persist field is set.
   */
  @Override
  public boolean hasPersist() {
    return analyzeCase_ == 14;
  }
  /**
   * <code>.spark.connect.AnalyzePlanRequest.Persist persist = 14;</code>
   * @return The persist.
   */
  @Override
  public org.apache.spark.connect.proto.AnalyzePlanRequest.Persist getPersist() {
    if (analyzeCase_ == 14) {
       return (org.apache.spark.connect.proto.AnalyzePlanRequest.Persist) analyze_;
    }
    return org.apache.spark.connect.proto.AnalyzePlanRequest.Persist.getDefaultInstance();
  }
  /**
   * <code>.spark.connect.AnalyzePlanRequest.Persist persist = 14;</code>
   */
  @Override
  public org.apache.spark.connect.proto.AnalyzePlanRequest.PersistOrBuilder getPersistOrBuilder() {
    if (analyzeCase_ == 14) {
       return (org.apache.spark.connect.proto.AnalyzePlanRequest.Persist) analyze_;
    }
    return org.apache.spark.connect.proto.AnalyzePlanRequest.Persist.getDefaultInstance();
  }

  public static final int UNPERSIST_FIELD_NUMBER = 15;
  /**
   * <code>.spark.connect.AnalyzePlanRequest.Unpersist unpersist = 15;</code>
   * @return Whether the unpersist field is set.
   */
  @Override
  public boolean hasUnpersist() {
    return analyzeCase_ == 15;
  }
  /**
   * <code>.spark.connect.AnalyzePlanRequest.Unpersist unpersist = 15;</code>
   * @return The unpersist.
   */
  @Override
  public org.apache.spark.connect.proto.AnalyzePlanRequest.Unpersist getUnpersist() {
    if (analyzeCase_ == 15) {
       return (org.apache.spark.connect.proto.AnalyzePlanRequest.Unpersist) analyze_;
    }
    return org.apache.spark.connect.proto.AnalyzePlanRequest.Unpersist.getDefaultInstance();
  }
  /**
   * <code>.spark.connect.AnalyzePlanRequest.Unpersist unpersist = 15;</code>
   */
  @Override
  public org.apache.spark.connect.proto.AnalyzePlanRequest.UnpersistOrBuilder getUnpersistOrBuilder() {
    if (analyzeCase_ == 15) {
       return (org.apache.spark.connect.proto.AnalyzePlanRequest.Unpersist) analyze_;
    }
    return org.apache.spark.connect.proto.AnalyzePlanRequest.Unpersist.getDefaultInstance();
  }

  public static final int GET_STORAGE_LEVEL_FIELD_NUMBER = 16;
  /**
   * <code>.spark.connect.AnalyzePlanRequest.GetStorageLevel get_storage_level = 16;</code>
   * @return Whether the getStorageLevel field is set.
   */
  @Override
  public boolean hasGetStorageLevel() {
    return analyzeCase_ == 16;
  }
  /**
   * <code>.spark.connect.AnalyzePlanRequest.GetStorageLevel get_storage_level = 16;</code>
   * @return The getStorageLevel.
   */
  @Override
  public org.apache.spark.connect.proto.AnalyzePlanRequest.GetStorageLevel getGetStorageLevel() {
    if (analyzeCase_ == 16) {
       return (org.apache.spark.connect.proto.AnalyzePlanRequest.GetStorageLevel) analyze_;
    }
    return org.apache.spark.connect.proto.AnalyzePlanRequest.GetStorageLevel.getDefaultInstance();
  }
  /**
   * <code>.spark.connect.AnalyzePlanRequest.GetStorageLevel get_storage_level = 16;</code>
   */
  @Override
  public org.apache.spark.connect.proto.AnalyzePlanRequest.GetStorageLevelOrBuilder getGetStorageLevelOrBuilder() {
    if (analyzeCase_ == 16) {
       return (org.apache.spark.connect.proto.AnalyzePlanRequest.GetStorageLevel) analyze_;
    }
    return org.apache.spark.connect.proto.AnalyzePlanRequest.GetStorageLevel.getDefaultInstance();
  }

  private byte memoizedIsInitialized = -1;
  @Override
  public final boolean isInitialized() {
    byte isInitialized = memoizedIsInitialized;
    if (isInitialized == 1) return true;
    if (isInitialized == 0) return false;

    memoizedIsInitialized = 1;
    return true;
  }

  @Override
  public void writeTo(com.google.protobuf.CodedOutputStream output)
                      throws java.io.IOException {
    if (!com.google.protobuf.GeneratedMessageV3.isStringEmpty(sessionId_)) {
      com.google.protobuf.GeneratedMessageV3.writeString(output, 1, sessionId_);
    }
    if (userContext_ != null) {
      output.writeMessage(2, getUserContext());
    }
    if (((bitField0_ & 0x00000001) != 0)) {
      com.google.protobuf.GeneratedMessageV3.writeString(output, 3, clientType_);
    }
    if (analyzeCase_ == 4) {
      output.writeMessage(4, (org.apache.spark.connect.proto.AnalyzePlanRequest.Schema) analyze_);
    }
    if (analyzeCase_ == 5) {
      output.writeMessage(5, (org.apache.spark.connect.proto.AnalyzePlanRequest.Explain) analyze_);
    }
    if (analyzeCase_ == 6) {
      output.writeMessage(6, (org.apache.spark.connect.proto.AnalyzePlanRequest.TreeString) analyze_);
    }
    if (analyzeCase_ == 7) {
      output.writeMessage(7, (org.apache.spark.connect.proto.AnalyzePlanRequest.IsLocal) analyze_);
    }
    if (analyzeCase_ == 8) {
      output.writeMessage(8, (org.apache.spark.connect.proto.AnalyzePlanRequest.IsStreaming) analyze_);
    }
    if (analyzeCase_ == 9) {
      output.writeMessage(9, (org.apache.spark.connect.proto.AnalyzePlanRequest.InputFiles) analyze_);
    }
    if (analyzeCase_ == 10) {
      output.writeMessage(10, (org.apache.spark.connect.proto.AnalyzePlanRequest.SparkVersion) analyze_);
    }
    if (analyzeCase_ == 11) {
      output.writeMessage(11, (org.apache.spark.connect.proto.AnalyzePlanRequest.DDLParse) analyze_);
    }
    if (analyzeCase_ == 12) {
      output.writeMessage(12, (org.apache.spark.connect.proto.AnalyzePlanRequest.SameSemantics) analyze_);
    }
    if (analyzeCase_ == 13) {
      output.writeMessage(13, (org.apache.spark.connect.proto.AnalyzePlanRequest.SemanticHash) analyze_);
    }
    if (analyzeCase_ == 14) {
      output.writeMessage(14, (org.apache.spark.connect.proto.AnalyzePlanRequest.Persist) analyze_);
    }
    if (analyzeCase_ == 15) {
      output.writeMessage(15, (org.apache.spark.connect.proto.AnalyzePlanRequest.Unpersist) analyze_);
    }
    if (analyzeCase_ == 16) {
      output.writeMessage(16, (org.apache.spark.connect.proto.AnalyzePlanRequest.GetStorageLevel) analyze_);
    }
    getUnknownFields().writeTo(output);
  }

  @Override
  public int getSerializedSize() {
    int size = memoizedSize;
    if (size != -1) return size;

    size = 0;
    if (!com.google.protobuf.GeneratedMessageV3.isStringEmpty(sessionId_)) {
      size += com.google.protobuf.GeneratedMessageV3.computeStringSize(1, sessionId_);
    }
    if (userContext_ != null) {
      size += com.google.protobuf.CodedOutputStream
        .computeMessageSize(2, getUserContext());
    }
    if (((bitField0_ & 0x00000001) != 0)) {
      size += com.google.protobuf.GeneratedMessageV3.computeStringSize(3, clientType_);
    }
    if (analyzeCase_ == 4) {
      size += com.google.protobuf.CodedOutputStream
        .computeMessageSize(4, (org.apache.spark.connect.proto.AnalyzePlanRequest.Schema) analyze_);
    }
    if (analyzeCase_ == 5) {
      size += com.google.protobuf.CodedOutputStream
        .computeMessageSize(5, (org.apache.spark.connect.proto.AnalyzePlanRequest.Explain) analyze_);
    }
    if (analyzeCase_ == 6) {
      size += com.google.protobuf.CodedOutputStream
        .computeMessageSize(6, (org.apache.spark.connect.proto.AnalyzePlanRequest.TreeString) analyze_);
    }
    if (analyzeCase_ == 7) {
      size += com.google.protobuf.CodedOutputStream
        .computeMessageSize(7, (org.apache.spark.connect.proto.AnalyzePlanRequest.IsLocal) analyze_);
    }
    if (analyzeCase_ == 8) {
      size += com.google.protobuf.CodedOutputStream
        .computeMessageSize(8, (org.apache.spark.connect.proto.AnalyzePlanRequest.IsStreaming) analyze_);
    }
    if (analyzeCase_ == 9) {
      size += com.google.protobuf.CodedOutputStream
        .computeMessageSize(9, (org.apache.spark.connect.proto.AnalyzePlanRequest.InputFiles) analyze_);
    }
    if (analyzeCase_ == 10) {
      size += com.google.protobuf.CodedOutputStream
        .computeMessageSize(10, (org.apache.spark.connect.proto.AnalyzePlanRequest.SparkVersion) analyze_);
    }
    if (analyzeCase_ == 11) {
      size += com.google.protobuf.CodedOutputStream
        .computeMessageSize(11, (org.apache.spark.connect.proto.AnalyzePlanRequest.DDLParse) analyze_);
    }
    if (analyzeCase_ == 12) {
      size += com.google.protobuf.CodedOutputStream
        .computeMessageSize(12, (org.apache.spark.connect.proto.AnalyzePlanRequest.SameSemantics) analyze_);
    }
    if (analyzeCase_ == 13) {
      size += com.google.protobuf.CodedOutputStream
        .computeMessageSize(13, (org.apache.spark.connect.proto.AnalyzePlanRequest.SemanticHash) analyze_);
    }
    if (analyzeCase_ == 14) {
      size += com.google.protobuf.CodedOutputStream
        .computeMessageSize(14, (org.apache.spark.connect.proto.AnalyzePlanRequest.Persist) analyze_);
    }
    if (analyzeCase_ == 15) {
      size += com.google.protobuf.CodedOutputStream
        .computeMessageSize(15, (org.apache.spark.connect.proto.AnalyzePlanRequest.Unpersist) analyze_);
    }
    if (analyzeCase_ == 16) {
      size += com.google.protobuf.CodedOutputStream
        .computeMessageSize(16, (org.apache.spark.connect.proto.AnalyzePlanRequest.GetStorageLevel) analyze_);
    }
    size += getUnknownFields().getSerializedSize();
    memoizedSize = size;
    return size;
  }

  @Override
  public boolean equals(final Object obj) {
    if (obj == this) {
     return true;
    }
    if (!(obj instanceof org.apache.spark.connect.proto.AnalyzePlanRequest)) {
      return super.equals(obj);
    }
    org.apache.spark.connect.proto.AnalyzePlanRequest other = (org.apache.spark.connect.proto.AnalyzePlanRequest) obj;

    if (!getSessionId()
        .equals(other.getSessionId())) return false;
    if (hasUserContext() != other.hasUserContext()) return false;
    if (hasUserContext()) {
      if (!getUserContext()
          .equals(other.getUserContext())) return false;
    }
    if (hasClientType() != other.hasClientType()) return false;
    if (hasClientType()) {
      if (!getClientType()
          .equals(other.getClientType())) return false;
    }
    if (!getAnalyzeCase().equals(other.getAnalyzeCase())) return false;
    switch (analyzeCase_) {
      case 4:
        if (!getSchema()
            .equals(other.getSchema())) return false;
        break;
      case 5:
        if (!getExplain()
            .equals(other.getExplain())) return false;
        break;
      case 6:
        if (!getTreeString()
            .equals(other.getTreeString())) return false;
        break;
      case 7:
        if (!getIsLocal()
            .equals(other.getIsLocal())) return false;
        break;
      case 8:
        if (!getIsStreaming()
            .equals(other.getIsStreaming())) return false;
        break;
      case 9:
        if (!getInputFiles()
            .equals(other.getInputFiles())) return false;
        break;
      case 10:
        if (!getSparkVersion()
            .equals(other.getSparkVersion())) return false;
        break;
      case 11:
        if (!getDdlParse()
            .equals(other.getDdlParse())) return false;
        break;
      case 12:
        if (!getSameSemantics()
            .equals(other.getSameSemantics())) return false;
        break;
      case 13:
        if (!getSemanticHash()
            .equals(other.getSemanticHash())) return false;
        break;
      case 14:
        if (!getPersist()
            .equals(other.getPersist())) return false;
        break;
      case 15:
        if (!getUnpersist()
            .equals(other.getUnpersist())) return false;
        break;
      case 16:
        if (!getGetStorageLevel()
            .equals(other.getGetStorageLevel())) return false;
        break;
      case 0:
      default:
    }
    if (!getUnknownFields().equals(other.getUnknownFields())) return false;
    return true;
  }

  @Override
  public int hashCode() {
    if (memoizedHashCode != 0) {
      return memoizedHashCode;
    }
    int hash = 41;
    hash = (19 * hash) + getDescriptor().hashCode();
    hash = (37 * hash) + SESSION_ID_FIELD_NUMBER;
    hash = (53 * hash) + getSessionId().hashCode();
    if (hasUserContext()) {
      hash = (37 * hash) + USER_CONTEXT_FIELD_NUMBER;
      hash = (53 * hash) + getUserContext().hashCode();
    }
    if (hasClientType()) {
      hash = (37 * hash) + CLIENT_TYPE_FIELD_NUMBER;
      hash = (53 * hash) + getClientType().hashCode();
    }
    switch (analyzeCase_) {
      case 4:
        hash = (37 * hash) + SCHEMA_FIELD_NUMBER;
        hash = (53 * hash) + getSchema().hashCode();
        break;
      case 5:
        hash = (37 * hash) + EXPLAIN_FIELD_NUMBER;
        hash = (53 * hash) + getExplain().hashCode();
        break;
      case 6:
        hash = (37 * hash) + TREE_STRING_FIELD_NUMBER;
        hash = (53 * hash) + getTreeString().hashCode();
        break;
      case 7:
        hash = (37 * hash) + IS_LOCAL_FIELD_NUMBER;
        hash = (53 * hash) + getIsLocal().hashCode();
        break;
      case 8:
        hash = (37 * hash) + IS_STREAMING_FIELD_NUMBER;
        hash = (53 * hash) + getIsStreaming().hashCode();
        break;
      case 9:
        hash = (37 * hash) + INPUT_FILES_FIELD_NUMBER;
        hash = (53 * hash) + getInputFiles().hashCode();
        break;
      case 10:
        hash = (37 * hash) + SPARK_VERSION_FIELD_NUMBER;
        hash = (53 * hash) + getSparkVersion().hashCode();
        break;
      case 11:
        hash = (37 * hash) + DDL_PARSE_FIELD_NUMBER;
        hash = (53 * hash) + getDdlParse().hashCode();
        break;
      case 12:
        hash = (37 * hash) + SAME_SEMANTICS_FIELD_NUMBER;
        hash = (53 * hash) + getSameSemantics().hashCode();
        break;
      case 13:
        hash = (37 * hash) + SEMANTIC_HASH_FIELD_NUMBER;
        hash = (53 * hash) + getSemanticHash().hashCode();
        break;
      case 14:
        hash = (37 * hash) + PERSIST_FIELD_NUMBER;
        hash = (53 * hash) + getPersist().hashCode();
        break;
      case 15:
        hash = (37 * hash) + UNPERSIST_FIELD_NUMBER;
        hash = (53 * hash) + getUnpersist().hashCode();
        break;
      case 16:
        hash = (37 * hash) + GET_STORAGE_LEVEL_FIELD_NUMBER;
        hash = (53 * hash) + getGetStorageLevel().hashCode();
        break;
      case 0:
      default:
    }
    hash = (29 * hash) + getUnknownFields().hashCode();
    memoizedHashCode = hash;
    return hash;
  }

  public static org.apache.spark.connect.proto.AnalyzePlanRequest parseFrom(
      java.nio.ByteBuffer data)
      throws com.google.protobuf.InvalidProtocolBufferException {
    return PARSER.parseFrom(data);
  }
  public static org.apache.spark.connect.proto.AnalyzePlanRequest parseFrom(
      java.nio.ByteBuffer data,
      com.google.protobuf.ExtensionRegistryLite extensionRegistry)
      throws com.google.protobuf.InvalidProtocolBufferException {
    return PARSER.parseFrom(data, extensionRegistry);
  }
  public static org.apache.spark.connect.proto.AnalyzePlanRequest parseFrom(
      com.google.protobuf.ByteString data)
      throws com.google.protobuf.InvalidProtocolBufferException {
    return PARSER.parseFrom(data);
  }
  public static org.apache.spark.connect.proto.AnalyzePlanRequest parseFrom(
      com.google.protobuf.ByteString data,
      com.google.protobuf.ExtensionRegistryLite extensionRegistry)
      throws com.google.protobuf.InvalidProtocolBufferException {
    return PARSER.parseFrom(data, extensionRegistry);
  }
  public static org.apache.spark.connect.proto.AnalyzePlanRequest parseFrom(byte[] data)
      throws com.google.protobuf.InvalidProtocolBufferException {
    return PARSER.parseFrom(data);
  }
  public static org.apache.spark.connect.proto.AnalyzePlanRequest parseFrom(
      byte[] data,
      com.google.protobuf.ExtensionRegistryLite extensionRegistry)
      throws com.google.protobuf.InvalidProtocolBufferException {
    return PARSER.parseFrom(data, extensionRegistry);
  }
  public static org.apache.spark.connect.proto.AnalyzePlanRequest parseFrom(java.io.InputStream input)
      throws java.io.IOException {
    return com.google.protobuf.GeneratedMessageV3
        .parseWithIOException(PARSER, input);
  }
  public static org.apache.spark.connect.proto.AnalyzePlanRequest parseFrom(
      java.io.InputStream input,
      com.google.protobuf.ExtensionRegistryLite extensionRegistry)
      throws java.io.IOException {
    return com.google.protobuf.GeneratedMessageV3
        .parseWithIOException(PARSER, input, extensionRegistry);
  }

  public static org.apache.spark.connect.proto.AnalyzePlanRequest parseDelimitedFrom(java.io.InputStream input)
      throws java.io.IOException {
    return com.google.protobuf.GeneratedMessageV3
        .parseDelimitedWithIOException(PARSER, input);
  }

  public static org.apache.spark.connect.proto.AnalyzePlanRequest parseDelimitedFrom(
      java.io.InputStream input,
      com.google.protobuf.ExtensionRegistryLite extensionRegistry)
      throws java.io.IOException {
    return com.google.protobuf.GeneratedMessageV3
        .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
  }
  public static org.apache.spark.connect.proto.AnalyzePlanRequest parseFrom(
      com.google.protobuf.CodedInputStream input)
      throws java.io.IOException {
    return com.google.protobuf.GeneratedMessageV3
        .parseWithIOException(PARSER, input);
  }
  public static org.apache.spark.connect.proto.AnalyzePlanRequest parseFrom(
      com.google.protobuf.CodedInputStream input,
      com.google.protobuf.ExtensionRegistryLite extensionRegistry)
      throws java.io.IOException {
    return com.google.protobuf.GeneratedMessageV3
        .parseWithIOException(PARSER, input, extensionRegistry);
  }

  @Override
  public Builder newBuilderForType() { return newBuilder(); }
  public static Builder newBuilder() {
    return DEFAULT_INSTANCE.toBuilder();
  }
  public static Builder newBuilder(org.apache.spark.connect.proto.AnalyzePlanRequest prototype) {
    return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
  }
  @Override
  public Builder toBuilder() {
    return this == DEFAULT_INSTANCE
        ? new Builder() : new Builder().mergeFrom(this);
  }

  @Override
  protected Builder newBuilderForType(
      BuilderParent parent) {
    Builder builder = new Builder(parent);
    return builder;
  }
  /**
   * <pre>
   * Request to perform plan analyze, optionally to explain the plan.
   * </pre>
   *
   * Protobuf type {@code spark.connect.AnalyzePlanRequest}
   */
  public static final class Builder extends
      com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
      // @@protoc_insertion_point(builder_implements:spark.connect.AnalyzePlanRequest)
      org.apache.spark.connect.proto.AnalyzePlanRequestOrBuilder {
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.connect.proto.Base.internal_static_spark_connect_AnalyzePlanRequest_descriptor;
    }

    @Override
    protected FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.connect.proto.Base.internal_static_spark_connect_AnalyzePlanRequest_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.connect.proto.AnalyzePlanRequest.class, org.apache.spark.connect.proto.AnalyzePlanRequest.Builder.class);
    }

    // Construct using org.apache.spark.connect.proto.AnalyzePlanRequest.newBuilder()
    private Builder() {

    }

    private Builder(
        BuilderParent parent) {
      super(parent);

    }
    @Override
    public Builder clear() {
      super.clear();
      bitField0_ = 0;
      sessionId_ = "";
      userContext_ = null;
      if (userContextBuilder_ != null) {
        userContextBuilder_.dispose();
        userContextBuilder_ = null;
      }
      clientType_ = "";
      if (schemaBuilder_ != null) {
        schemaBuilder_.clear();
      }
      if (explainBuilder_ != null) {
        explainBuilder_.clear();
      }
      if (treeStringBuilder_ != null) {
        treeStringBuilder_.clear();
      }
      if (isLocalBuilder_ != null) {
        isLocalBuilder_.clear();
      }
      if (isStreamingBuilder_ != null) {
        isStreamingBuilder_.clear();
      }
      if (inputFilesBuilder_ != null) {
        inputFilesBuilder_.clear();
      }
      if (sparkVersionBuilder_ != null) {
        sparkVersionBuilder_.clear();
      }
      if (ddlParseBuilder_ != null) {
        ddlParseBuilder_.clear();
      }
      if (sameSemanticsBuilder_ != null) {
        sameSemanticsBuilder_.clear();
      }
      if (semanticHashBuilder_ != null) {
        semanticHashBuilder_.clear();
      }
      if (persistBuilder_ != null) {
        persistBuilder_.clear();
      }
      if (unpersistBuilder_ != null) {
        unpersistBuilder_.clear();
      }
      if (getStorageLevelBuilder_ != null) {
        getStorageLevelBuilder_.clear();
      }
      analyzeCase_ = 0;
      analyze_ = null;
      return this;
    }

    @Override
    public com.google.protobuf.Descriptors.Descriptor
        getDescriptorForType() {
      return org.apache.spark.connect.proto.Base.internal_static_spark_connect_AnalyzePlanRequest_descriptor;
    }

    @Override
    public org.apache.spark.connect.proto.AnalyzePlanRequest getDefaultInstanceForType() {
      return org.apache.spark.connect.proto.AnalyzePlanRequest.getDefaultInstance();
    }

    @Override
    public org.apache.spark.connect.proto.AnalyzePlanRequest build() {
      org.apache.spark.connect.proto.AnalyzePlanRequest result = buildPartial();
      if (!result.isInitialized()) {
        throw newUninitializedMessageException(result);
      }
      return result;
    }

    @Override
    public org.apache.spark.connect.proto.AnalyzePlanRequest buildPartial() {
      org.apache.spark.connect.proto.AnalyzePlanRequest result = new org.apache.spark.connect.proto.AnalyzePlanRequest(this);
      if (bitField0_ != 0) { buildPartial0(result); }
      buildPartialOneofs(result);
      onBuilt();
      return result;
    }

    private void buildPartial0(org.apache.spark.connect.proto.AnalyzePlanRequest result) {
      int from_bitField0_ = bitField0_;
      if (((from_bitField0_ & 0x00000001) != 0)) {
        result.sessionId_ = sessionId_;
      }
      if (((from_bitField0_ & 0x00000002) != 0)) {
        result.userContext_ = userContextBuilder_ == null
            ? userContext_
            : userContextBuilder_.build();
      }
      int to_bitField0_ = 0;
      if (((from_bitField0_ & 0x00000004) != 0)) {
        result.clientType_ = clientType_;
        to_bitField0_ |= 0x00000001;
      }
      result.bitField0_ |= to_bitField0_;
    }

    private void buildPartialOneofs(org.apache.spark.connect.proto.AnalyzePlanRequest result) {
      result.analyzeCase_ = analyzeCase_;
      result.analyze_ = this.analyze_;
      if (analyzeCase_ == 4 &&
          schemaBuilder_ != null) {
        result.analyze_ = schemaBuilder_.build();
      }
      if (analyzeCase_ == 5 &&
          explainBuilder_ != null) {
        result.analyze_ = explainBuilder_.build();
      }
      if (analyzeCase_ == 6 &&
          treeStringBuilder_ != null) {
        result.analyze_ = treeStringBuilder_.build();
      }
      if (analyzeCase_ == 7 &&
          isLocalBuilder_ != null) {
        result.analyze_ = isLocalBuilder_.build();
      }
      if (analyzeCase_ == 8 &&
          isStreamingBuilder_ != null) {
        result.analyze_ = isStreamingBuilder_.build();
      }
      if (analyzeCase_ == 9 &&
          inputFilesBuilder_ != null) {
        result.analyze_ = inputFilesBuilder_.build();
      }
      if (analyzeCase_ == 10 &&
          sparkVersionBuilder_ != null) {
        result.analyze_ = sparkVersionBuilder_.build();
      }
      if (analyzeCase_ == 11 &&
          ddlParseBuilder_ != null) {
        result.analyze_ = ddlParseBuilder_.build();
      }
      if (analyzeCase_ == 12 &&
          sameSemanticsBuilder_ != null) {
        result.analyze_ = sameSemanticsBuilder_.build();
      }
      if (analyzeCase_ == 13 &&
          semanticHashBuilder_ != null) {
        result.analyze_ = semanticHashBuilder_.build();
      }
      if (analyzeCase_ == 14 &&
          persistBuilder_ != null) {
        result.analyze_ = persistBuilder_.build();
      }
      if (analyzeCase_ == 15 &&
          unpersistBuilder_ != null) {
        result.analyze_ = unpersistBuilder_.build();
      }
      if (analyzeCase_ == 16 &&
          getStorageLevelBuilder_ != null) {
        result.analyze_ = getStorageLevelBuilder_.build();
      }
    }

    @Override
    public Builder clone() {
      return super.clone();
    }
    @Override
    public Builder setField(
        com.google.protobuf.Descriptors.FieldDescriptor field,
        Object value) {
      return super.setField(field, value);
    }
    @Override
    public Builder clearField(
        com.google.protobuf.Descriptors.FieldDescriptor field) {
      return super.clearField(field);
    }
    @Override
    public Builder clearOneof(
        com.google.protobuf.Descriptors.OneofDescriptor oneof) {
      return super.clearOneof(oneof);
    }
    @Override
    public Builder setRepeatedField(
        com.google.protobuf.Descriptors.FieldDescriptor field,
        int index, Object value) {
      return super.setRepeatedField(field, index, value);
    }
    @Override
    public Builder addRepeatedField(
        com.google.protobuf.Descriptors.FieldDescriptor field,
        Object value) {
      return super.addRepeatedField(field, value);
    }
    @Override
    public Builder mergeFrom(com.google.protobuf.Message other) {
      if (other instanceof org.apache.spark.connect.proto.AnalyzePlanRequest) {
        return mergeFrom((org.apache.spark.connect.proto.AnalyzePlanRequest)other);
      } else {
        super.mergeFrom(other);
        return this;
      }
    }

    public Builder mergeFrom(org.apache.spark.connect.proto.AnalyzePlanRequest other) {
      if (other == org.apache.spark.connect.proto.AnalyzePlanRequest.getDefaultInstance()) return this;
      if (!other.getSessionId().isEmpty()) {
        sessionId_ = other.sessionId_;
        bitField0_ |= 0x00000001;
        onChanged();
      }
      if (other.hasUserContext()) {
        mergeUserContext(other.getUserContext());
      }
      if (other.hasClientType()) {
        clientType_ = other.clientType_;
        bitField0_ |= 0x00000004;
        onChanged();
      }
      switch (other.getAnalyzeCase()) {
        case SCHEMA: {
          mergeSchema(other.getSchema());
          break;
        }
        case EXPLAIN: {
          mergeExplain(other.getExplain());
          break;
        }
        case TREE_STRING: {
          mergeTreeString(other.getTreeString());
          break;
        }
        case IS_LOCAL: {
          mergeIsLocal(other.getIsLocal());
          break;
        }
        case IS_STREAMING: {
          mergeIsStreaming(other.getIsStreaming());
          break;
        }
        case INPUT_FILES: {
          mergeInputFiles(other.getInputFiles());
          break;
        }
        case SPARK_VERSION: {
          mergeSparkVersion(other.getSparkVersion());
          break;
        }
        case DDL_PARSE: {
          mergeDdlParse(other.getDdlParse());
          break;
        }
        case SAME_SEMANTICS: {
          mergeSameSemantics(other.getSameSemantics());
          break;
        }
        case SEMANTIC_HASH: {
          mergeSemanticHash(other.getSemanticHash());
          break;
        }
        case PERSIST: {
          mergePersist(other.getPersist());
          break;
        }
        case UNPERSIST: {
          mergeUnpersist(other.getUnpersist());
          break;
        }
        case GET_STORAGE_LEVEL: {
          mergeGetStorageLevel(other.getGetStorageLevel());
          break;
        }
        case ANALYZE_NOT_SET: {
          break;
        }
      }
      this.mergeUnknownFields(other.getUnknownFields());
      onChanged();
      return this;
    }

    @Override
    public final boolean isInitialized() {
      return true;
    }

    @Override
    public Builder mergeFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      if (extensionRegistry == null) {
        throw new NullPointerException();
      }
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              sessionId_ = input.readStringRequireUtf8();
              bitField0_ |= 0x00000001;
              break;
            } // case 10
            case 18: {
              input.readMessage(
                  getUserContextFieldBuilder().getBuilder(),
                  extensionRegistry);
              bitField0_ |= 0x00000002;
              break;
            } // case 18
            case 26: {
              clientType_ = input.readStringRequireUtf8();
              bitField0_ |= 0x00000004;
              break;
            } // case 26
            case 34: {
              input.readMessage(
                  getSchemaFieldBuilder().getBuilder(),
                  extensionRegistry);
              analyzeCase_ = 4;
              break;
            } // case 34
            case 42: {
              input.readMessage(
                  getExplainFieldBuilder().getBuilder(),
                  extensionRegistry);
              analyzeCase_ = 5;
              break;
            } // case 42
            case 50: {
              input.readMessage(
                  getTreeStringFieldBuilder().getBuilder(),
                  extensionRegistry);
              analyzeCase_ = 6;
              break;
            } // case 50
            case 58: {
              input.readMessage(
                  getIsLocalFieldBuilder().getBuilder(),
                  extensionRegistry);
              analyzeCase_ = 7;
              break;
            } // case 58
            case 66: {
              input.readMessage(
                  getIsStreamingFieldBuilder().getBuilder(),
                  extensionRegistry);
              analyzeCase_ = 8;
              break;
            } // case 66
            case 74: {
              input.readMessage(
                  getInputFilesFieldBuilder().getBuilder(),
                  extensionRegistry);
              analyzeCase_ = 9;
              break;
            } // case 74
            case 82: {
              input.readMessage(
                  getSparkVersionFieldBuilder().getBuilder(),
                  extensionRegistry);
              analyzeCase_ = 10;
              break;
            } // case 82
            case 90: {
              input.readMessage(
                  getDdlParseFieldBuilder().getBuilder(),
                  extensionRegistry);
              analyzeCase_ = 11;
              break;
            } // case 90
            case 98: {
              input.readMessage(
                  getSameSemanticsFieldBuilder().getBuilder(),
                  extensionRegistry);
              analyzeCase_ = 12;
              break;
            } // case 98
            case 106: {
              input.readMessage(
                  getSemanticHashFieldBuilder().getBuilder(),
                  extensionRegistry);
              analyzeCase_ = 13;
              break;
            } // case 106
            case 114: {
              input.readMessage(
                  getPersistFieldBuilder().getBuilder(),
                  extensionRegistry);
              analyzeCase_ = 14;
              break;
            } // case 114
            case 122: {
              input.readMessage(
                  getUnpersistFieldBuilder().getBuilder(),
                  extensionRegistry);
              analyzeCase_ = 15;
              break;
            } // case 122
            case 130: {
              input.readMessage(
                  getGetStorageLevelFieldBuilder().getBuilder(),
                  extensionRegistry);
              analyzeCase_ = 16;
              break;
            } // case 130
            default: {
              if (!super.parseUnknownField(input, extensionRegistry, tag)) {
                done = true; // was an endgroup tag
              }
              break;
            } // default:
          } // switch (tag)
        } // while (!done)
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.unwrapIOException();
      } finally {
        onChanged();
      } // finally
      return this;
    }
    private int analyzeCase_ = 0;
    private Object analyze_;
    public AnalyzeCase
        getAnalyzeCase() {
      return AnalyzeCase.forNumber(
          analyzeCase_);
    }

    public Builder clearAnalyze() {
      analyzeCase_ = 0;
      analyze_ = null;
      onChanged();
      return this;
    }

    private int bitField0_;

    private Object sessionId_ = "";
    /**
     * <pre>
     * (Required)
     *
     * The session_id specifies a spark session for a user id (which is specified
     * by user_context.user_id). The session_id is set by the client to be able to
     * collate streaming responses from different queries within the dedicated session.
     * The id should be an UUID string of the format `00112233-4455-6677-8899-aabbccddeeff`
     * </pre>
     *
     * <code>string session_id = 1;</code>
     * @return The sessionId.
     */
    public String getSessionId() {
      Object ref = sessionId_;
      if (!(ref instanceof String)) {
        com.google.protobuf.ByteString bs =
            (com.google.protobuf.ByteString) ref;
        String s = bs.toStringUtf8();
        sessionId_ = s;
        return s;
      } else {
        return (String) ref;
      }
    }
    /**
     * <pre>
     * (Required)
     *
     * The session_id specifies a spark session for a user id (which is specified
     * by user_context.user_id). The session_id is set by the client to be able to
     * collate streaming responses from different queries within the dedicated session.
     * The id should be an UUID string of the format `00112233-4455-6677-8899-aabbccddeeff`
     * </pre>
     *
     * <code>string session_id = 1;</code>
     * @return The bytes for sessionId.
     */
    public com.google.protobuf.ByteString
        getSessionIdBytes() {
      Object ref = sessionId_;
      if (ref instanceof String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (String) ref);
        sessionId_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }
    /**
     * <pre>
     * (Required)
     *
     * The session_id specifies a spark session for a user id (which is specified
     * by user_context.user_id). The session_id is set by the client to be able to
     * collate streaming responses from different queries within the dedicated session.
     * The id should be an UUID string of the format `00112233-4455-6677-8899-aabbccddeeff`
     * </pre>
     *
     * <code>string session_id = 1;</code>
     * @param value The sessionId to set.
     * @return This builder for chaining.
     */
    public Builder setSessionId(
        String value) {
      if (value == null) { throw new NullPointerException(); }
      sessionId_ = value;
      bitField0_ |= 0x00000001;
      onChanged();
      return this;
    }
    /**
     * <pre>
     * (Required)
     *
     * The session_id specifies a spark session for a user id (which is specified
     * by user_context.user_id). The session_id is set by the client to be able to
     * collate streaming responses from different queries within the dedicated session.
     * The id should be an UUID string of the format `00112233-4455-6677-8899-aabbccddeeff`
     * </pre>
     *
     * <code>string session_id = 1;</code>
     * @return This builder for chaining.
     */
    public Builder clearSessionId() {
      sessionId_ = getDefaultInstance().getSessionId();
      bitField0_ = (bitField0_ & ~0x00000001);
      onChanged();
      return this;
    }
    /**
     * <pre>
     * (Required)
     *
     * The session_id specifies a spark session for a user id (which is specified
     * by user_context.user_id). The session_id is set by the client to be able to
     * collate streaming responses from different queries within the dedicated session.
     * The id should be an UUID string of the format `00112233-4455-6677-8899-aabbccddeeff`
     * </pre>
     *
     * <code>string session_id = 1;</code>
     * @param value The bytes for sessionId to set.
     * @return This builder for chaining.
     */
    public Builder setSessionIdBytes(
        com.google.protobuf.ByteString value) {
      if (value == null) { throw new NullPointerException(); }
      checkByteStringIsUtf8(value);
      sessionId_ = value;
      bitField0_ |= 0x00000001;
      onChanged();
      return this;
    }

    private org.apache.spark.connect.proto.UserContext userContext_;
    private com.google.protobuf.SingleFieldBuilderV3<
        org.apache.spark.connect.proto.UserContext, org.apache.spark.connect.proto.UserContext.Builder, org.apache.spark.connect.proto.UserContextOrBuilder> userContextBuilder_;
    /**
     * <pre>
     * (Required) User context
     * </pre>
     *
     * <code>.spark.connect.UserContext user_context = 2;</code>
     * @return Whether the userContext field is set.
     */
    public boolean hasUserContext() {
      return ((bitField0_ & 0x00000002) != 0);
    }
    /**
     * <pre>
     * (Required) User context
     * </pre>
     *
     * <code>.spark.connect.UserContext user_context = 2;</code>
     * @return The userContext.
     */
    public org.apache.spark.connect.proto.UserContext getUserContext() {
      if (userContextBuilder_ == null) {
        return userContext_ == null ? org.apache.spark.connect.proto.UserContext.getDefaultInstance() : userContext_;
      } else {
        return userContextBuilder_.getMessage();
      }
    }
    /**
     * <pre>
     * (Required) User context
     * </pre>
     *
     * <code>.spark.connect.UserContext user_context = 2;</code>
     */
    public Builder setUserContext(org.apache.spark.connect.proto.UserContext value) {
      if (userContextBuilder_ == null) {
        if (value == null) {
          throw new NullPointerException();
        }
        userContext_ = value;
      } else {
        userContextBuilder_.setMessage(value);
      }
      bitField0_ |= 0x00000002;
      onChanged();
      return this;
    }
    /**
     * <pre>
     * (Required) User context
     * </pre>
     *
     * <code>.spark.connect.UserContext user_context = 2;</code>
     */
    public Builder setUserContext(
        org.apache.spark.connect.proto.UserContext.Builder builderForValue) {
      if (userContextBuilder_ == null) {
        userContext_ = builderForValue.build();
      } else {
        userContextBuilder_.setMessage(builderForValue.build());
      }
      bitField0_ |= 0x00000002;
      onChanged();
      return this;
    }
    /**
     * <pre>
     * (Required) User context
     * </pre>
     *
     * <code>.spark.connect.UserContext user_context = 2;</code>
     */
    public Builder mergeUserContext(org.apache.spark.connect.proto.UserContext value) {
      if (userContextBuilder_ == null) {
        if (((bitField0_ & 0x00000002) != 0) &&
          userContext_ != null &&
          userContext_ != org.apache.spark.connect.proto.UserContext.getDefaultInstance()) {
          getUserContextBuilder().mergeFrom(value);
        } else {
          userContext_ = value;
        }
      } else {
        userContextBuilder_.mergeFrom(value);
      }
      bitField0_ |= 0x00000002;
      onChanged();
      return this;
    }
    /**
     * <pre>
     * (Required) User context
     * </pre>
     *
     * <code>.spark.connect.UserContext user_context = 2;</code>
     */
    public Builder clearUserContext() {
      bitField0_ = (bitField0_ & ~0x00000002);
      userContext_ = null;
      if (userContextBuilder_ != null) {
        userContextBuilder_.dispose();
        userContextBuilder_ = null;
      }
      onChanged();
      return this;
    }
    /**
     * <pre>
     * (Required) User context
     * </pre>
     *
     * <code>.spark.connect.UserContext user_context = 2;</code>
     */
    public org.apache.spark.connect.proto.UserContext.Builder getUserContextBuilder() {
      bitField0_ |= 0x00000002;
      onChanged();
      return getUserContextFieldBuilder().getBuilder();
    }
    /**
     * <pre>
     * (Required) User context
     * </pre>
     *
     * <code>.spark.connect.UserContext user_context = 2;</code>
     */
    public org.apache.spark.connect.proto.UserContextOrBuilder getUserContextOrBuilder() {
      if (userContextBuilder_ != null) {
        return userContextBuilder_.getMessageOrBuilder();
      } else {
        return userContext_ == null ?
            org.apache.spark.connect.proto.UserContext.getDefaultInstance() : userContext_;
      }
    }
    /**
     * <pre>
     * (Required) User context
     * </pre>
     *
     * <code>.spark.connect.UserContext user_context = 2;</code>
     */
    private com.google.protobuf.SingleFieldBuilderV3<
        org.apache.spark.connect.proto.UserContext, org.apache.spark.connect.proto.UserContext.Builder, org.apache.spark.connect.proto.UserContextOrBuilder> 
        getUserContextFieldBuilder() {
      if (userContextBuilder_ == null) {
        userContextBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
            org.apache.spark.connect.proto.UserContext, org.apache.spark.connect.proto.UserContext.Builder, org.apache.spark.connect.proto.UserContextOrBuilder>(
                getUserContext(),
                getParentForChildren(),
                isClean());
        userContext_ = null;
      }
      return userContextBuilder_;
    }

    private Object clientType_ = "";
    /**
     * <pre>
     * Provides optional information about the client sending the request. This field
     * can be used for language or version specific information and is only intended for
     * logging purposes and will not be interpreted by the server.
     * </pre>
     *
     * <code>optional string client_type = 3;</code>
     * @return Whether the clientType field is set.
     */
    public boolean hasClientType() {
      return ((bitField0_ & 0x00000004) != 0);
    }
    /**
     * <pre>
     * Provides optional information about the client sending the request. This field
     * can be used for language or version specific information and is only intended for
     * logging purposes and will not be interpreted by the server.
     * </pre>
     *
     * <code>optional string client_type = 3;</code>
     * @return The clientType.
     */
    public String getClientType() {
      Object ref = clientType_;
      if (!(ref instanceof String)) {
        com.google.protobuf.ByteString bs =
            (com.google.protobuf.ByteString) ref;
        String s = bs.toStringUtf8();
        clientType_ = s;
        return s;
      } else {
        return (String) ref;
      }
    }
    /**
     * <pre>
     * Provides optional information about the client sending the request. This field
     * can be used for language or version specific information and is only intended for
     * logging purposes and will not be interpreted by the server.
     * </pre>
     *
     * <code>optional string client_type = 3;</code>
     * @return The bytes for clientType.
     */
    public com.google.protobuf.ByteString
        getClientTypeBytes() {
      Object ref = clientType_;
      if (ref instanceof String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (String) ref);
        clientType_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }
    /**
     * <pre>
     * Provides optional information about the client sending the request. This field
     * can be used for language or version specific information and is only intended for
     * logging purposes and will not be interpreted by the server.
     * </pre>
     *
     * <code>optional string client_type = 3;</code>
     * @param value The clientType to set.
     * @return This builder for chaining.
     */
    public Builder setClientType(
        String value) {
      if (value == null) { throw new NullPointerException(); }
      clientType_ = value;
      bitField0_ |= 0x00000004;
      onChanged();
      return this;
    }
    /**
     * <pre>
     * Provides optional information about the client sending the request. This field
     * can be used for language or version specific information and is only intended for
     * logging purposes and will not be interpreted by the server.
     * </pre>
     *
     * <code>optional string client_type = 3;</code>
     * @return This builder for chaining.
     */
    public Builder clearClientType() {
      clientType_ = getDefaultInstance().getClientType();
      bitField0_ = (bitField0_ & ~0x00000004);
      onChanged();
      return this;
    }
    /**
     * <pre>
     * Provides optional information about the client sending the request. This field
     * can be used for language or version specific information and is only intended for
     * logging purposes and will not be interpreted by the server.
     * </pre>
     *
     * <code>optional string client_type = 3;</code>
     * @param value The bytes for clientType to set.
     * @return This builder for chaining.
     */
    public Builder setClientTypeBytes(
        com.google.protobuf.ByteString value) {
      if (value == null) { throw new NullPointerException(); }
      checkByteStringIsUtf8(value);
      clientType_ = value;
      bitField0_ |= 0x00000004;
      onChanged();
      return this;
    }

    private com.google.protobuf.SingleFieldBuilderV3<
        org.apache.spark.connect.proto.AnalyzePlanRequest.Schema, org.apache.spark.connect.proto.AnalyzePlanRequest.Schema.Builder, org.apache.spark.connect.proto.AnalyzePlanRequest.SchemaOrBuilder> schemaBuilder_;
    /**
     * <code>.spark.connect.AnalyzePlanRequest.Schema schema = 4;</code>
     * @return Whether the schema field is set.
     */
    @Override
    public boolean hasSchema() {
      return analyzeCase_ == 4;
    }
    /**
     * <code>.spark.connect.AnalyzePlanRequest.Schema schema = 4;</code>
     * @return The schema.
     */
    @Override
    public org.apache.spark.connect.proto.AnalyzePlanRequest.Schema getSchema() {
      if (schemaBuilder_ == null) {
        if (analyzeCase_ == 4) {
          return (org.apache.spark.connect.proto.AnalyzePlanRequest.Schema) analyze_;
        }
        return org.apache.spark.connect.proto.AnalyzePlanRequest.Schema.getDefaultInstance();
      } else {
        if (analyzeCase_ == 4) {
          return schemaBuilder_.getMessage();
        }
        return org.apache.spark.connect.proto.AnalyzePlanRequest.Schema.getDefaultInstance();
      }
    }
    /**
     * <code>.spark.connect.AnalyzePlanRequest.Schema schema = 4;</code>
     */
    public Builder setSchema(org.apache.spark.connect.proto.AnalyzePlanRequest.Schema value) {
      if (schemaBuilder_ == null) {
        if (value == null) {
          throw new NullPointerException();
        }
        analyze_ = value;
        onChanged();
      } else {
        schemaBuilder_.setMessage(value);
      }
      analyzeCase_ = 4;
      return this;
    }
    /**
     * <code>.spark.connect.AnalyzePlanRequest.Schema schema = 4;</code>
     */
    public Builder setSchema(
        org.apache.spark.connect.proto.AnalyzePlanRequest.Schema.Builder builderForValue) {
      if (schemaBuilder_ == null) {
        analyze_ = builderForValue.build();
        onChanged();
      } else {
        schemaBuilder_.setMessage(builderForValue.build());
      }
      analyzeCase_ = 4;
      return this;
    }
    /**
     * <code>.spark.connect.AnalyzePlanRequest.Schema schema = 4;</code>
     */
    public Builder mergeSchema(org.apache.spark.connect.proto.AnalyzePlanRequest.Schema value) {
      if (schemaBuilder_ == null) {
        if (analyzeCase_ == 4 &&
            analyze_ != org.apache.spark.connect.proto.AnalyzePlanRequest.Schema.getDefaultInstance()) {
          analyze_ = org.apache.spark.connect.proto.AnalyzePlanRequest.Schema.newBuilder((org.apache.spark.connect.proto.AnalyzePlanRequest.Schema) analyze_)
              .mergeFrom(value).buildPartial();
        } else {
          analyze_ = value;
        }
        onChanged();
      } else {
        if (analyzeCase_ == 4) {
          schemaBuilder_.mergeFrom(value);
        } else {
          schemaBuilder_.setMessage(value);
        }
      }
      analyzeCase_ = 4;
      return this;
    }
    /**
     * <code>.spark.connect.AnalyzePlanRequest.Schema schema = 4;</code>
     */
    public Builder clearSchema() {
      if (schemaBuilder_ == null) {
        if (analyzeCase_ == 4) {
          analyzeCase_ = 0;
          analyze_ = null;
          onChanged();
        }
      } else {
        if (analyzeCase_ == 4) {
          analyzeCase_ = 0;
          analyze_ = null;
        }
        schemaBuilder_.clear();
      }
      return this;
    }
    /**
     * <code>.spark.connect.AnalyzePlanRequest.Schema schema = 4;</code>
     */
    public org.apache.spark.connect.proto.AnalyzePlanRequest.Schema.Builder getSchemaBuilder() {
      return getSchemaFieldBuilder().getBuilder();
    }
    /**
     * <code>.spark.connect.AnalyzePlanRequest.Schema schema = 4;</code>
     */
    @Override
    public org.apache.spark.connect.proto.AnalyzePlanRequest.SchemaOrBuilder getSchemaOrBuilder() {
      if ((analyzeCase_ == 4) && (schemaBuilder_ != null)) {
        return schemaBuilder_.getMessageOrBuilder();
      } else {
        if (analyzeCase_ == 4) {
          return (org.apache.spark.connect.proto.AnalyzePlanRequest.Schema) analyze_;
        }
        return org.apache.spark.connect.proto.AnalyzePlanRequest.Schema.getDefaultInstance();
      }
    }
    /**
     * <code>.spark.connect.AnalyzePlanRequest.Schema schema = 4;</code>
     */
    private com.google.protobuf.SingleFieldBuilderV3<
        org.apache.spark.connect.proto.AnalyzePlanRequest.Schema, org.apache.spark.connect.proto.AnalyzePlanRequest.Schema.Builder, org.apache.spark.connect.proto.AnalyzePlanRequest.SchemaOrBuilder> 
        getSchemaFieldBuilder() {
      if (schemaBuilder_ == null) {
        if (!(analyzeCase_ == 4)) {
          analyze_ = org.apache.spark.connect.proto.AnalyzePlanRequest.Schema.getDefaultInstance();
        }
        schemaBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
            org.apache.spark.connect.proto.AnalyzePlanRequest.Schema, org.apache.spark.connect.proto.AnalyzePlanRequest.Schema.Builder, org.apache.spark.connect.proto.AnalyzePlanRequest.SchemaOrBuilder>(
                (org.apache.spark.connect.proto.AnalyzePlanRequest.Schema) analyze_,
                getParentForChildren(),
                isClean());
        analyze_ = null;
      }
      analyzeCase_ = 4;
      onChanged();
      return schemaBuilder_;
    }

    private com.google.protobuf.SingleFieldBuilderV3<
        org.apache.spark.connect.proto.AnalyzePlanRequest.Explain, org.apache.spark.connect.proto.AnalyzePlanRequest.Explain.Builder, org.apache.spark.connect.proto.AnalyzePlanRequest.ExplainOrBuilder> explainBuilder_;
    /**
     * <code>.spark.connect.AnalyzePlanRequest.Explain explain = 5;</code>
     * @return Whether the explain field is set.
     */
    @Override
    public boolean hasExplain() {
      return analyzeCase_ == 5;
    }
    /**
     * <code>.spark.connect.AnalyzePlanRequest.Explain explain = 5;</code>
     * @return The explain.
     */
    @Override
    public org.apache.spark.connect.proto.AnalyzePlanRequest.Explain getExplain() {
      if (explainBuilder_ == null) {
        if (analyzeCase_ == 5) {
          return (org.apache.spark.connect.proto.AnalyzePlanRequest.Explain) analyze_;
        }
        return org.apache.spark.connect.proto.AnalyzePlanRequest.Explain.getDefaultInstance();
      } else {
        if (analyzeCase_ == 5) {
          return explainBuilder_.getMessage();
        }
        return org.apache.spark.connect.proto.AnalyzePlanRequest.Explain.getDefaultInstance();
      }
    }
    /**
     * <code>.spark.connect.AnalyzePlanRequest.Explain explain = 5;</code>
     */
    public Builder setExplain(org.apache.spark.connect.proto.AnalyzePlanRequest.Explain value) {
      if (explainBuilder_ == null) {
        if (value == null) {
          throw new NullPointerException();
        }
        analyze_ = value;
        onChanged();
      } else {
        explainBuilder_.setMessage(value);
      }
      analyzeCase_ = 5;
      return this;
    }
    /**
     * <code>.spark.connect.AnalyzePlanRequest.Explain explain = 5;</code>
     */
    public Builder setExplain(
        org.apache.spark.connect.proto.AnalyzePlanRequest.Explain.Builder builderForValue) {
      if (explainBuilder_ == null) {
        analyze_ = builderForValue.build();
        onChanged();
      } else {
        explainBuilder_.setMessage(builderForValue.build());
      }
      analyzeCase_ = 5;
      return this;
    }
    /**
     * <code>.spark.connect.AnalyzePlanRequest.Explain explain = 5;</code>
     */
    public Builder mergeExplain(org.apache.spark.connect.proto.AnalyzePlanRequest.Explain value) {
      if (explainBuilder_ == null) {
        if (analyzeCase_ == 5 &&
            analyze_ != org.apache.spark.connect.proto.AnalyzePlanRequest.Explain.getDefaultInstance()) {
          analyze_ = org.apache.spark.connect.proto.AnalyzePlanRequest.Explain.newBuilder((org.apache.spark.connect.proto.AnalyzePlanRequest.Explain) analyze_)
              .mergeFrom(value).buildPartial();
        } else {
          analyze_ = value;
        }
        onChanged();
      } else {
        if (analyzeCase_ == 5) {
          explainBuilder_.mergeFrom(value);
        } else {
          explainBuilder_.setMessage(value);
        }
      }
      analyzeCase_ = 5;
      return this;
    }
    /**
     * <code>.spark.connect.AnalyzePlanRequest.Explain explain = 5;</code>
     */
    public Builder clearExplain() {
      if (explainBuilder_ == null) {
        if (analyzeCase_ == 5) {
          analyzeCase_ = 0;
          analyze_ = null;
          onChanged();
        }
      } else {
        if (analyzeCase_ == 5) {
          analyzeCase_ = 0;
          analyze_ = null;
        }
        explainBuilder_.clear();
      }
      return this;
    }
    /**
     * <code>.spark.connect.AnalyzePlanRequest.Explain explain = 5;</code>
     */
    public org.apache.spark.connect.proto.AnalyzePlanRequest.Explain.Builder getExplainBuilder() {
      return getExplainFieldBuilder().getBuilder();
    }
    /**
     * <code>.spark.connect.AnalyzePlanRequest.Explain explain = 5;</code>
     */
    @Override
    public org.apache.spark.connect.proto.AnalyzePlanRequest.ExplainOrBuilder getExplainOrBuilder() {
      if ((analyzeCase_ == 5) && (explainBuilder_ != null)) {
        return explainBuilder_.getMessageOrBuilder();
      } else {
        if (analyzeCase_ == 5) {
          return (org.apache.spark.connect.proto.AnalyzePlanRequest.Explain) analyze_;
        }
        return org.apache.spark.connect.proto.AnalyzePlanRequest.Explain.getDefaultInstance();
      }
    }
    /**
     * <code>.spark.connect.AnalyzePlanRequest.Explain explain = 5;</code>
     */
    private com.google.protobuf.SingleFieldBuilderV3<
        org.apache.spark.connect.proto.AnalyzePlanRequest.Explain, org.apache.spark.connect.proto.AnalyzePlanRequest.Explain.Builder, org.apache.spark.connect.proto.AnalyzePlanRequest.ExplainOrBuilder> 
        getExplainFieldBuilder() {
      if (explainBuilder_ == null) {
        if (!(analyzeCase_ == 5)) {
          analyze_ = org.apache.spark.connect.proto.AnalyzePlanRequest.Explain.getDefaultInstance();
        }
        explainBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
            org.apache.spark.connect.proto.AnalyzePlanRequest.Explain, org.apache.spark.connect.proto.AnalyzePlanRequest.Explain.Builder, org.apache.spark.connect.proto.AnalyzePlanRequest.ExplainOrBuilder>(
                (org.apache.spark.connect.proto.AnalyzePlanRequest.Explain) analyze_,
                getParentForChildren(),
                isClean());
        analyze_ = null;
      }
      analyzeCase_ = 5;
      onChanged();
      return explainBuilder_;
    }

    private com.google.protobuf.SingleFieldBuilderV3<
        org.apache.spark.connect.proto.AnalyzePlanRequest.TreeString, org.apache.spark.connect.proto.AnalyzePlanRequest.TreeString.Builder, org.apache.spark.connect.proto.AnalyzePlanRequest.TreeStringOrBuilder> treeStringBuilder_;
    /**
     * <code>.spark.connect.AnalyzePlanRequest.TreeString tree_string = 6;</code>
     * @return Whether the treeString field is set.
     */
    @Override
    public boolean hasTreeString() {
      return analyzeCase_ == 6;
    }
    /**
     * <code>.spark.connect.AnalyzePlanRequest.TreeString tree_string = 6;</code>
     * @return The treeString.
     */
    @Override
    public org.apache.spark.connect.proto.AnalyzePlanRequest.TreeString getTreeString() {
      if (treeStringBuilder_ == null) {
        if (analyzeCase_ == 6) {
          return (org.apache.spark.connect.proto.AnalyzePlanRequest.TreeString) analyze_;
        }
        return org.apache.spark.connect.proto.AnalyzePlanRequest.TreeString.getDefaultInstance();
      } else {
        if (analyzeCase_ == 6) {
          return treeStringBuilder_.getMessage();
        }
        return org.apache.spark.connect.proto.AnalyzePlanRequest.TreeString.getDefaultInstance();
      }
    }
    /**
     * <code>.spark.connect.AnalyzePlanRequest.TreeString tree_string = 6;</code>
     */
    public Builder setTreeString(org.apache.spark.connect.proto.AnalyzePlanRequest.TreeString value) {
      if (treeStringBuilder_ == null) {
        if (value == null) {
          throw new NullPointerException();
        }
        analyze_ = value;
        onChanged();
      } else {
        treeStringBuilder_.setMessage(value);
      }
      analyzeCase_ = 6;
      return this;
    }
    /**
     * <code>.spark.connect.AnalyzePlanRequest.TreeString tree_string = 6;</code>
     */
    public Builder setTreeString(
        org.apache.spark.connect.proto.AnalyzePlanRequest.TreeString.Builder builderForValue) {
      if (treeStringBuilder_ == null) {
        analyze_ = builderForValue.build();
        onChanged();
      } else {
        treeStringBuilder_.setMessage(builderForValue.build());
      }
      analyzeCase_ = 6;
      return this;
    }
    /**
     * <code>.spark.connect.AnalyzePlanRequest.TreeString tree_string = 6;</code>
     */
    public Builder mergeTreeString(org.apache.spark.connect.proto.AnalyzePlanRequest.TreeString value) {
      if (treeStringBuilder_ == null) {
        if (analyzeCase_ == 6 &&
            analyze_ != org.apache.spark.connect.proto.AnalyzePlanRequest.TreeString.getDefaultInstance()) {
          analyze_ = org.apache.spark.connect.proto.AnalyzePlanRequest.TreeString.newBuilder((org.apache.spark.connect.proto.AnalyzePlanRequest.TreeString) analyze_)
              .mergeFrom(value).buildPartial();
        } else {
          analyze_ = value;
        }
        onChanged();
      } else {
        if (analyzeCase_ == 6) {
          treeStringBuilder_.mergeFrom(value);
        } else {
          treeStringBuilder_.setMessage(value);
        }
      }
      analyzeCase_ = 6;
      return this;
    }
    /**
     * <code>.spark.connect.AnalyzePlanRequest.TreeString tree_string = 6;</code>
     */
    public Builder clearTreeString() {
      if (treeStringBuilder_ == null) {
        if (analyzeCase_ == 6) {
          analyzeCase_ = 0;
          analyze_ = null;
          onChanged();
        }
      } else {
        if (analyzeCase_ == 6) {
          analyzeCase_ = 0;
          analyze_ = null;
        }
        treeStringBuilder_.clear();
      }
      return this;
    }
    /**
     * <code>.spark.connect.AnalyzePlanRequest.TreeString tree_string = 6;</code>
     */
    public org.apache.spark.connect.proto.AnalyzePlanRequest.TreeString.Builder getTreeStringBuilder() {
      return getTreeStringFieldBuilder().getBuilder();
    }
    /**
     * <code>.spark.connect.AnalyzePlanRequest.TreeString tree_string = 6;</code>
     */
    @Override
    public org.apache.spark.connect.proto.AnalyzePlanRequest.TreeStringOrBuilder getTreeStringOrBuilder() {
      if ((analyzeCase_ == 6) && (treeStringBuilder_ != null)) {
        return treeStringBuilder_.getMessageOrBuilder();
      } else {
        if (analyzeCase_ == 6) {
          return (org.apache.spark.connect.proto.AnalyzePlanRequest.TreeString) analyze_;
        }
        return org.apache.spark.connect.proto.AnalyzePlanRequest.TreeString.getDefaultInstance();
      }
    }
    /**
     * <code>.spark.connect.AnalyzePlanRequest.TreeString tree_string = 6;</code>
     */
    private com.google.protobuf.SingleFieldBuilderV3<
        org.apache.spark.connect.proto.AnalyzePlanRequest.TreeString, org.apache.spark.connect.proto.AnalyzePlanRequest.TreeString.Builder, org.apache.spark.connect.proto.AnalyzePlanRequest.TreeStringOrBuilder> 
        getTreeStringFieldBuilder() {
      if (treeStringBuilder_ == null) {
        if (!(analyzeCase_ == 6)) {
          analyze_ = org.apache.spark.connect.proto.AnalyzePlanRequest.TreeString.getDefaultInstance();
        }
        treeStringBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
            org.apache.spark.connect.proto.AnalyzePlanRequest.TreeString, org.apache.spark.connect.proto.AnalyzePlanRequest.TreeString.Builder, org.apache.spark.connect.proto.AnalyzePlanRequest.TreeStringOrBuilder>(
                (org.apache.spark.connect.proto.AnalyzePlanRequest.TreeString) analyze_,
                getParentForChildren(),
                isClean());
        analyze_ = null;
      }
      analyzeCase_ = 6;
      onChanged();
      return treeStringBuilder_;
    }

    private com.google.protobuf.SingleFieldBuilderV3<
        org.apache.spark.connect.proto.AnalyzePlanRequest.IsLocal, org.apache.spark.connect.proto.AnalyzePlanRequest.IsLocal.Builder, org.apache.spark.connect.proto.AnalyzePlanRequest.IsLocalOrBuilder> isLocalBuilder_;
    /**
     * <code>.spark.connect.AnalyzePlanRequest.IsLocal is_local = 7;</code>
     * @return Whether the isLocal field is set.
     */
    @Override
    public boolean hasIsLocal() {
      return analyzeCase_ == 7;
    }
    /**
     * <code>.spark.connect.AnalyzePlanRequest.IsLocal is_local = 7;</code>
     * @return The isLocal.
     */
    @Override
    public org.apache.spark.connect.proto.AnalyzePlanRequest.IsLocal getIsLocal() {
      if (isLocalBuilder_ == null) {
        if (analyzeCase_ == 7) {
          return (org.apache.spark.connect.proto.AnalyzePlanRequest.IsLocal) analyze_;
        }
        return org.apache.spark.connect.proto.AnalyzePlanRequest.IsLocal.getDefaultInstance();
      } else {
        if (analyzeCase_ == 7) {
          return isLocalBuilder_.getMessage();
        }
        return org.apache.spark.connect.proto.AnalyzePlanRequest.IsLocal.getDefaultInstance();
      }
    }
    /**
     * <code>.spark.connect.AnalyzePlanRequest.IsLocal is_local = 7;</code>
     */
    public Builder setIsLocal(org.apache.spark.connect.proto.AnalyzePlanRequest.IsLocal value) {
      if (isLocalBuilder_ == null) {
        if (value == null) {
          throw new NullPointerException();
        }
        analyze_ = value;
        onChanged();
      } else {
        isLocalBuilder_.setMessage(value);
      }
      analyzeCase_ = 7;
      return this;
    }
    /**
     * <code>.spark.connect.AnalyzePlanRequest.IsLocal is_local = 7;</code>
     */
    public Builder setIsLocal(
        org.apache.spark.connect.proto.AnalyzePlanRequest.IsLocal.Builder builderForValue) {
      if (isLocalBuilder_ == null) {
        analyze_ = builderForValue.build();
        onChanged();
      } else {
        isLocalBuilder_.setMessage(builderForValue.build());
      }
      analyzeCase_ = 7;
      return this;
    }
    /**
     * <code>.spark.connect.AnalyzePlanRequest.IsLocal is_local = 7;</code>
     */
    public Builder mergeIsLocal(org.apache.spark.connect.proto.AnalyzePlanRequest.IsLocal value) {
      if (isLocalBuilder_ == null) {
        if (analyzeCase_ == 7 &&
            analyze_ != org.apache.spark.connect.proto.AnalyzePlanRequest.IsLocal.getDefaultInstance()) {
          analyze_ = org.apache.spark.connect.proto.AnalyzePlanRequest.IsLocal.newBuilder((org.apache.spark.connect.proto.AnalyzePlanRequest.IsLocal) analyze_)
              .mergeFrom(value).buildPartial();
        } else {
          analyze_ = value;
        }
        onChanged();
      } else {
        if (analyzeCase_ == 7) {
          isLocalBuilder_.mergeFrom(value);
        } else {
          isLocalBuilder_.setMessage(value);
        }
      }
      analyzeCase_ = 7;
      return this;
    }
    /**
     * <code>.spark.connect.AnalyzePlanRequest.IsLocal is_local = 7;</code>
     */
    public Builder clearIsLocal() {
      if (isLocalBuilder_ == null) {
        if (analyzeCase_ == 7) {
          analyzeCase_ = 0;
          analyze_ = null;
          onChanged();
        }
      } else {
        if (analyzeCase_ == 7) {
          analyzeCase_ = 0;
          analyze_ = null;
        }
        isLocalBuilder_.clear();
      }
      return this;
    }
    /**
     * <code>.spark.connect.AnalyzePlanRequest.IsLocal is_local = 7;</code>
     */
    public org.apache.spark.connect.proto.AnalyzePlanRequest.IsLocal.Builder getIsLocalBuilder() {
      return getIsLocalFieldBuilder().getBuilder();
    }
    /**
     * <code>.spark.connect.AnalyzePlanRequest.IsLocal is_local = 7;</code>
     */
    @Override
    public org.apache.spark.connect.proto.AnalyzePlanRequest.IsLocalOrBuilder getIsLocalOrBuilder() {
      if ((analyzeCase_ == 7) && (isLocalBuilder_ != null)) {
        return isLocalBuilder_.getMessageOrBuilder();
      } else {
        if (analyzeCase_ == 7) {
          return (org.apache.spark.connect.proto.AnalyzePlanRequest.IsLocal) analyze_;
        }
        return org.apache.spark.connect.proto.AnalyzePlanRequest.IsLocal.getDefaultInstance();
      }
    }
    /**
     * <code>.spark.connect.AnalyzePlanRequest.IsLocal is_local = 7;</code>
     */
    private com.google.protobuf.SingleFieldBuilderV3<
        org.apache.spark.connect.proto.AnalyzePlanRequest.IsLocal, org.apache.spark.connect.proto.AnalyzePlanRequest.IsLocal.Builder, org.apache.spark.connect.proto.AnalyzePlanRequest.IsLocalOrBuilder> 
        getIsLocalFieldBuilder() {
      if (isLocalBuilder_ == null) {
        if (!(analyzeCase_ == 7)) {
          analyze_ = org.apache.spark.connect.proto.AnalyzePlanRequest.IsLocal.getDefaultInstance();
        }
        isLocalBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
            org.apache.spark.connect.proto.AnalyzePlanRequest.IsLocal, org.apache.spark.connect.proto.AnalyzePlanRequest.IsLocal.Builder, org.apache.spark.connect.proto.AnalyzePlanRequest.IsLocalOrBuilder>(
                (org.apache.spark.connect.proto.AnalyzePlanRequest.IsLocal) analyze_,
                getParentForChildren(),
                isClean());
        analyze_ = null;
      }
      analyzeCase_ = 7;
      onChanged();
      return isLocalBuilder_;
    }

    private com.google.protobuf.SingleFieldBuilderV3<
        org.apache.spark.connect.proto.AnalyzePlanRequest.IsStreaming, org.apache.spark.connect.proto.AnalyzePlanRequest.IsStreaming.Builder, org.apache.spark.connect.proto.AnalyzePlanRequest.IsStreamingOrBuilder> isStreamingBuilder_;
    /**
     * <code>.spark.connect.AnalyzePlanRequest.IsStreaming is_streaming = 8;</code>
     * @return Whether the isStreaming field is set.
     */
    @Override
    public boolean hasIsStreaming() {
      return analyzeCase_ == 8;
    }
    /**
     * <code>.spark.connect.AnalyzePlanRequest.IsStreaming is_streaming = 8;</code>
     * @return The isStreaming.
     */
    @Override
    public org.apache.spark.connect.proto.AnalyzePlanRequest.IsStreaming getIsStreaming() {
      if (isStreamingBuilder_ == null) {
        if (analyzeCase_ == 8) {
          return (org.apache.spark.connect.proto.AnalyzePlanRequest.IsStreaming) analyze_;
        }
        return org.apache.spark.connect.proto.AnalyzePlanRequest.IsStreaming.getDefaultInstance();
      } else {
        if (analyzeCase_ == 8) {
          return isStreamingBuilder_.getMessage();
        }
        return org.apache.spark.connect.proto.AnalyzePlanRequest.IsStreaming.getDefaultInstance();
      }
    }
    /**
     * <code>.spark.connect.AnalyzePlanRequest.IsStreaming is_streaming = 8;</code>
     */
    public Builder setIsStreaming(org.apache.spark.connect.proto.AnalyzePlanRequest.IsStreaming value) {
      if (isStreamingBuilder_ == null) {
        if (value == null) {
          throw new NullPointerException();
        }
        analyze_ = value;
        onChanged();
      } else {
        isStreamingBuilder_.setMessage(value);
      }
      analyzeCase_ = 8;
      return this;
    }
    /**
     * <code>.spark.connect.AnalyzePlanRequest.IsStreaming is_streaming = 8;</code>
     */
    public Builder setIsStreaming(
        org.apache.spark.connect.proto.AnalyzePlanRequest.IsStreaming.Builder builderForValue) {
      if (isStreamingBuilder_ == null) {
        analyze_ = builderForValue.build();
        onChanged();
      } else {
        isStreamingBuilder_.setMessage(builderForValue.build());
      }
      analyzeCase_ = 8;
      return this;
    }
    /**
     * <code>.spark.connect.AnalyzePlanRequest.IsStreaming is_streaming = 8;</code>
     */
    public Builder mergeIsStreaming(org.apache.spark.connect.proto.AnalyzePlanRequest.IsStreaming value) {
      if (isStreamingBuilder_ == null) {
        if (analyzeCase_ == 8 &&
            analyze_ != org.apache.spark.connect.proto.AnalyzePlanRequest.IsStreaming.getDefaultInstance()) {
          analyze_ = org.apache.spark.connect.proto.AnalyzePlanRequest.IsStreaming.newBuilder((org.apache.spark.connect.proto.AnalyzePlanRequest.IsStreaming) analyze_)
              .mergeFrom(value).buildPartial();
        } else {
          analyze_ = value;
        }
        onChanged();
      } else {
        if (analyzeCase_ == 8) {
          isStreamingBuilder_.mergeFrom(value);
        } else {
          isStreamingBuilder_.setMessage(value);
        }
      }
      analyzeCase_ = 8;
      return this;
    }
    /**
     * <code>.spark.connect.AnalyzePlanRequest.IsStreaming is_streaming = 8;</code>
     */
    public Builder clearIsStreaming() {
      if (isStreamingBuilder_ == null) {
        if (analyzeCase_ == 8) {
          analyzeCase_ = 0;
          analyze_ = null;
          onChanged();
        }
      } else {
        if (analyzeCase_ == 8) {
          analyzeCase_ = 0;
          analyze_ = null;
        }
        isStreamingBuilder_.clear();
      }
      return this;
    }
    /**
     * <code>.spark.connect.AnalyzePlanRequest.IsStreaming is_streaming = 8;</code>
     */
    public org.apache.spark.connect.proto.AnalyzePlanRequest.IsStreaming.Builder getIsStreamingBuilder() {
      return getIsStreamingFieldBuilder().getBuilder();
    }
    /**
     * <code>.spark.connect.AnalyzePlanRequest.IsStreaming is_streaming = 8;</code>
     */
    @Override
    public org.apache.spark.connect.proto.AnalyzePlanRequest.IsStreamingOrBuilder getIsStreamingOrBuilder() {
      if ((analyzeCase_ == 8) && (isStreamingBuilder_ != null)) {
        return isStreamingBuilder_.getMessageOrBuilder();
      } else {
        if (analyzeCase_ == 8) {
          return (org.apache.spark.connect.proto.AnalyzePlanRequest.IsStreaming) analyze_;
        }
        return org.apache.spark.connect.proto.AnalyzePlanRequest.IsStreaming.getDefaultInstance();
      }
    }
    /**
     * <code>.spark.connect.AnalyzePlanRequest.IsStreaming is_streaming = 8;</code>
     */
    private com.google.protobuf.SingleFieldBuilderV3<
        org.apache.spark.connect.proto.AnalyzePlanRequest.IsStreaming, org.apache.spark.connect.proto.AnalyzePlanRequest.IsStreaming.Builder, org.apache.spark.connect.proto.AnalyzePlanRequest.IsStreamingOrBuilder> 
        getIsStreamingFieldBuilder() {
      if (isStreamingBuilder_ == null) {
        if (!(analyzeCase_ == 8)) {
          analyze_ = org.apache.spark.connect.proto.AnalyzePlanRequest.IsStreaming.getDefaultInstance();
        }
        isStreamingBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
            org.apache.spark.connect.proto.AnalyzePlanRequest.IsStreaming, org.apache.spark.connect.proto.AnalyzePlanRequest.IsStreaming.Builder, org.apache.spark.connect.proto.AnalyzePlanRequest.IsStreamingOrBuilder>(
                (org.apache.spark.connect.proto.AnalyzePlanRequest.IsStreaming) analyze_,
                getParentForChildren(),
                isClean());
        analyze_ = null;
      }
      analyzeCase_ = 8;
      onChanged();
      return isStreamingBuilder_;
    }

    private com.google.protobuf.SingleFieldBuilderV3<
        org.apache.spark.connect.proto.AnalyzePlanRequest.InputFiles, org.apache.spark.connect.proto.AnalyzePlanRequest.InputFiles.Builder, org.apache.spark.connect.proto.AnalyzePlanRequest.InputFilesOrBuilder> inputFilesBuilder_;
    /**
     * <code>.spark.connect.AnalyzePlanRequest.InputFiles input_files = 9;</code>
     * @return Whether the inputFiles field is set.
     */
    @Override
    public boolean hasInputFiles() {
      return analyzeCase_ == 9;
    }
    /**
     * <code>.spark.connect.AnalyzePlanRequest.InputFiles input_files = 9;</code>
     * @return The inputFiles.
     */
    @Override
    public org.apache.spark.connect.proto.AnalyzePlanRequest.InputFiles getInputFiles() {
      if (inputFilesBuilder_ == null) {
        if (analyzeCase_ == 9) {
          return (org.apache.spark.connect.proto.AnalyzePlanRequest.InputFiles) analyze_;
        }
        return org.apache.spark.connect.proto.AnalyzePlanRequest.InputFiles.getDefaultInstance();
      } else {
        if (analyzeCase_ == 9) {
          return inputFilesBuilder_.getMessage();
        }
        return org.apache.spark.connect.proto.AnalyzePlanRequest.InputFiles.getDefaultInstance();
      }
    }
    /**
     * <code>.spark.connect.AnalyzePlanRequest.InputFiles input_files = 9;</code>
     */
    public Builder setInputFiles(org.apache.spark.connect.proto.AnalyzePlanRequest.InputFiles value) {
      if (inputFilesBuilder_ == null) {
        if (value == null) {
          throw new NullPointerException();
        }
        analyze_ = value;
        onChanged();
      } else {
        inputFilesBuilder_.setMessage(value);
      }
      analyzeCase_ = 9;
      return this;
    }
    /**
     * <code>.spark.connect.AnalyzePlanRequest.InputFiles input_files = 9;</code>
     */
    public Builder setInputFiles(
        org.apache.spark.connect.proto.AnalyzePlanRequest.InputFiles.Builder builderForValue) {
      if (inputFilesBuilder_ == null) {
        analyze_ = builderForValue.build();
        onChanged();
      } else {
        inputFilesBuilder_.setMessage(builderForValue.build());
      }
      analyzeCase_ = 9;
      return this;
    }
    /**
     * <code>.spark.connect.AnalyzePlanRequest.InputFiles input_files = 9;</code>
     */
    public Builder mergeInputFiles(org.apache.spark.connect.proto.AnalyzePlanRequest.InputFiles value) {
      if (inputFilesBuilder_ == null) {
        if (analyzeCase_ == 9 &&
            analyze_ != org.apache.spark.connect.proto.AnalyzePlanRequest.InputFiles.getDefaultInstance()) {
          analyze_ = org.apache.spark.connect.proto.AnalyzePlanRequest.InputFiles.newBuilder((org.apache.spark.connect.proto.AnalyzePlanRequest.InputFiles) analyze_)
              .mergeFrom(value).buildPartial();
        } else {
          analyze_ = value;
        }
        onChanged();
      } else {
        if (analyzeCase_ == 9) {
          inputFilesBuilder_.mergeFrom(value);
        } else {
          inputFilesBuilder_.setMessage(value);
        }
      }
      analyzeCase_ = 9;
      return this;
    }
    /**
     * <code>.spark.connect.AnalyzePlanRequest.InputFiles input_files = 9;</code>
     */
    public Builder clearInputFiles() {
      if (inputFilesBuilder_ == null) {
        if (analyzeCase_ == 9) {
          analyzeCase_ = 0;
          analyze_ = null;
          onChanged();
        }
      } else {
        if (analyzeCase_ == 9) {
          analyzeCase_ = 0;
          analyze_ = null;
        }
        inputFilesBuilder_.clear();
      }
      return this;
    }
    /**
     * <code>.spark.connect.AnalyzePlanRequest.InputFiles input_files = 9;</code>
     */
    public org.apache.spark.connect.proto.AnalyzePlanRequest.InputFiles.Builder getInputFilesBuilder() {
      return getInputFilesFieldBuilder().getBuilder();
    }
    /**
     * <code>.spark.connect.AnalyzePlanRequest.InputFiles input_files = 9;</code>
     */
    @Override
    public org.apache.spark.connect.proto.AnalyzePlanRequest.InputFilesOrBuilder getInputFilesOrBuilder() {
      if ((analyzeCase_ == 9) && (inputFilesBuilder_ != null)) {
        return inputFilesBuilder_.getMessageOrBuilder();
      } else {
        if (analyzeCase_ == 9) {
          return (org.apache.spark.connect.proto.AnalyzePlanRequest.InputFiles) analyze_;
        }
        return org.apache.spark.connect.proto.AnalyzePlanRequest.InputFiles.getDefaultInstance();
      }
    }
    /**
     * <code>.spark.connect.AnalyzePlanRequest.InputFiles input_files = 9;</code>
     */
    private com.google.protobuf.SingleFieldBuilderV3<
        org.apache.spark.connect.proto.AnalyzePlanRequest.InputFiles, org.apache.spark.connect.proto.AnalyzePlanRequest.InputFiles.Builder, org.apache.spark.connect.proto.AnalyzePlanRequest.InputFilesOrBuilder> 
        getInputFilesFieldBuilder() {
      if (inputFilesBuilder_ == null) {
        if (!(analyzeCase_ == 9)) {
          analyze_ = org.apache.spark.connect.proto.AnalyzePlanRequest.InputFiles.getDefaultInstance();
        }
        inputFilesBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
            org.apache.spark.connect.proto.AnalyzePlanRequest.InputFiles, org.apache.spark.connect.proto.AnalyzePlanRequest.InputFiles.Builder, org.apache.spark.connect.proto.AnalyzePlanRequest.InputFilesOrBuilder>(
                (org.apache.spark.connect.proto.AnalyzePlanRequest.InputFiles) analyze_,
                getParentForChildren(),
                isClean());
        analyze_ = null;
      }
      analyzeCase_ = 9;
      onChanged();
      return inputFilesBuilder_;
    }

    private com.google.protobuf.SingleFieldBuilderV3<
        org.apache.spark.connect.proto.AnalyzePlanRequest.SparkVersion, org.apache.spark.connect.proto.AnalyzePlanRequest.SparkVersion.Builder, org.apache.spark.connect.proto.AnalyzePlanRequest.SparkVersionOrBuilder> sparkVersionBuilder_;
    /**
     * <code>.spark.connect.AnalyzePlanRequest.SparkVersion spark_version = 10;</code>
     * @return Whether the sparkVersion field is set.
     */
    @Override
    public boolean hasSparkVersion() {
      return analyzeCase_ == 10;
    }
    /**
     * <code>.spark.connect.AnalyzePlanRequest.SparkVersion spark_version = 10;</code>
     * @return The sparkVersion.
     */
    @Override
    public org.apache.spark.connect.proto.AnalyzePlanRequest.SparkVersion getSparkVersion() {
      if (sparkVersionBuilder_ == null) {
        if (analyzeCase_ == 10) {
          return (org.apache.spark.connect.proto.AnalyzePlanRequest.SparkVersion) analyze_;
        }
        return org.apache.spark.connect.proto.AnalyzePlanRequest.SparkVersion.getDefaultInstance();
      } else {
        if (analyzeCase_ == 10) {
          return sparkVersionBuilder_.getMessage();
        }
        return org.apache.spark.connect.proto.AnalyzePlanRequest.SparkVersion.getDefaultInstance();
      }
    }
    /**
     * <code>.spark.connect.AnalyzePlanRequest.SparkVersion spark_version = 10;</code>
     */
    public Builder setSparkVersion(org.apache.spark.connect.proto.AnalyzePlanRequest.SparkVersion value) {
      if (sparkVersionBuilder_ == null) {
        if (value == null) {
          throw new NullPointerException();
        }
        analyze_ = value;
        onChanged();
      } else {
        sparkVersionBuilder_.setMessage(value);
      }
      analyzeCase_ = 10;
      return this;
    }
    /**
     * <code>.spark.connect.AnalyzePlanRequest.SparkVersion spark_version = 10;</code>
     */
    public Builder setSparkVersion(
        org.apache.spark.connect.proto.AnalyzePlanRequest.SparkVersion.Builder builderForValue) {
      if (sparkVersionBuilder_ == null) {
        analyze_ = builderForValue.build();
        onChanged();
      } else {
        sparkVersionBuilder_.setMessage(builderForValue.build());
      }
      analyzeCase_ = 10;
      return this;
    }
    /**
     * <code>.spark.connect.AnalyzePlanRequest.SparkVersion spark_version = 10;</code>
     */
    public Builder mergeSparkVersion(org.apache.spark.connect.proto.AnalyzePlanRequest.SparkVersion value) {
      if (sparkVersionBuilder_ == null) {
        if (analyzeCase_ == 10 &&
            analyze_ != org.apache.spark.connect.proto.AnalyzePlanRequest.SparkVersion.getDefaultInstance()) {
          analyze_ = org.apache.spark.connect.proto.AnalyzePlanRequest.SparkVersion.newBuilder((org.apache.spark.connect.proto.AnalyzePlanRequest.SparkVersion) analyze_)
              .mergeFrom(value).buildPartial();
        } else {
          analyze_ = value;
        }
        onChanged();
      } else {
        if (analyzeCase_ == 10) {
          sparkVersionBuilder_.mergeFrom(value);
        } else {
          sparkVersionBuilder_.setMessage(value);
        }
      }
      analyzeCase_ = 10;
      return this;
    }
    /**
     * <code>.spark.connect.AnalyzePlanRequest.SparkVersion spark_version = 10;</code>
     */
    public Builder clearSparkVersion() {
      if (sparkVersionBuilder_ == null) {
        if (analyzeCase_ == 10) {
          analyzeCase_ = 0;
          analyze_ = null;
          onChanged();
        }
      } else {
        if (analyzeCase_ == 10) {
          analyzeCase_ = 0;
          analyze_ = null;
        }
        sparkVersionBuilder_.clear();
      }
      return this;
    }
    /**
     * <code>.spark.connect.AnalyzePlanRequest.SparkVersion spark_version = 10;</code>
     */
    public org.apache.spark.connect.proto.AnalyzePlanRequest.SparkVersion.Builder getSparkVersionBuilder() {
      return getSparkVersionFieldBuilder().getBuilder();
    }
    /**
     * <code>.spark.connect.AnalyzePlanRequest.SparkVersion spark_version = 10;</code>
     */
    @Override
    public org.apache.spark.connect.proto.AnalyzePlanRequest.SparkVersionOrBuilder getSparkVersionOrBuilder() {
      if ((analyzeCase_ == 10) && (sparkVersionBuilder_ != null)) {
        return sparkVersionBuilder_.getMessageOrBuilder();
      } else {
        if (analyzeCase_ == 10) {
          return (org.apache.spark.connect.proto.AnalyzePlanRequest.SparkVersion) analyze_;
        }
        return org.apache.spark.connect.proto.AnalyzePlanRequest.SparkVersion.getDefaultInstance();
      }
    }
    /**
     * <code>.spark.connect.AnalyzePlanRequest.SparkVersion spark_version = 10;</code>
     */
    private com.google.protobuf.SingleFieldBuilderV3<
        org.apache.spark.connect.proto.AnalyzePlanRequest.SparkVersion, org.apache.spark.connect.proto.AnalyzePlanRequest.SparkVersion.Builder, org.apache.spark.connect.proto.AnalyzePlanRequest.SparkVersionOrBuilder> 
        getSparkVersionFieldBuilder() {
      if (sparkVersionBuilder_ == null) {
        if (!(analyzeCase_ == 10)) {
          analyze_ = org.apache.spark.connect.proto.AnalyzePlanRequest.SparkVersion.getDefaultInstance();
        }
        sparkVersionBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
            org.apache.spark.connect.proto.AnalyzePlanRequest.SparkVersion, org.apache.spark.connect.proto.AnalyzePlanRequest.SparkVersion.Builder, org.apache.spark.connect.proto.AnalyzePlanRequest.SparkVersionOrBuilder>(
                (org.apache.spark.connect.proto.AnalyzePlanRequest.SparkVersion) analyze_,
                getParentForChildren(),
                isClean());
        analyze_ = null;
      }
      analyzeCase_ = 10;
      onChanged();
      return sparkVersionBuilder_;
    }

    private com.google.protobuf.SingleFieldBuilderV3<
        org.apache.spark.connect.proto.AnalyzePlanRequest.DDLParse, org.apache.spark.connect.proto.AnalyzePlanRequest.DDLParse.Builder, org.apache.spark.connect.proto.AnalyzePlanRequest.DDLParseOrBuilder> ddlParseBuilder_;
    /**
     * <code>.spark.connect.AnalyzePlanRequest.DDLParse ddl_parse = 11;</code>
     * @return Whether the ddlParse field is set.
     */
    @Override
    public boolean hasDdlParse() {
      return analyzeCase_ == 11;
    }
    /**
     * <code>.spark.connect.AnalyzePlanRequest.DDLParse ddl_parse = 11;</code>
     * @return The ddlParse.
     */
    @Override
    public org.apache.spark.connect.proto.AnalyzePlanRequest.DDLParse getDdlParse() {
      if (ddlParseBuilder_ == null) {
        if (analyzeCase_ == 11) {
          return (org.apache.spark.connect.proto.AnalyzePlanRequest.DDLParse) analyze_;
        }
        return org.apache.spark.connect.proto.AnalyzePlanRequest.DDLParse.getDefaultInstance();
      } else {
        if (analyzeCase_ == 11) {
          return ddlParseBuilder_.getMessage();
        }
        return org.apache.spark.connect.proto.AnalyzePlanRequest.DDLParse.getDefaultInstance();
      }
    }
    /**
     * <code>.spark.connect.AnalyzePlanRequest.DDLParse ddl_parse = 11;</code>
     */
    public Builder setDdlParse(org.apache.spark.connect.proto.AnalyzePlanRequest.DDLParse value) {
      if (ddlParseBuilder_ == null) {
        if (value == null) {
          throw new NullPointerException();
        }
        analyze_ = value;
        onChanged();
      } else {
        ddlParseBuilder_.setMessage(value);
      }
      analyzeCase_ = 11;
      return this;
    }
    /**
     * <code>.spark.connect.AnalyzePlanRequest.DDLParse ddl_parse = 11;</code>
     */
    public Builder setDdlParse(
        org.apache.spark.connect.proto.AnalyzePlanRequest.DDLParse.Builder builderForValue) {
      if (ddlParseBuilder_ == null) {
        analyze_ = builderForValue.build();
        onChanged();
      } else {
        ddlParseBuilder_.setMessage(builderForValue.build());
      }
      analyzeCase_ = 11;
      return this;
    }
    /**
     * <code>.spark.connect.AnalyzePlanRequest.DDLParse ddl_parse = 11;</code>
     */
    public Builder mergeDdlParse(org.apache.spark.connect.proto.AnalyzePlanRequest.DDLParse value) {
      if (ddlParseBuilder_ == null) {
        if (analyzeCase_ == 11 &&
            analyze_ != org.apache.spark.connect.proto.AnalyzePlanRequest.DDLParse.getDefaultInstance()) {
          analyze_ = org.apache.spark.connect.proto.AnalyzePlanRequest.DDLParse.newBuilder((org.apache.spark.connect.proto.AnalyzePlanRequest.DDLParse) analyze_)
              .mergeFrom(value).buildPartial();
        } else {
          analyze_ = value;
        }
        onChanged();
      } else {
        if (analyzeCase_ == 11) {
          ddlParseBuilder_.mergeFrom(value);
        } else {
          ddlParseBuilder_.setMessage(value);
        }
      }
      analyzeCase_ = 11;
      return this;
    }
    /**
     * <code>.spark.connect.AnalyzePlanRequest.DDLParse ddl_parse = 11;</code>
     */
    public Builder clearDdlParse() {
      if (ddlParseBuilder_ == null) {
        if (analyzeCase_ == 11) {
          analyzeCase_ = 0;
          analyze_ = null;
          onChanged();
        }
      } else {
        if (analyzeCase_ == 11) {
          analyzeCase_ = 0;
          analyze_ = null;
        }
        ddlParseBuilder_.clear();
      }
      return this;
    }
    /**
     * <code>.spark.connect.AnalyzePlanRequest.DDLParse ddl_parse = 11;</code>
     */
    public org.apache.spark.connect.proto.AnalyzePlanRequest.DDLParse.Builder getDdlParseBuilder() {
      return getDdlParseFieldBuilder().getBuilder();
    }
    /**
     * <code>.spark.connect.AnalyzePlanRequest.DDLParse ddl_parse = 11;</code>
     */
    @Override
    public org.apache.spark.connect.proto.AnalyzePlanRequest.DDLParseOrBuilder getDdlParseOrBuilder() {
      if ((analyzeCase_ == 11) && (ddlParseBuilder_ != null)) {
        return ddlParseBuilder_.getMessageOrBuilder();
      } else {
        if (analyzeCase_ == 11) {
          return (org.apache.spark.connect.proto.AnalyzePlanRequest.DDLParse) analyze_;
        }
        return org.apache.spark.connect.proto.AnalyzePlanRequest.DDLParse.getDefaultInstance();
      }
    }
    /**
     * <code>.spark.connect.AnalyzePlanRequest.DDLParse ddl_parse = 11;</code>
     */
    private com.google.protobuf.SingleFieldBuilderV3<
        org.apache.spark.connect.proto.AnalyzePlanRequest.DDLParse, org.apache.spark.connect.proto.AnalyzePlanRequest.DDLParse.Builder, org.apache.spark.connect.proto.AnalyzePlanRequest.DDLParseOrBuilder> 
        getDdlParseFieldBuilder() {
      if (ddlParseBuilder_ == null) {
        if (!(analyzeCase_ == 11)) {
          analyze_ = org.apache.spark.connect.proto.AnalyzePlanRequest.DDLParse.getDefaultInstance();
        }
        ddlParseBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
            org.apache.spark.connect.proto.AnalyzePlanRequest.DDLParse, org.apache.spark.connect.proto.AnalyzePlanRequest.DDLParse.Builder, org.apache.spark.connect.proto.AnalyzePlanRequest.DDLParseOrBuilder>(
                (org.apache.spark.connect.proto.AnalyzePlanRequest.DDLParse) analyze_,
                getParentForChildren(),
                isClean());
        analyze_ = null;
      }
      analyzeCase_ = 11;
      onChanged();
      return ddlParseBuilder_;
    }

    private com.google.protobuf.SingleFieldBuilderV3<
        org.apache.spark.connect.proto.AnalyzePlanRequest.SameSemantics, org.apache.spark.connect.proto.AnalyzePlanRequest.SameSemantics.Builder, org.apache.spark.connect.proto.AnalyzePlanRequest.SameSemanticsOrBuilder> sameSemanticsBuilder_;
    /**
     * <code>.spark.connect.AnalyzePlanRequest.SameSemantics same_semantics = 12;</code>
     * @return Whether the sameSemantics field is set.
     */
    @Override
    public boolean hasSameSemantics() {
      return analyzeCase_ == 12;
    }
    /**
     * <code>.spark.connect.AnalyzePlanRequest.SameSemantics same_semantics = 12;</code>
     * @return The sameSemantics.
     */
    @Override
    public org.apache.spark.connect.proto.AnalyzePlanRequest.SameSemantics getSameSemantics() {
      if (sameSemanticsBuilder_ == null) {
        if (analyzeCase_ == 12) {
          return (org.apache.spark.connect.proto.AnalyzePlanRequest.SameSemantics) analyze_;
        }
        return org.apache.spark.connect.proto.AnalyzePlanRequest.SameSemantics.getDefaultInstance();
      } else {
        if (analyzeCase_ == 12) {
          return sameSemanticsBuilder_.getMessage();
        }
        return org.apache.spark.connect.proto.AnalyzePlanRequest.SameSemantics.getDefaultInstance();
      }
    }
    /**
     * <code>.spark.connect.AnalyzePlanRequest.SameSemantics same_semantics = 12;</code>
     */
    public Builder setSameSemantics(org.apache.spark.connect.proto.AnalyzePlanRequest.SameSemantics value) {
      if (sameSemanticsBuilder_ == null) {
        if (value == null) {
          throw new NullPointerException();
        }
        analyze_ = value;
        onChanged();
      } else {
        sameSemanticsBuilder_.setMessage(value);
      }
      analyzeCase_ = 12;
      return this;
    }
    /**
     * <code>.spark.connect.AnalyzePlanRequest.SameSemantics same_semantics = 12;</code>
     */
    public Builder setSameSemantics(
        org.apache.spark.connect.proto.AnalyzePlanRequest.SameSemantics.Builder builderForValue) {
      if (sameSemanticsBuilder_ == null) {
        analyze_ = builderForValue.build();
        onChanged();
      } else {
        sameSemanticsBuilder_.setMessage(builderForValue.build());
      }
      analyzeCase_ = 12;
      return this;
    }
    /**
     * <code>.spark.connect.AnalyzePlanRequest.SameSemantics same_semantics = 12;</code>
     */
    public Builder mergeSameSemantics(org.apache.spark.connect.proto.AnalyzePlanRequest.SameSemantics value) {
      if (sameSemanticsBuilder_ == null) {
        if (analyzeCase_ == 12 &&
            analyze_ != org.apache.spark.connect.proto.AnalyzePlanRequest.SameSemantics.getDefaultInstance()) {
          analyze_ = org.apache.spark.connect.proto.AnalyzePlanRequest.SameSemantics.newBuilder((org.apache.spark.connect.proto.AnalyzePlanRequest.SameSemantics) analyze_)
              .mergeFrom(value).buildPartial();
        } else {
          analyze_ = value;
        }
        onChanged();
      } else {
        if (analyzeCase_ == 12) {
          sameSemanticsBuilder_.mergeFrom(value);
        } else {
          sameSemanticsBuilder_.setMessage(value);
        }
      }
      analyzeCase_ = 12;
      return this;
    }
    /**
     * <code>.spark.connect.AnalyzePlanRequest.SameSemantics same_semantics = 12;</code>
     */
    public Builder clearSameSemantics() {
      if (sameSemanticsBuilder_ == null) {
        if (analyzeCase_ == 12) {
          analyzeCase_ = 0;
          analyze_ = null;
          onChanged();
        }
      } else {
        if (analyzeCase_ == 12) {
          analyzeCase_ = 0;
          analyze_ = null;
        }
        sameSemanticsBuilder_.clear();
      }
      return this;
    }
    /**
     * <code>.spark.connect.AnalyzePlanRequest.SameSemantics same_semantics = 12;</code>
     */
    public org.apache.spark.connect.proto.AnalyzePlanRequest.SameSemantics.Builder getSameSemanticsBuilder() {
      return getSameSemanticsFieldBuilder().getBuilder();
    }
    /**
     * <code>.spark.connect.AnalyzePlanRequest.SameSemantics same_semantics = 12;</code>
     */
    @Override
    public org.apache.spark.connect.proto.AnalyzePlanRequest.SameSemanticsOrBuilder getSameSemanticsOrBuilder() {
      if ((analyzeCase_ == 12) && (sameSemanticsBuilder_ != null)) {
        return sameSemanticsBuilder_.getMessageOrBuilder();
      } else {
        if (analyzeCase_ == 12) {
          return (org.apache.spark.connect.proto.AnalyzePlanRequest.SameSemantics) analyze_;
        }
        return org.apache.spark.connect.proto.AnalyzePlanRequest.SameSemantics.getDefaultInstance();
      }
    }
    /**
     * <code>.spark.connect.AnalyzePlanRequest.SameSemantics same_semantics = 12;</code>
     */
    private com.google.protobuf.SingleFieldBuilderV3<
        org.apache.spark.connect.proto.AnalyzePlanRequest.SameSemantics, org.apache.spark.connect.proto.AnalyzePlanRequest.SameSemantics.Builder, org.apache.spark.connect.proto.AnalyzePlanRequest.SameSemanticsOrBuilder> 
        getSameSemanticsFieldBuilder() {
      if (sameSemanticsBuilder_ == null) {
        if (!(analyzeCase_ == 12)) {
          analyze_ = org.apache.spark.connect.proto.AnalyzePlanRequest.SameSemantics.getDefaultInstance();
        }
        sameSemanticsBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
            org.apache.spark.connect.proto.AnalyzePlanRequest.SameSemantics, org.apache.spark.connect.proto.AnalyzePlanRequest.SameSemantics.Builder, org.apache.spark.connect.proto.AnalyzePlanRequest.SameSemanticsOrBuilder>(
                (org.apache.spark.connect.proto.AnalyzePlanRequest.SameSemantics) analyze_,
                getParentForChildren(),
                isClean());
        analyze_ = null;
      }
      analyzeCase_ = 12;
      onChanged();
      return sameSemanticsBuilder_;
    }

    private com.google.protobuf.SingleFieldBuilderV3<
        org.apache.spark.connect.proto.AnalyzePlanRequest.SemanticHash, org.apache.spark.connect.proto.AnalyzePlanRequest.SemanticHash.Builder, org.apache.spark.connect.proto.AnalyzePlanRequest.SemanticHashOrBuilder> semanticHashBuilder_;
    /**
     * <code>.spark.connect.AnalyzePlanRequest.SemanticHash semantic_hash = 13;</code>
     * @return Whether the semanticHash field is set.
     */
    @Override
    public boolean hasSemanticHash() {
      return analyzeCase_ == 13;
    }
    /**
     * <code>.spark.connect.AnalyzePlanRequest.SemanticHash semantic_hash = 13;</code>
     * @return The semanticHash.
     */
    @Override
    public org.apache.spark.connect.proto.AnalyzePlanRequest.SemanticHash getSemanticHash() {
      if (semanticHashBuilder_ == null) {
        if (analyzeCase_ == 13) {
          return (org.apache.spark.connect.proto.AnalyzePlanRequest.SemanticHash) analyze_;
        }
        return org.apache.spark.connect.proto.AnalyzePlanRequest.SemanticHash.getDefaultInstance();
      } else {
        if (analyzeCase_ == 13) {
          return semanticHashBuilder_.getMessage();
        }
        return org.apache.spark.connect.proto.AnalyzePlanRequest.SemanticHash.getDefaultInstance();
      }
    }
    /**
     * <code>.spark.connect.AnalyzePlanRequest.SemanticHash semantic_hash = 13;</code>
     */
    public Builder setSemanticHash(org.apache.spark.connect.proto.AnalyzePlanRequest.SemanticHash value) {
      if (semanticHashBuilder_ == null) {
        if (value == null) {
          throw new NullPointerException();
        }
        analyze_ = value;
        onChanged();
      } else {
        semanticHashBuilder_.setMessage(value);
      }
      analyzeCase_ = 13;
      return this;
    }
    /**
     * <code>.spark.connect.AnalyzePlanRequest.SemanticHash semantic_hash = 13;</code>
     */
    public Builder setSemanticHash(
        org.apache.spark.connect.proto.AnalyzePlanRequest.SemanticHash.Builder builderForValue) {
      if (semanticHashBuilder_ == null) {
        analyze_ = builderForValue.build();
        onChanged();
      } else {
        semanticHashBuilder_.setMessage(builderForValue.build());
      }
      analyzeCase_ = 13;
      return this;
    }
    /**
     * <code>.spark.connect.AnalyzePlanRequest.SemanticHash semantic_hash = 13;</code>
     */
    public Builder mergeSemanticHash(org.apache.spark.connect.proto.AnalyzePlanRequest.SemanticHash value) {
      if (semanticHashBuilder_ == null) {
        if (analyzeCase_ == 13 &&
            analyze_ != org.apache.spark.connect.proto.AnalyzePlanRequest.SemanticHash.getDefaultInstance()) {
          analyze_ = org.apache.spark.connect.proto.AnalyzePlanRequest.SemanticHash.newBuilder((org.apache.spark.connect.proto.AnalyzePlanRequest.SemanticHash) analyze_)
              .mergeFrom(value).buildPartial();
        } else {
          analyze_ = value;
        }
        onChanged();
      } else {
        if (analyzeCase_ == 13) {
          semanticHashBuilder_.mergeFrom(value);
        } else {
          semanticHashBuilder_.setMessage(value);
        }
      }
      analyzeCase_ = 13;
      return this;
    }
    /**
     * <code>.spark.connect.AnalyzePlanRequest.SemanticHash semantic_hash = 13;</code>
     */
    public Builder clearSemanticHash() {
      if (semanticHashBuilder_ == null) {
        if (analyzeCase_ == 13) {
          analyzeCase_ = 0;
          analyze_ = null;
          onChanged();
        }
      } else {
        if (analyzeCase_ == 13) {
          analyzeCase_ = 0;
          analyze_ = null;
        }
        semanticHashBuilder_.clear();
      }
      return this;
    }
    /**
     * <code>.spark.connect.AnalyzePlanRequest.SemanticHash semantic_hash = 13;</code>
     */
    public org.apache.spark.connect.proto.AnalyzePlanRequest.SemanticHash.Builder getSemanticHashBuilder() {
      return getSemanticHashFieldBuilder().getBuilder();
    }
    /**
     * <code>.spark.connect.AnalyzePlanRequest.SemanticHash semantic_hash = 13;</code>
     */
    @Override
    public org.apache.spark.connect.proto.AnalyzePlanRequest.SemanticHashOrBuilder getSemanticHashOrBuilder() {
      if ((analyzeCase_ == 13) && (semanticHashBuilder_ != null)) {
        return semanticHashBuilder_.getMessageOrBuilder();
      } else {
        if (analyzeCase_ == 13) {
          return (org.apache.spark.connect.proto.AnalyzePlanRequest.SemanticHash) analyze_;
        }
        return org.apache.spark.connect.proto.AnalyzePlanRequest.SemanticHash.getDefaultInstance();
      }
    }
    /**
     * <code>.spark.connect.AnalyzePlanRequest.SemanticHash semantic_hash = 13;</code>
     */
    private com.google.protobuf.SingleFieldBuilderV3<
        org.apache.spark.connect.proto.AnalyzePlanRequest.SemanticHash, org.apache.spark.connect.proto.AnalyzePlanRequest.SemanticHash.Builder, org.apache.spark.connect.proto.AnalyzePlanRequest.SemanticHashOrBuilder> 
        getSemanticHashFieldBuilder() {
      if (semanticHashBuilder_ == null) {
        if (!(analyzeCase_ == 13)) {
          analyze_ = org.apache.spark.connect.proto.AnalyzePlanRequest.SemanticHash.getDefaultInstance();
        }
        semanticHashBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
            org.apache.spark.connect.proto.AnalyzePlanRequest.SemanticHash, org.apache.spark.connect.proto.AnalyzePlanRequest.SemanticHash.Builder, org.apache.spark.connect.proto.AnalyzePlanRequest.SemanticHashOrBuilder>(
                (org.apache.spark.connect.proto.AnalyzePlanRequest.SemanticHash) analyze_,
                getParentForChildren(),
                isClean());
        analyze_ = null;
      }
      analyzeCase_ = 13;
      onChanged();
      return semanticHashBuilder_;
    }

    private com.google.protobuf.SingleFieldBuilderV3<
        org.apache.spark.connect.proto.AnalyzePlanRequest.Persist, org.apache.spark.connect.proto.AnalyzePlanRequest.Persist.Builder, org.apache.spark.connect.proto.AnalyzePlanRequest.PersistOrBuilder> persistBuilder_;
    /**
     * <code>.spark.connect.AnalyzePlanRequest.Persist persist = 14;</code>
     * @return Whether the persist field is set.
     */
    @Override
    public boolean hasPersist() {
      return analyzeCase_ == 14;
    }
    /**
     * <code>.spark.connect.AnalyzePlanRequest.Persist persist = 14;</code>
     * @return The persist.
     */
    @Override
    public org.apache.spark.connect.proto.AnalyzePlanRequest.Persist getPersist() {
      if (persistBuilder_ == null) {
        if (analyzeCase_ == 14) {
          return (org.apache.spark.connect.proto.AnalyzePlanRequest.Persist) analyze_;
        }
        return org.apache.spark.connect.proto.AnalyzePlanRequest.Persist.getDefaultInstance();
      } else {
        if (analyzeCase_ == 14) {
          return persistBuilder_.getMessage();
        }
        return org.apache.spark.connect.proto.AnalyzePlanRequest.Persist.getDefaultInstance();
      }
    }
    /**
     * <code>.spark.connect.AnalyzePlanRequest.Persist persist = 14;</code>
     */
    public Builder setPersist(org.apache.spark.connect.proto.AnalyzePlanRequest.Persist value) {
      if (persistBuilder_ == null) {
        if (value == null) {
          throw new NullPointerException();
        }
        analyze_ = value;
        onChanged();
      } else {
        persistBuilder_.setMessage(value);
      }
      analyzeCase_ = 14;
      return this;
    }
    /**
     * <code>.spark.connect.AnalyzePlanRequest.Persist persist = 14;</code>
     */
    public Builder setPersist(
        org.apache.spark.connect.proto.AnalyzePlanRequest.Persist.Builder builderForValue) {
      if (persistBuilder_ == null) {
        analyze_ = builderForValue.build();
        onChanged();
      } else {
        persistBuilder_.setMessage(builderForValue.build());
      }
      analyzeCase_ = 14;
      return this;
    }
    /**
     * <code>.spark.connect.AnalyzePlanRequest.Persist persist = 14;</code>
     */
    public Builder mergePersist(org.apache.spark.connect.proto.AnalyzePlanRequest.Persist value) {
      if (persistBuilder_ == null) {
        if (analyzeCase_ == 14 &&
            analyze_ != org.apache.spark.connect.proto.AnalyzePlanRequest.Persist.getDefaultInstance()) {
          analyze_ = org.apache.spark.connect.proto.AnalyzePlanRequest.Persist.newBuilder((org.apache.spark.connect.proto.AnalyzePlanRequest.Persist) analyze_)
              .mergeFrom(value).buildPartial();
        } else {
          analyze_ = value;
        }
        onChanged();
      } else {
        if (analyzeCase_ == 14) {
          persistBuilder_.mergeFrom(value);
        } else {
          persistBuilder_.setMessage(value);
        }
      }
      analyzeCase_ = 14;
      return this;
    }
    /**
     * <code>.spark.connect.AnalyzePlanRequest.Persist persist = 14;</code>
     */
    public Builder clearPersist() {
      if (persistBuilder_ == null) {
        if (analyzeCase_ == 14) {
          analyzeCase_ = 0;
          analyze_ = null;
          onChanged();
        }
      } else {
        if (analyzeCase_ == 14) {
          analyzeCase_ = 0;
          analyze_ = null;
        }
        persistBuilder_.clear();
      }
      return this;
    }
    /**
     * <code>.spark.connect.AnalyzePlanRequest.Persist persist = 14;</code>
     */
    public org.apache.spark.connect.proto.AnalyzePlanRequest.Persist.Builder getPersistBuilder() {
      return getPersistFieldBuilder().getBuilder();
    }
    /**
     * <code>.spark.connect.AnalyzePlanRequest.Persist persist = 14;</code>
     */
    @Override
    public org.apache.spark.connect.proto.AnalyzePlanRequest.PersistOrBuilder getPersistOrBuilder() {
      if ((analyzeCase_ == 14) && (persistBuilder_ != null)) {
        return persistBuilder_.getMessageOrBuilder();
      } else {
        if (analyzeCase_ == 14) {
          return (org.apache.spark.connect.proto.AnalyzePlanRequest.Persist) analyze_;
        }
        return org.apache.spark.connect.proto.AnalyzePlanRequest.Persist.getDefaultInstance();
      }
    }
    /**
     * <code>.spark.connect.AnalyzePlanRequest.Persist persist = 14;</code>
     */
    private com.google.protobuf.SingleFieldBuilderV3<
        org.apache.spark.connect.proto.AnalyzePlanRequest.Persist, org.apache.spark.connect.proto.AnalyzePlanRequest.Persist.Builder, org.apache.spark.connect.proto.AnalyzePlanRequest.PersistOrBuilder> 
        getPersistFieldBuilder() {
      if (persistBuilder_ == null) {
        if (!(analyzeCase_ == 14)) {
          analyze_ = org.apache.spark.connect.proto.AnalyzePlanRequest.Persist.getDefaultInstance();
        }
        persistBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
            org.apache.spark.connect.proto.AnalyzePlanRequest.Persist, org.apache.spark.connect.proto.AnalyzePlanRequest.Persist.Builder, org.apache.spark.connect.proto.AnalyzePlanRequest.PersistOrBuilder>(
                (org.apache.spark.connect.proto.AnalyzePlanRequest.Persist) analyze_,
                getParentForChildren(),
                isClean());
        analyze_ = null;
      }
      analyzeCase_ = 14;
      onChanged();
      return persistBuilder_;
    }

    private com.google.protobuf.SingleFieldBuilderV3<
        org.apache.spark.connect.proto.AnalyzePlanRequest.Unpersist, org.apache.spark.connect.proto.AnalyzePlanRequest.Unpersist.Builder, org.apache.spark.connect.proto.AnalyzePlanRequest.UnpersistOrBuilder> unpersistBuilder_;
    /**
     * <code>.spark.connect.AnalyzePlanRequest.Unpersist unpersist = 15;</code>
     * @return Whether the unpersist field is set.
     */
    @Override
    public boolean hasUnpersist() {
      return analyzeCase_ == 15;
    }
    /**
     * <code>.spark.connect.AnalyzePlanRequest.Unpersist unpersist = 15;</code>
     * @return The unpersist.
     */
    @Override
    public org.apache.spark.connect.proto.AnalyzePlanRequest.Unpersist getUnpersist() {
      if (unpersistBuilder_ == null) {
        if (analyzeCase_ == 15) {
          return (org.apache.spark.connect.proto.AnalyzePlanRequest.Unpersist) analyze_;
        }
        return org.apache.spark.connect.proto.AnalyzePlanRequest.Unpersist.getDefaultInstance();
      } else {
        if (analyzeCase_ == 15) {
          return unpersistBuilder_.getMessage();
        }
        return org.apache.spark.connect.proto.AnalyzePlanRequest.Unpersist.getDefaultInstance();
      }
    }
    /**
     * <code>.spark.connect.AnalyzePlanRequest.Unpersist unpersist = 15;</code>
     */
    public Builder setUnpersist(org.apache.spark.connect.proto.AnalyzePlanRequest.Unpersist value) {
      if (unpersistBuilder_ == null) {
        if (value == null) {
          throw new NullPointerException();
        }
        analyze_ = value;
        onChanged();
      } else {
        unpersistBuilder_.setMessage(value);
      }
      analyzeCase_ = 15;
      return this;
    }
    /**
     * <code>.spark.connect.AnalyzePlanRequest.Unpersist unpersist = 15;</code>
     */
    public Builder setUnpersist(
        org.apache.spark.connect.proto.AnalyzePlanRequest.Unpersist.Builder builderForValue) {
      if (unpersistBuilder_ == null) {
        analyze_ = builderForValue.build();
        onChanged();
      } else {
        unpersistBuilder_.setMessage(builderForValue.build());
      }
      analyzeCase_ = 15;
      return this;
    }
    /**
     * <code>.spark.connect.AnalyzePlanRequest.Unpersist unpersist = 15;</code>
     */
    public Builder mergeUnpersist(org.apache.spark.connect.proto.AnalyzePlanRequest.Unpersist value) {
      if (unpersistBuilder_ == null) {
        if (analyzeCase_ == 15 &&
            analyze_ != org.apache.spark.connect.proto.AnalyzePlanRequest.Unpersist.getDefaultInstance()) {
          analyze_ = org.apache.spark.connect.proto.AnalyzePlanRequest.Unpersist.newBuilder((org.apache.spark.connect.proto.AnalyzePlanRequest.Unpersist) analyze_)
              .mergeFrom(value).buildPartial();
        } else {
          analyze_ = value;
        }
        onChanged();
      } else {
        if (analyzeCase_ == 15) {
          unpersistBuilder_.mergeFrom(value);
        } else {
          unpersistBuilder_.setMessage(value);
        }
      }
      analyzeCase_ = 15;
      return this;
    }
    /**
     * <code>.spark.connect.AnalyzePlanRequest.Unpersist unpersist = 15;</code>
     */
    public Builder clearUnpersist() {
      if (unpersistBuilder_ == null) {
        if (analyzeCase_ == 15) {
          analyzeCase_ = 0;
          analyze_ = null;
          onChanged();
        }
      } else {
        if (analyzeCase_ == 15) {
          analyzeCase_ = 0;
          analyze_ = null;
        }
        unpersistBuilder_.clear();
      }
      return this;
    }
    /**
     * <code>.spark.connect.AnalyzePlanRequest.Unpersist unpersist = 15;</code>
     */
    public org.apache.spark.connect.proto.AnalyzePlanRequest.Unpersist.Builder getUnpersistBuilder() {
      return getUnpersistFieldBuilder().getBuilder();
    }
    /**
     * <code>.spark.connect.AnalyzePlanRequest.Unpersist unpersist = 15;</code>
     */
    @Override
    public org.apache.spark.connect.proto.AnalyzePlanRequest.UnpersistOrBuilder getUnpersistOrBuilder() {
      if ((analyzeCase_ == 15) && (unpersistBuilder_ != null)) {
        return unpersistBuilder_.getMessageOrBuilder();
      } else {
        if (analyzeCase_ == 15) {
          return (org.apache.spark.connect.proto.AnalyzePlanRequest.Unpersist) analyze_;
        }
        return org.apache.spark.connect.proto.AnalyzePlanRequest.Unpersist.getDefaultInstance();
      }
    }
    /**
     * <code>.spark.connect.AnalyzePlanRequest.Unpersist unpersist = 15;</code>
     */
    private com.google.protobuf.SingleFieldBuilderV3<
        org.apache.spark.connect.proto.AnalyzePlanRequest.Unpersist, org.apache.spark.connect.proto.AnalyzePlanRequest.Unpersist.Builder, org.apache.spark.connect.proto.AnalyzePlanRequest.UnpersistOrBuilder> 
        getUnpersistFieldBuilder() {
      if (unpersistBuilder_ == null) {
        if (!(analyzeCase_ == 15)) {
          analyze_ = org.apache.spark.connect.proto.AnalyzePlanRequest.Unpersist.getDefaultInstance();
        }
        unpersistBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
            org.apache.spark.connect.proto.AnalyzePlanRequest.Unpersist, org.apache.spark.connect.proto.AnalyzePlanRequest.Unpersist.Builder, org.apache.spark.connect.proto.AnalyzePlanRequest.UnpersistOrBuilder>(
                (org.apache.spark.connect.proto.AnalyzePlanRequest.Unpersist) analyze_,
                getParentForChildren(),
                isClean());
        analyze_ = null;
      }
      analyzeCase_ = 15;
      onChanged();
      return unpersistBuilder_;
    }

    private com.google.protobuf.SingleFieldBuilderV3<
        org.apache.spark.connect.proto.AnalyzePlanRequest.GetStorageLevel, org.apache.spark.connect.proto.AnalyzePlanRequest.GetStorageLevel.Builder, org.apache.spark.connect.proto.AnalyzePlanRequest.GetStorageLevelOrBuilder> getStorageLevelBuilder_;
    /**
     * <code>.spark.connect.AnalyzePlanRequest.GetStorageLevel get_storage_level = 16;</code>
     * @return Whether the getStorageLevel field is set.
     */
    @Override
    public boolean hasGetStorageLevel() {
      return analyzeCase_ == 16;
    }
    /**
     * <code>.spark.connect.AnalyzePlanRequest.GetStorageLevel get_storage_level = 16;</code>
     * @return The getStorageLevel.
     */
    @Override
    public org.apache.spark.connect.proto.AnalyzePlanRequest.GetStorageLevel getGetStorageLevel() {
      if (getStorageLevelBuilder_ == null) {
        if (analyzeCase_ == 16) {
          return (org.apache.spark.connect.proto.AnalyzePlanRequest.GetStorageLevel) analyze_;
        }
        return org.apache.spark.connect.proto.AnalyzePlanRequest.GetStorageLevel.getDefaultInstance();
      } else {
        if (analyzeCase_ == 16) {
          return getStorageLevelBuilder_.getMessage();
        }
        return org.apache.spark.connect.proto.AnalyzePlanRequest.GetStorageLevel.getDefaultInstance();
      }
    }
    /**
     * <code>.spark.connect.AnalyzePlanRequest.GetStorageLevel get_storage_level = 16;</code>
     */
    public Builder setGetStorageLevel(org.apache.spark.connect.proto.AnalyzePlanRequest.GetStorageLevel value) {
      if (getStorageLevelBuilder_ == null) {
        if (value == null) {
          throw new NullPointerException();
        }
        analyze_ = value;
        onChanged();
      } else {
        getStorageLevelBuilder_.setMessage(value);
      }
      analyzeCase_ = 16;
      return this;
    }
    /**
     * <code>.spark.connect.AnalyzePlanRequest.GetStorageLevel get_storage_level = 16;</code>
     */
    public Builder setGetStorageLevel(
        org.apache.spark.connect.proto.AnalyzePlanRequest.GetStorageLevel.Builder builderForValue) {
      if (getStorageLevelBuilder_ == null) {
        analyze_ = builderForValue.build();
        onChanged();
      } else {
        getStorageLevelBuilder_.setMessage(builderForValue.build());
      }
      analyzeCase_ = 16;
      return this;
    }
    /**
     * <code>.spark.connect.AnalyzePlanRequest.GetStorageLevel get_storage_level = 16;</code>
     */
    public Builder mergeGetStorageLevel(org.apache.spark.connect.proto.AnalyzePlanRequest.GetStorageLevel value) {
      if (getStorageLevelBuilder_ == null) {
        if (analyzeCase_ == 16 &&
            analyze_ != org.apache.spark.connect.proto.AnalyzePlanRequest.GetStorageLevel.getDefaultInstance()) {
          analyze_ = org.apache.spark.connect.proto.AnalyzePlanRequest.GetStorageLevel.newBuilder((org.apache.spark.connect.proto.AnalyzePlanRequest.GetStorageLevel) analyze_)
              .mergeFrom(value).buildPartial();
        } else {
          analyze_ = value;
        }
        onChanged();
      } else {
        if (analyzeCase_ == 16) {
          getStorageLevelBuilder_.mergeFrom(value);
        } else {
          getStorageLevelBuilder_.setMessage(value);
        }
      }
      analyzeCase_ = 16;
      return this;
    }
    /**
     * <code>.spark.connect.AnalyzePlanRequest.GetStorageLevel get_storage_level = 16;</code>
     */
    public Builder clearGetStorageLevel() {
      if (getStorageLevelBuilder_ == null) {
        if (analyzeCase_ == 16) {
          analyzeCase_ = 0;
          analyze_ = null;
          onChanged();
        }
      } else {
        if (analyzeCase_ == 16) {
          analyzeCase_ = 0;
          analyze_ = null;
        }
        getStorageLevelBuilder_.clear();
      }
      return this;
    }
    /**
     * <code>.spark.connect.AnalyzePlanRequest.GetStorageLevel get_storage_level = 16;</code>
     */
    public org.apache.spark.connect.proto.AnalyzePlanRequest.GetStorageLevel.Builder getGetStorageLevelBuilder() {
      return getGetStorageLevelFieldBuilder().getBuilder();
    }
    /**
     * <code>.spark.connect.AnalyzePlanRequest.GetStorageLevel get_storage_level = 16;</code>
     */
    @Override
    public org.apache.spark.connect.proto.AnalyzePlanRequest.GetStorageLevelOrBuilder getGetStorageLevelOrBuilder() {
      if ((analyzeCase_ == 16) && (getStorageLevelBuilder_ != null)) {
        return getStorageLevelBuilder_.getMessageOrBuilder();
      } else {
        if (analyzeCase_ == 16) {
          return (org.apache.spark.connect.proto.AnalyzePlanRequest.GetStorageLevel) analyze_;
        }
        return org.apache.spark.connect.proto.AnalyzePlanRequest.GetStorageLevel.getDefaultInstance();
      }
    }
    /**
     * <code>.spark.connect.AnalyzePlanRequest.GetStorageLevel get_storage_level = 16;</code>
     */
    private com.google.protobuf.SingleFieldBuilderV3<
        org.apache.spark.connect.proto.AnalyzePlanRequest.GetStorageLevel, org.apache.spark.connect.proto.AnalyzePlanRequest.GetStorageLevel.Builder, org.apache.spark.connect.proto.AnalyzePlanRequest.GetStorageLevelOrBuilder> 
        getGetStorageLevelFieldBuilder() {
      if (getStorageLevelBuilder_ == null) {
        if (!(analyzeCase_ == 16)) {
          analyze_ = org.apache.spark.connect.proto.AnalyzePlanRequest.GetStorageLevel.getDefaultInstance();
        }
        getStorageLevelBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
            org.apache.spark.connect.proto.AnalyzePlanRequest.GetStorageLevel, org.apache.spark.connect.proto.AnalyzePlanRequest.GetStorageLevel.Builder, org.apache.spark.connect.proto.AnalyzePlanRequest.GetStorageLevelOrBuilder>(
                (org.apache.spark.connect.proto.AnalyzePlanRequest.GetStorageLevel) analyze_,
                getParentForChildren(),
                isClean());
        analyze_ = null;
      }
      analyzeCase_ = 16;
      onChanged();
      return getStorageLevelBuilder_;
    }
    @Override
    public final Builder setUnknownFields(
        final com.google.protobuf.UnknownFieldSet unknownFields) {
      return super.setUnknownFields(unknownFields);
    }

    @Override
    public final Builder mergeUnknownFields(
        final com.google.protobuf.UnknownFieldSet unknownFields) {
      return super.mergeUnknownFields(unknownFields);
    }


    // @@protoc_insertion_point(builder_scope:spark.connect.AnalyzePlanRequest)
  }

  // @@protoc_insertion_point(class_scope:spark.connect.AnalyzePlanRequest)
  private static final org.apache.spark.connect.proto.AnalyzePlanRequest DEFAULT_INSTANCE;
  static {
    DEFAULT_INSTANCE = new org.apache.spark.connect.proto.AnalyzePlanRequest();
  }

  public static org.apache.spark.connect.proto.AnalyzePlanRequest getDefaultInstance() {
    return DEFAULT_INSTANCE;
  }

  private static final com.google.protobuf.Parser<AnalyzePlanRequest>
      PARSER = new com.google.protobuf.AbstractParser<AnalyzePlanRequest>() {
    @Override
    public AnalyzePlanRequest parsePartialFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      Builder builder = newBuilder();
      try {
        builder.mergeFrom(input, extensionRegistry);
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(builder.buildPartial());
      } catch (com.google.protobuf.UninitializedMessageException e) {
        throw e.asInvalidProtocolBufferException().setUnfinishedMessage(builder.buildPartial());
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(e)
            .setUnfinishedMessage(builder.buildPartial());
      }
      return builder.buildPartial();
    }
  };

  public static com.google.protobuf.Parser<AnalyzePlanRequest> parser() {
    return PARSER;
  }

  @Override
  public com.google.protobuf.Parser<AnalyzePlanRequest> getParserForType() {
    return PARSER;
  }

  @Override
  public org.apache.spark.connect.proto.AnalyzePlanRequest getDefaultInstanceForType() {
    return DEFAULT_INSTANCE;
  }

}


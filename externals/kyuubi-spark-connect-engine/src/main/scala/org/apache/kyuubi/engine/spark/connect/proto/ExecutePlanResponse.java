// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: spark/connect/base.proto

package org.apache.kyuubi.engine.spark.connect.proto;

/**
 * <pre>
 * The response of a query, can be one or more for each request. Responses belonging to the
 * same input query, carry the same `session_id`.
 * </pre>
 *
 * Protobuf type {@code spark.connect.ExecutePlanResponse}
 */
public final class ExecutePlanResponse extends
    com.google.protobuf.GeneratedMessageV3 implements
    // @@protoc_insertion_point(message_implements:spark.connect.ExecutePlanResponse)
    ExecutePlanResponseOrBuilder {
private static final long serialVersionUID = 0L;
  // Use ExecutePlanResponse.newBuilder() to construct.
  private ExecutePlanResponse(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
    super(builder);
  }
  private ExecutePlanResponse() {
    sessionId_ = "";
    operationId_ = "";
    responseId_ = "";
    observedMetrics_ = java.util.Collections.emptyList();
  }

  @Override
  @SuppressWarnings({"unused"})
  protected Object newInstance(
      UnusedPrivateParameter unused) {
    return new ExecutePlanResponse();
  }

  public static final com.google.protobuf.Descriptors.Descriptor
      getDescriptor() {
    return org.apache.spark.connect.proto.Base.internal_static_spark_connect_ExecutePlanResponse_descriptor;
  }

  @Override
  protected FieldAccessorTable
      internalGetFieldAccessorTable() {
    return org.apache.spark.connect.proto.Base.internal_static_spark_connect_ExecutePlanResponse_fieldAccessorTable
        .ensureFieldAccessorsInitialized(
            org.apache.spark.connect.proto.ExecutePlanResponse.class, org.apache.spark.connect.proto.ExecutePlanResponse.Builder.class);
  }

  public interface SqlCommandResultOrBuilder extends
      // @@protoc_insertion_point(interface_extends:spark.connect.ExecutePlanResponse.SqlCommandResult)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>.spark.connect.Relation relation = 1;</code>
     * @return Whether the relation field is set.
     */
    boolean hasRelation();
    /**
     * <code>.spark.connect.Relation relation = 1;</code>
     * @return The relation.
     */
    org.apache.spark.connect.proto.Relation getRelation();
    /**
     * <code>.spark.connect.Relation relation = 1;</code>
     */
    org.apache.spark.connect.proto.RelationOrBuilder getRelationOrBuilder();
  }
  /**
   * <pre>
   * A SQL command returns an opaque Relation that can be directly used as input for the next
   * call.
   * </pre>
   *
   * Protobuf type {@code spark.connect.ExecutePlanResponse.SqlCommandResult}
   */
  public static final class SqlCommandResult extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:spark.connect.ExecutePlanResponse.SqlCommandResult)
      SqlCommandResultOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use SqlCommandResult.newBuilder() to construct.
    private SqlCommandResult(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private SqlCommandResult() {
    }

    @Override
    @SuppressWarnings({"unused"})
    protected Object newInstance(
        UnusedPrivateParameter unused) {
      return new SqlCommandResult();
    }

    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.connect.proto.Base.internal_static_spark_connect_ExecutePlanResponse_SqlCommandResult_descriptor;
    }

    @Override
    protected FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.connect.proto.Base.internal_static_spark_connect_ExecutePlanResponse_SqlCommandResult_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.connect.proto.ExecutePlanResponse.SqlCommandResult.class, org.apache.spark.connect.proto.ExecutePlanResponse.SqlCommandResult.Builder.class);
    }

    public static final int RELATION_FIELD_NUMBER = 1;
    private org.apache.spark.connect.proto.Relation relation_;
    /**
     * <code>.spark.connect.Relation relation = 1;</code>
     * @return Whether the relation field is set.
     */
    @Override
    public boolean hasRelation() {
      return relation_ != null;
    }
    /**
     * <code>.spark.connect.Relation relation = 1;</code>
     * @return The relation.
     */
    @Override
    public org.apache.spark.connect.proto.Relation getRelation() {
      return relation_ == null ? org.apache.spark.connect.proto.Relation.getDefaultInstance() : relation_;
    }
    /**
     * <code>.spark.connect.Relation relation = 1;</code>
     */
    @Override
    public org.apache.spark.connect.proto.RelationOrBuilder getRelationOrBuilder() {
      return relation_ == null ? org.apache.spark.connect.proto.Relation.getDefaultInstance() : relation_;
    }

    private byte memoizedIsInitialized = -1;
    @Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (relation_ != null) {
        output.writeMessage(1, getRelation());
      }
      getUnknownFields().writeTo(output);
    }

    @Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (relation_ != null) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, getRelation());
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @Override
    public boolean equals(final Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.connect.proto.ExecutePlanResponse.SqlCommandResult)) {
        return super.equals(obj);
      }
      org.apache.spark.connect.proto.ExecutePlanResponse.SqlCommandResult other = (org.apache.spark.connect.proto.ExecutePlanResponse.SqlCommandResult) obj;

      if (hasRelation() != other.hasRelation()) return false;
      if (hasRelation()) {
        if (!getRelation()
            .equals(other.getRelation())) return false;
      }
      if (!getUnknownFields().equals(other.getUnknownFields())) return false;
      return true;
    }

    @Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (hasRelation()) {
        hash = (37 * hash) + RELATION_FIELD_NUMBER;
        hash = (53 * hash) + getRelation().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.connect.proto.ExecutePlanResponse.SqlCommandResult parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.connect.proto.ExecutePlanResponse.SqlCommandResult parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.connect.proto.ExecutePlanResponse.SqlCommandResult parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.connect.proto.ExecutePlanResponse.SqlCommandResult parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.connect.proto.ExecutePlanResponse.SqlCommandResult parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.connect.proto.ExecutePlanResponse.SqlCommandResult parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.connect.proto.ExecutePlanResponse.SqlCommandResult parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.connect.proto.ExecutePlanResponse.SqlCommandResult parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    public static org.apache.spark.connect.proto.ExecutePlanResponse.SqlCommandResult parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }

    public static org.apache.spark.connect.proto.ExecutePlanResponse.SqlCommandResult parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.connect.proto.ExecutePlanResponse.SqlCommandResult parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.connect.proto.ExecutePlanResponse.SqlCommandResult parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.connect.proto.ExecutePlanResponse.SqlCommandResult prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @Override
    protected Builder newBuilderForType(
        BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     * A SQL command returns an opaque Relation that can be directly used as input for the next
     * call.
     * </pre>
     *
     * Protobuf type {@code spark.connect.ExecutePlanResponse.SqlCommandResult}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:spark.connect.ExecutePlanResponse.SqlCommandResult)
        org.apache.spark.connect.proto.ExecutePlanResponse.SqlCommandResultOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.connect.proto.Base.internal_static_spark_connect_ExecutePlanResponse_SqlCommandResult_descriptor;
      }

      @Override
      protected FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.connect.proto.Base.internal_static_spark_connect_ExecutePlanResponse_SqlCommandResult_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.connect.proto.ExecutePlanResponse.SqlCommandResult.class, org.apache.spark.connect.proto.ExecutePlanResponse.SqlCommandResult.Builder.class);
      }

      // Construct using org.apache.spark.connect.proto.ExecutePlanResponse.SqlCommandResult.newBuilder()
      private Builder() {

      }

      private Builder(
          BuilderParent parent) {
        super(parent);

      }
      @Override
      public Builder clear() {
        super.clear();
        bitField0_ = 0;
        relation_ = null;
        if (relationBuilder_ != null) {
          relationBuilder_.dispose();
          relationBuilder_ = null;
        }
        return this;
      }

      @Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.connect.proto.Base.internal_static_spark_connect_ExecutePlanResponse_SqlCommandResult_descriptor;
      }

      @Override
      public org.apache.spark.connect.proto.ExecutePlanResponse.SqlCommandResult getDefaultInstanceForType() {
        return org.apache.spark.connect.proto.ExecutePlanResponse.SqlCommandResult.getDefaultInstance();
      }

      @Override
      public org.apache.spark.connect.proto.ExecutePlanResponse.SqlCommandResult build() {
        org.apache.spark.connect.proto.ExecutePlanResponse.SqlCommandResult result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @Override
      public org.apache.spark.connect.proto.ExecutePlanResponse.SqlCommandResult buildPartial() {
        org.apache.spark.connect.proto.ExecutePlanResponse.SqlCommandResult result = new org.apache.spark.connect.proto.ExecutePlanResponse.SqlCommandResult(this);
        if (bitField0_ != 0) { buildPartial0(result); }
        onBuilt();
        return result;
      }

      private void buildPartial0(org.apache.spark.connect.proto.ExecutePlanResponse.SqlCommandResult result) {
        int from_bitField0_ = bitField0_;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          result.relation_ = relationBuilder_ == null
              ? relation_
              : relationBuilder_.build();
        }
      }

      @Override
      public Builder clone() {
        return super.clone();
      }
      @Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          Object value) {
        return super.setField(field, value);
      }
      @Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          Object value) {
        return super.addRepeatedField(field, value);
      }
      @Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.connect.proto.ExecutePlanResponse.SqlCommandResult) {
          return mergeFrom((org.apache.spark.connect.proto.ExecutePlanResponse.SqlCommandResult)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.connect.proto.ExecutePlanResponse.SqlCommandResult other) {
        if (other == org.apache.spark.connect.proto.ExecutePlanResponse.SqlCommandResult.getDefaultInstance()) return this;
        if (other.hasRelation()) {
          mergeRelation(other.getRelation());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        onChanged();
        return this;
      }

      @Override
      public final boolean isInitialized() {
        return true;
      }

      @Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        if (extensionRegistry == null) {
          throw new NullPointerException();
        }
        try {
          boolean done = false;
          while (!done) {
            int tag = input.readTag();
            switch (tag) {
              case 0:
                done = true;
                break;
              case 10: {
                input.readMessage(
                    getRelationFieldBuilder().getBuilder(),
                    extensionRegistry);
                bitField0_ |= 0x00000001;
                break;
              } // case 10
              default: {
                if (!super.parseUnknownField(input, extensionRegistry, tag)) {
                  done = true; // was an endgroup tag
                }
                break;
              } // default:
            } // switch (tag)
          } // while (!done)
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.unwrapIOException();
        } finally {
          onChanged();
        } // finally
        return this;
      }
      private int bitField0_;

      private org.apache.spark.connect.proto.Relation relation_;
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.connect.proto.Relation, org.apache.spark.connect.proto.Relation.Builder, org.apache.spark.connect.proto.RelationOrBuilder> relationBuilder_;
      /**
       * <code>.spark.connect.Relation relation = 1;</code>
       * @return Whether the relation field is set.
       */
      public boolean hasRelation() {
        return ((bitField0_ & 0x00000001) != 0);
      }
      /**
       * <code>.spark.connect.Relation relation = 1;</code>
       * @return The relation.
       */
      public org.apache.spark.connect.proto.Relation getRelation() {
        if (relationBuilder_ == null) {
          return relation_ == null ? org.apache.spark.connect.proto.Relation.getDefaultInstance() : relation_;
        } else {
          return relationBuilder_.getMessage();
        }
      }
      /**
       * <code>.spark.connect.Relation relation = 1;</code>
       */
      public Builder setRelation(org.apache.spark.connect.proto.Relation value) {
        if (relationBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          relation_ = value;
        } else {
          relationBuilder_.setMessage(value);
        }
        bitField0_ |= 0x00000001;
        onChanged();
        return this;
      }
      /**
       * <code>.spark.connect.Relation relation = 1;</code>
       */
      public Builder setRelation(
          org.apache.spark.connect.proto.Relation.Builder builderForValue) {
        if (relationBuilder_ == null) {
          relation_ = builderForValue.build();
        } else {
          relationBuilder_.setMessage(builderForValue.build());
        }
        bitField0_ |= 0x00000001;
        onChanged();
        return this;
      }
      /**
       * <code>.spark.connect.Relation relation = 1;</code>
       */
      public Builder mergeRelation(org.apache.spark.connect.proto.Relation value) {
        if (relationBuilder_ == null) {
          if (((bitField0_ & 0x00000001) != 0) &&
            relation_ != null &&
            relation_ != org.apache.spark.connect.proto.Relation.getDefaultInstance()) {
            getRelationBuilder().mergeFrom(value);
          } else {
            relation_ = value;
          }
        } else {
          relationBuilder_.mergeFrom(value);
        }
        bitField0_ |= 0x00000001;
        onChanged();
        return this;
      }
      /**
       * <code>.spark.connect.Relation relation = 1;</code>
       */
      public Builder clearRelation() {
        bitField0_ = (bitField0_ & ~0x00000001);
        relation_ = null;
        if (relationBuilder_ != null) {
          relationBuilder_.dispose();
          relationBuilder_ = null;
        }
        onChanged();
        return this;
      }
      /**
       * <code>.spark.connect.Relation relation = 1;</code>
       */
      public org.apache.spark.connect.proto.Relation.Builder getRelationBuilder() {
        bitField0_ |= 0x00000001;
        onChanged();
        return getRelationFieldBuilder().getBuilder();
      }
      /**
       * <code>.spark.connect.Relation relation = 1;</code>
       */
      public org.apache.spark.connect.proto.RelationOrBuilder getRelationOrBuilder() {
        if (relationBuilder_ != null) {
          return relationBuilder_.getMessageOrBuilder();
        } else {
          return relation_ == null ?
              org.apache.spark.connect.proto.Relation.getDefaultInstance() : relation_;
        }
      }
      /**
       * <code>.spark.connect.Relation relation = 1;</code>
       */
      private com.google.protobuf.SingleFieldBuilderV3<
          org.apache.spark.connect.proto.Relation, org.apache.spark.connect.proto.Relation.Builder, org.apache.spark.connect.proto.RelationOrBuilder> 
          getRelationFieldBuilder() {
        if (relationBuilder_ == null) {
          relationBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
              org.apache.spark.connect.proto.Relation, org.apache.spark.connect.proto.Relation.Builder, org.apache.spark.connect.proto.RelationOrBuilder>(
                  getRelation(),
                  getParentForChildren(),
                  isClean());
          relation_ = null;
        }
        return relationBuilder_;
      }
      @Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:spark.connect.ExecutePlanResponse.SqlCommandResult)
    }

    // @@protoc_insertion_point(class_scope:spark.connect.ExecutePlanResponse.SqlCommandResult)
    private static final org.apache.spark.connect.proto.ExecutePlanResponse.SqlCommandResult DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.connect.proto.ExecutePlanResponse.SqlCommandResult();
    }

    public static org.apache.spark.connect.proto.ExecutePlanResponse.SqlCommandResult getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<SqlCommandResult>
        PARSER = new com.google.protobuf.AbstractParser<SqlCommandResult>() {
      @Override
      public SqlCommandResult parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        Builder builder = newBuilder();
        try {
          builder.mergeFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.setUnfinishedMessage(builder.buildPartial());
        } catch (com.google.protobuf.UninitializedMessageException e) {
          throw e.asInvalidProtocolBufferException().setUnfinishedMessage(builder.buildPartial());
        } catch (java.io.IOException e) {
          throw new com.google.protobuf.InvalidProtocolBufferException(e)
              .setUnfinishedMessage(builder.buildPartial());
        }
        return builder.buildPartial();
      }
    };

    public static com.google.protobuf.Parser<SqlCommandResult> parser() {
      return PARSER;
    }

    @Override
    public com.google.protobuf.Parser<SqlCommandResult> getParserForType() {
      return PARSER;
    }

    @Override
    public org.apache.spark.connect.proto.ExecutePlanResponse.SqlCommandResult getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface ArrowBatchOrBuilder extends
      // @@protoc_insertion_point(interface_extends:spark.connect.ExecutePlanResponse.ArrowBatch)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <pre>
     * Count rows in `data`. Must match the number of rows inside `data`.
     * </pre>
     *
     * <code>int64 row_count = 1;</code>
     * @return The rowCount.
     */
    long getRowCount();

    /**
     * <pre>
     * Serialized Arrow data.
     * </pre>
     *
     * <code>bytes data = 2;</code>
     * @return The data.
     */
    com.google.protobuf.ByteString getData();

    /**
     * <pre>
     * If set, row offset of the start of this ArrowBatch in execution results.
     * </pre>
     *
     * <code>optional int64 start_offset = 3;</code>
     * @return Whether the startOffset field is set.
     */
    boolean hasStartOffset();
    /**
     * <pre>
     * If set, row offset of the start of this ArrowBatch in execution results.
     * </pre>
     *
     * <code>optional int64 start_offset = 3;</code>
     * @return The startOffset.
     */
    long getStartOffset();
  }
  /**
   * <pre>
   * Batch results of metrics.
   * </pre>
   *
   * Protobuf type {@code spark.connect.ExecutePlanResponse.ArrowBatch}
   */
  public static final class ArrowBatch extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:spark.connect.ExecutePlanResponse.ArrowBatch)
      ArrowBatchOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use ArrowBatch.newBuilder() to construct.
    private ArrowBatch(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private ArrowBatch() {
      data_ = com.google.protobuf.ByteString.EMPTY;
    }

    @Override
    @SuppressWarnings({"unused"})
    protected Object newInstance(
        UnusedPrivateParameter unused) {
      return new ArrowBatch();
    }

    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.connect.proto.Base.internal_static_spark_connect_ExecutePlanResponse_ArrowBatch_descriptor;
    }

    @Override
    protected FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.connect.proto.Base.internal_static_spark_connect_ExecutePlanResponse_ArrowBatch_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.connect.proto.ExecutePlanResponse.ArrowBatch.class, org.apache.spark.connect.proto.ExecutePlanResponse.ArrowBatch.Builder.class);
    }

    private int bitField0_;
    public static final int ROW_COUNT_FIELD_NUMBER = 1;
    private long rowCount_ = 0L;
    /**
     * <pre>
     * Count rows in `data`. Must match the number of rows inside `data`.
     * </pre>
     *
     * <code>int64 row_count = 1;</code>
     * @return The rowCount.
     */
    @Override
    public long getRowCount() {
      return rowCount_;
    }

    public static final int DATA_FIELD_NUMBER = 2;
    private com.google.protobuf.ByteString data_ = com.google.protobuf.ByteString.EMPTY;
    /**
     * <pre>
     * Serialized Arrow data.
     * </pre>
     *
     * <code>bytes data = 2;</code>
     * @return The data.
     */
    @Override
    public com.google.protobuf.ByteString getData() {
      return data_;
    }

    public static final int START_OFFSET_FIELD_NUMBER = 3;
    private long startOffset_ = 0L;
    /**
     * <pre>
     * If set, row offset of the start of this ArrowBatch in execution results.
     * </pre>
     *
     * <code>optional int64 start_offset = 3;</code>
     * @return Whether the startOffset field is set.
     */
    @Override
    public boolean hasStartOffset() {
      return ((bitField0_ & 0x00000001) != 0);
    }
    /**
     * <pre>
     * If set, row offset of the start of this ArrowBatch in execution results.
     * </pre>
     *
     * <code>optional int64 start_offset = 3;</code>
     * @return The startOffset.
     */
    @Override
    public long getStartOffset() {
      return startOffset_;
    }

    private byte memoizedIsInitialized = -1;
    @Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (rowCount_ != 0L) {
        output.writeInt64(1, rowCount_);
      }
      if (!data_.isEmpty()) {
        output.writeBytes(2, data_);
      }
      if (((bitField0_ & 0x00000001) != 0)) {
        output.writeInt64(3, startOffset_);
      }
      getUnknownFields().writeTo(output);
    }

    @Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (rowCount_ != 0L) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(1, rowCount_);
      }
      if (!data_.isEmpty()) {
        size += com.google.protobuf.CodedOutputStream
          .computeBytesSize(2, data_);
      }
      if (((bitField0_ & 0x00000001) != 0)) {
        size += com.google.protobuf.CodedOutputStream
          .computeInt64Size(3, startOffset_);
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @Override
    public boolean equals(final Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.connect.proto.ExecutePlanResponse.ArrowBatch)) {
        return super.equals(obj);
      }
      org.apache.spark.connect.proto.ExecutePlanResponse.ArrowBatch other = (org.apache.spark.connect.proto.ExecutePlanResponse.ArrowBatch) obj;

      if (getRowCount()
          != other.getRowCount()) return false;
      if (!getData()
          .equals(other.getData())) return false;
      if (hasStartOffset() != other.hasStartOffset()) return false;
      if (hasStartOffset()) {
        if (getStartOffset()
            != other.getStartOffset()) return false;
      }
      if (!getUnknownFields().equals(other.getUnknownFields())) return false;
      return true;
    }

    @Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      hash = (37 * hash) + ROW_COUNT_FIELD_NUMBER;
      hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
          getRowCount());
      hash = (37 * hash) + DATA_FIELD_NUMBER;
      hash = (53 * hash) + getData().hashCode();
      if (hasStartOffset()) {
        hash = (37 * hash) + START_OFFSET_FIELD_NUMBER;
        hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
            getStartOffset());
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.connect.proto.ExecutePlanResponse.ArrowBatch parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.connect.proto.ExecutePlanResponse.ArrowBatch parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.connect.proto.ExecutePlanResponse.ArrowBatch parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.connect.proto.ExecutePlanResponse.ArrowBatch parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.connect.proto.ExecutePlanResponse.ArrowBatch parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.connect.proto.ExecutePlanResponse.ArrowBatch parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.connect.proto.ExecutePlanResponse.ArrowBatch parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.connect.proto.ExecutePlanResponse.ArrowBatch parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    public static org.apache.spark.connect.proto.ExecutePlanResponse.ArrowBatch parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }

    public static org.apache.spark.connect.proto.ExecutePlanResponse.ArrowBatch parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.connect.proto.ExecutePlanResponse.ArrowBatch parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.connect.proto.ExecutePlanResponse.ArrowBatch parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.connect.proto.ExecutePlanResponse.ArrowBatch prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @Override
    protected Builder newBuilderForType(
        BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     * Batch results of metrics.
     * </pre>
     *
     * Protobuf type {@code spark.connect.ExecutePlanResponse.ArrowBatch}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:spark.connect.ExecutePlanResponse.ArrowBatch)
        org.apache.spark.connect.proto.ExecutePlanResponse.ArrowBatchOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.connect.proto.Base.internal_static_spark_connect_ExecutePlanResponse_ArrowBatch_descriptor;
      }

      @Override
      protected FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.connect.proto.Base.internal_static_spark_connect_ExecutePlanResponse_ArrowBatch_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.connect.proto.ExecutePlanResponse.ArrowBatch.class, org.apache.spark.connect.proto.ExecutePlanResponse.ArrowBatch.Builder.class);
      }

      // Construct using org.apache.spark.connect.proto.ExecutePlanResponse.ArrowBatch.newBuilder()
      private Builder() {

      }

      private Builder(
          BuilderParent parent) {
        super(parent);

      }
      @Override
      public Builder clear() {
        super.clear();
        bitField0_ = 0;
        rowCount_ = 0L;
        data_ = com.google.protobuf.ByteString.EMPTY;
        startOffset_ = 0L;
        return this;
      }

      @Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.connect.proto.Base.internal_static_spark_connect_ExecutePlanResponse_ArrowBatch_descriptor;
      }

      @Override
      public org.apache.spark.connect.proto.ExecutePlanResponse.ArrowBatch getDefaultInstanceForType() {
        return org.apache.spark.connect.proto.ExecutePlanResponse.ArrowBatch.getDefaultInstance();
      }

      @Override
      public org.apache.spark.connect.proto.ExecutePlanResponse.ArrowBatch build() {
        org.apache.spark.connect.proto.ExecutePlanResponse.ArrowBatch result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @Override
      public org.apache.spark.connect.proto.ExecutePlanResponse.ArrowBatch buildPartial() {
        org.apache.spark.connect.proto.ExecutePlanResponse.ArrowBatch result = new org.apache.spark.connect.proto.ExecutePlanResponse.ArrowBatch(this);
        if (bitField0_ != 0) { buildPartial0(result); }
        onBuilt();
        return result;
      }

      private void buildPartial0(org.apache.spark.connect.proto.ExecutePlanResponse.ArrowBatch result) {
        int from_bitField0_ = bitField0_;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          result.rowCount_ = rowCount_;
        }
        if (((from_bitField0_ & 0x00000002) != 0)) {
          result.data_ = data_;
        }
        int to_bitField0_ = 0;
        if (((from_bitField0_ & 0x00000004) != 0)) {
          result.startOffset_ = startOffset_;
          to_bitField0_ |= 0x00000001;
        }
        result.bitField0_ |= to_bitField0_;
      }

      @Override
      public Builder clone() {
        return super.clone();
      }
      @Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          Object value) {
        return super.setField(field, value);
      }
      @Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          Object value) {
        return super.addRepeatedField(field, value);
      }
      @Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.connect.proto.ExecutePlanResponse.ArrowBatch) {
          return mergeFrom((org.apache.spark.connect.proto.ExecutePlanResponse.ArrowBatch)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.connect.proto.ExecutePlanResponse.ArrowBatch other) {
        if (other == org.apache.spark.connect.proto.ExecutePlanResponse.ArrowBatch.getDefaultInstance()) return this;
        if (other.getRowCount() != 0L) {
          setRowCount(other.getRowCount());
        }
        if (other.getData() != com.google.protobuf.ByteString.EMPTY) {
          setData(other.getData());
        }
        if (other.hasStartOffset()) {
          setStartOffset(other.getStartOffset());
        }
        this.mergeUnknownFields(other.getUnknownFields());
        onChanged();
        return this;
      }

      @Override
      public final boolean isInitialized() {
        return true;
      }

      @Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        if (extensionRegistry == null) {
          throw new NullPointerException();
        }
        try {
          boolean done = false;
          while (!done) {
            int tag = input.readTag();
            switch (tag) {
              case 0:
                done = true;
                break;
              case 8: {
                rowCount_ = input.readInt64();
                bitField0_ |= 0x00000001;
                break;
              } // case 8
              case 18: {
                data_ = input.readBytes();
                bitField0_ |= 0x00000002;
                break;
              } // case 18
              case 24: {
                startOffset_ = input.readInt64();
                bitField0_ |= 0x00000004;
                break;
              } // case 24
              default: {
                if (!super.parseUnknownField(input, extensionRegistry, tag)) {
                  done = true; // was an endgroup tag
                }
                break;
              } // default:
            } // switch (tag)
          } // while (!done)
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.unwrapIOException();
        } finally {
          onChanged();
        } // finally
        return this;
      }
      private int bitField0_;

      private long rowCount_ ;
      /**
       * <pre>
       * Count rows in `data`. Must match the number of rows inside `data`.
       * </pre>
       *
       * <code>int64 row_count = 1;</code>
       * @return The rowCount.
       */
      @Override
      public long getRowCount() {
        return rowCount_;
      }
      /**
       * <pre>
       * Count rows in `data`. Must match the number of rows inside `data`.
       * </pre>
       *
       * <code>int64 row_count = 1;</code>
       * @param value The rowCount to set.
       * @return This builder for chaining.
       */
      public Builder setRowCount(long value) {

        rowCount_ = value;
        bitField0_ |= 0x00000001;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * Count rows in `data`. Must match the number of rows inside `data`.
       * </pre>
       *
       * <code>int64 row_count = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearRowCount() {
        bitField0_ = (bitField0_ & ~0x00000001);
        rowCount_ = 0L;
        onChanged();
        return this;
      }

      private com.google.protobuf.ByteString data_ = com.google.protobuf.ByteString.EMPTY;
      /**
       * <pre>
       * Serialized Arrow data.
       * </pre>
       *
       * <code>bytes data = 2;</code>
       * @return The data.
       */
      @Override
      public com.google.protobuf.ByteString getData() {
        return data_;
      }
      /**
       * <pre>
       * Serialized Arrow data.
       * </pre>
       *
       * <code>bytes data = 2;</code>
       * @param value The data to set.
       * @return This builder for chaining.
       */
      public Builder setData(com.google.protobuf.ByteString value) {
        if (value == null) { throw new NullPointerException(); }
        data_ = value;
        bitField0_ |= 0x00000002;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * Serialized Arrow data.
       * </pre>
       *
       * <code>bytes data = 2;</code>
       * @return This builder for chaining.
       */
      public Builder clearData() {
        bitField0_ = (bitField0_ & ~0x00000002);
        data_ = getDefaultInstance().getData();
        onChanged();
        return this;
      }

      private long startOffset_ ;
      /**
       * <pre>
       * If set, row offset of the start of this ArrowBatch in execution results.
       * </pre>
       *
       * <code>optional int64 start_offset = 3;</code>
       * @return Whether the startOffset field is set.
       */
      @Override
      public boolean hasStartOffset() {
        return ((bitField0_ & 0x00000004) != 0);
      }
      /**
       * <pre>
       * If set, row offset of the start of this ArrowBatch in execution results.
       * </pre>
       *
       * <code>optional int64 start_offset = 3;</code>
       * @return The startOffset.
       */
      @Override
      public long getStartOffset() {
        return startOffset_;
      }
      /**
       * <pre>
       * If set, row offset of the start of this ArrowBatch in execution results.
       * </pre>
       *
       * <code>optional int64 start_offset = 3;</code>
       * @param value The startOffset to set.
       * @return This builder for chaining.
       */
      public Builder setStartOffset(long value) {

        startOffset_ = value;
        bitField0_ |= 0x00000004;
        onChanged();
        return this;
      }
      /**
       * <pre>
       * If set, row offset of the start of this ArrowBatch in execution results.
       * </pre>
       *
       * <code>optional int64 start_offset = 3;</code>
       * @return This builder for chaining.
       */
      public Builder clearStartOffset() {
        bitField0_ = (bitField0_ & ~0x00000004);
        startOffset_ = 0L;
        onChanged();
        return this;
      }
      @Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:spark.connect.ExecutePlanResponse.ArrowBatch)
    }

    // @@protoc_insertion_point(class_scope:spark.connect.ExecutePlanResponse.ArrowBatch)
    private static final org.apache.spark.connect.proto.ExecutePlanResponse.ArrowBatch DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.connect.proto.ExecutePlanResponse.ArrowBatch();
    }

    public static org.apache.spark.connect.proto.ExecutePlanResponse.ArrowBatch getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<ArrowBatch>
        PARSER = new com.google.protobuf.AbstractParser<ArrowBatch>() {
      @Override
      public ArrowBatch parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        Builder builder = newBuilder();
        try {
          builder.mergeFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.setUnfinishedMessage(builder.buildPartial());
        } catch (com.google.protobuf.UninitializedMessageException e) {
          throw e.asInvalidProtocolBufferException().setUnfinishedMessage(builder.buildPartial());
        } catch (java.io.IOException e) {
          throw new com.google.protobuf.InvalidProtocolBufferException(e)
              .setUnfinishedMessage(builder.buildPartial());
        }
        return builder.buildPartial();
      }
    };

    public static com.google.protobuf.Parser<ArrowBatch> parser() {
      return PARSER;
    }

    @Override
    public com.google.protobuf.Parser<ArrowBatch> getParserForType() {
      return PARSER;
    }

    @Override
    public org.apache.spark.connect.proto.ExecutePlanResponse.ArrowBatch getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface MetricsOrBuilder extends
      // @@protoc_insertion_point(interface_extends:spark.connect.ExecutePlanResponse.Metrics)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>repeated .spark.connect.ExecutePlanResponse.Metrics.MetricObject metrics = 1;</code>
     */
    java.util.List<org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricObject> 
        getMetricsList();
    /**
     * <code>repeated .spark.connect.ExecutePlanResponse.Metrics.MetricObject metrics = 1;</code>
     */
    org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricObject getMetrics(int index);
    /**
     * <code>repeated .spark.connect.ExecutePlanResponse.Metrics.MetricObject metrics = 1;</code>
     */
    int getMetricsCount();
    /**
     * <code>repeated .spark.connect.ExecutePlanResponse.Metrics.MetricObject metrics = 1;</code>
     */
    java.util.List<? extends org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricObjectOrBuilder> 
        getMetricsOrBuilderList();
    /**
     * <code>repeated .spark.connect.ExecutePlanResponse.Metrics.MetricObject metrics = 1;</code>
     */
    org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricObjectOrBuilder getMetricsOrBuilder(
        int index);
  }
  /**
   * Protobuf type {@code spark.connect.ExecutePlanResponse.Metrics}
   */
  public static final class Metrics extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:spark.connect.ExecutePlanResponse.Metrics)
      MetricsOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use Metrics.newBuilder() to construct.
    private Metrics(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private Metrics() {
      metrics_ = java.util.Collections.emptyList();
    }

    @Override
    @SuppressWarnings({"unused"})
    protected Object newInstance(
        UnusedPrivateParameter unused) {
      return new Metrics();
    }

    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.connect.proto.Base.internal_static_spark_connect_ExecutePlanResponse_Metrics_descriptor;
    }

    @Override
    protected FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.connect.proto.Base.internal_static_spark_connect_ExecutePlanResponse_Metrics_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.class, org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.Builder.class);
    }

    public interface MetricObjectOrBuilder extends
        // @@protoc_insertion_point(interface_extends:spark.connect.ExecutePlanResponse.Metrics.MetricObject)
        com.google.protobuf.MessageOrBuilder {

      /**
       * <code>string name = 1;</code>
       * @return The name.
       */
      String getName();
      /**
       * <code>string name = 1;</code>
       * @return The bytes for name.
       */
      com.google.protobuf.ByteString
          getNameBytes();

      /**
       * <code>int64 plan_id = 2;</code>
       * @return The planId.
       */
      long getPlanId();

      /**
       * <code>int64 parent = 3;</code>
       * @return The parent.
       */
      long getParent();

      /**
       * <code>map&lt;string, .spark.connect.ExecutePlanResponse.Metrics.MetricValue&gt; execution_metrics = 4;</code>
       */
      int getExecutionMetricsCount();
      /**
       * <code>map&lt;string, .spark.connect.ExecutePlanResponse.Metrics.MetricValue&gt; execution_metrics = 4;</code>
       */
      boolean containsExecutionMetrics(
          String key);
      /**
       * Use {@link #getExecutionMetricsMap()} instead.
       */
      @Deprecated
      java.util.Map<String, org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricValue>
      getExecutionMetrics();
      /**
       * <code>map&lt;string, .spark.connect.ExecutePlanResponse.Metrics.MetricValue&gt; execution_metrics = 4;</code>
       */
      java.util.Map<String, org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricValue>
      getExecutionMetricsMap();
      /**
       * <code>map&lt;string, .spark.connect.ExecutePlanResponse.Metrics.MetricValue&gt; execution_metrics = 4;</code>
       */
      /* nullable */
org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricValue getExecutionMetricsOrDefault(
          String key,
          /* nullable */
org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricValue defaultValue);
      /**
       * <code>map&lt;string, .spark.connect.ExecutePlanResponse.Metrics.MetricValue&gt; execution_metrics = 4;</code>
       */
      org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricValue getExecutionMetricsOrThrow(
          String key);
    }
    /**
     * Protobuf type {@code spark.connect.ExecutePlanResponse.Metrics.MetricObject}
     */
    public static final class MetricObject extends
        com.google.protobuf.GeneratedMessageV3 implements
        // @@protoc_insertion_point(message_implements:spark.connect.ExecutePlanResponse.Metrics.MetricObject)
        MetricObjectOrBuilder {
    private static final long serialVersionUID = 0L;
      // Use MetricObject.newBuilder() to construct.
      private MetricObject(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
        super(builder);
      }
      private MetricObject() {
        name_ = "";
      }

      @Override
      @SuppressWarnings({"unused"})
      protected Object newInstance(
          UnusedPrivateParameter unused) {
        return new MetricObject();
      }

      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.connect.proto.Base.internal_static_spark_connect_ExecutePlanResponse_Metrics_MetricObject_descriptor;
      }

      @SuppressWarnings({"rawtypes"})
      @Override
      protected com.google.protobuf.MapField internalGetMapField(
          int number) {
        switch (number) {
          case 4:
            return internalGetExecutionMetrics();
          default:
            throw new RuntimeException(
                "Invalid map field number: " + number);
        }
      }
      @Override
      protected FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.connect.proto.Base.internal_static_spark_connect_ExecutePlanResponse_Metrics_MetricObject_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricObject.class, org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricObject.Builder.class);
      }

      public static final int NAME_FIELD_NUMBER = 1;
      @SuppressWarnings("serial")
      private volatile Object name_ = "";
      /**
       * <code>string name = 1;</code>
       * @return The name.
       */
      @Override
      public String getName() {
        Object ref = name_;
        if (ref instanceof String) {
          return (String) ref;
        } else {
          com.google.protobuf.ByteString bs = 
              (com.google.protobuf.ByteString) ref;
          String s = bs.toStringUtf8();
          name_ = s;
          return s;
        }
      }
      /**
       * <code>string name = 1;</code>
       * @return The bytes for name.
       */
      @Override
      public com.google.protobuf.ByteString
          getNameBytes() {
        Object ref = name_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (String) ref);
          name_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }

      public static final int PLAN_ID_FIELD_NUMBER = 2;
      private long planId_ = 0L;
      /**
       * <code>int64 plan_id = 2;</code>
       * @return The planId.
       */
      @Override
      public long getPlanId() {
        return planId_;
      }

      public static final int PARENT_FIELD_NUMBER = 3;
      private long parent_ = 0L;
      /**
       * <code>int64 parent = 3;</code>
       * @return The parent.
       */
      @Override
      public long getParent() {
        return parent_;
      }

      public static final int EXECUTION_METRICS_FIELD_NUMBER = 4;
      private static final class ExecutionMetricsDefaultEntryHolder {
        static final com.google.protobuf.MapEntry<
            String, org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricValue> defaultEntry =
                com.google.protobuf.MapEntry
                .<String, org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricValue>newDefaultInstance(
                    org.apache.spark.connect.proto.Base.internal_static_spark_connect_ExecutePlanResponse_Metrics_MetricObject_ExecutionMetricsEntry_descriptor, 
                    com.google.protobuf.WireFormat.FieldType.STRING,
                    "",
                    com.google.protobuf.WireFormat.FieldType.MESSAGE,
                    org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricValue.getDefaultInstance());
      }
      @SuppressWarnings("serial")
      private com.google.protobuf.MapField<
          String, org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricValue> executionMetrics_;
      private com.google.protobuf.MapField<String, org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricValue>
      internalGetExecutionMetrics() {
        if (executionMetrics_ == null) {
          return com.google.protobuf.MapField.emptyMapField(
              ExecutionMetricsDefaultEntryHolder.defaultEntry);
        }
        return executionMetrics_;
      }
      public int getExecutionMetricsCount() {
        return internalGetExecutionMetrics().getMap().size();
      }
      /**
       * <code>map&lt;string, .spark.connect.ExecutePlanResponse.Metrics.MetricValue&gt; execution_metrics = 4;</code>
       */
      @Override
      public boolean containsExecutionMetrics(
          String key) {
        if (key == null) { throw new NullPointerException("map key"); }
        return internalGetExecutionMetrics().getMap().containsKey(key);
      }
      /**
       * Use {@link #getExecutionMetricsMap()} instead.
       */
      @Override
      @Deprecated
      public java.util.Map<String, org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricValue> getExecutionMetrics() {
        return getExecutionMetricsMap();
      }
      /**
       * <code>map&lt;string, .spark.connect.ExecutePlanResponse.Metrics.MetricValue&gt; execution_metrics = 4;</code>
       */
      @Override
      public java.util.Map<String, org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricValue> getExecutionMetricsMap() {
        return internalGetExecutionMetrics().getMap();
      }
      /**
       * <code>map&lt;string, .spark.connect.ExecutePlanResponse.Metrics.MetricValue&gt; execution_metrics = 4;</code>
       */
      @Override
      public /* nullable */
org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricValue getExecutionMetricsOrDefault(
          String key,
          /* nullable */
org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricValue defaultValue) {
        if (key == null) { throw new NullPointerException("map key"); }
        java.util.Map<String, org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricValue> map =
            internalGetExecutionMetrics().getMap();
        return map.containsKey(key) ? map.get(key) : defaultValue;
      }
      /**
       * <code>map&lt;string, .spark.connect.ExecutePlanResponse.Metrics.MetricValue&gt; execution_metrics = 4;</code>
       */
      @Override
      public org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricValue getExecutionMetricsOrThrow(
          String key) {
        if (key == null) { throw new NullPointerException("map key"); }
        java.util.Map<String, org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricValue> map =
            internalGetExecutionMetrics().getMap();
        if (!map.containsKey(key)) {
          throw new IllegalArgumentException();
        }
        return map.get(key);
      }

      private byte memoizedIsInitialized = -1;
      @Override
      public final boolean isInitialized() {
        byte isInitialized = memoizedIsInitialized;
        if (isInitialized == 1) return true;
        if (isInitialized == 0) return false;

        memoizedIsInitialized = 1;
        return true;
      }

      @Override
      public void writeTo(com.google.protobuf.CodedOutputStream output)
                          throws java.io.IOException {
        if (!com.google.protobuf.GeneratedMessageV3.isStringEmpty(name_)) {
          com.google.protobuf.GeneratedMessageV3.writeString(output, 1, name_);
        }
        if (planId_ != 0L) {
          output.writeInt64(2, planId_);
        }
        if (parent_ != 0L) {
          output.writeInt64(3, parent_);
        }
        com.google.protobuf.GeneratedMessageV3
          .serializeStringMapTo(
            output,
            internalGetExecutionMetrics(),
            ExecutionMetricsDefaultEntryHolder.defaultEntry,
            4);
        getUnknownFields().writeTo(output);
      }

      @Override
      public int getSerializedSize() {
        int size = memoizedSize;
        if (size != -1) return size;

        size = 0;
        if (!com.google.protobuf.GeneratedMessageV3.isStringEmpty(name_)) {
          size += com.google.protobuf.GeneratedMessageV3.computeStringSize(1, name_);
        }
        if (planId_ != 0L) {
          size += com.google.protobuf.CodedOutputStream
            .computeInt64Size(2, planId_);
        }
        if (parent_ != 0L) {
          size += com.google.protobuf.CodedOutputStream
            .computeInt64Size(3, parent_);
        }
        for (java.util.Map.Entry<String, org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricValue> entry
             : internalGetExecutionMetrics().getMap().entrySet()) {
          com.google.protobuf.MapEntry<String, org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricValue>
          executionMetrics__ = ExecutionMetricsDefaultEntryHolder.defaultEntry.newBuilderForType()
              .setKey(entry.getKey())
              .setValue(entry.getValue())
              .build();
          size += com.google.protobuf.CodedOutputStream
              .computeMessageSize(4, executionMetrics__);
        }
        size += getUnknownFields().getSerializedSize();
        memoizedSize = size;
        return size;
      }

      @Override
      public boolean equals(final Object obj) {
        if (obj == this) {
         return true;
        }
        if (!(obj instanceof org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricObject)) {
          return super.equals(obj);
        }
        org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricObject other = (org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricObject) obj;

        if (!getName()
            .equals(other.getName())) return false;
        if (getPlanId()
            != other.getPlanId()) return false;
        if (getParent()
            != other.getParent()) return false;
        if (!internalGetExecutionMetrics().equals(
            other.internalGetExecutionMetrics())) return false;
        if (!getUnknownFields().equals(other.getUnknownFields())) return false;
        return true;
      }

      @Override
      public int hashCode() {
        if (memoizedHashCode != 0) {
          return memoizedHashCode;
        }
        int hash = 41;
        hash = (19 * hash) + getDescriptor().hashCode();
        hash = (37 * hash) + NAME_FIELD_NUMBER;
        hash = (53 * hash) + getName().hashCode();
        hash = (37 * hash) + PLAN_ID_FIELD_NUMBER;
        hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
            getPlanId());
        hash = (37 * hash) + PARENT_FIELD_NUMBER;
        hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
            getParent());
        if (!internalGetExecutionMetrics().getMap().isEmpty()) {
          hash = (37 * hash) + EXECUTION_METRICS_FIELD_NUMBER;
          hash = (53 * hash) + internalGetExecutionMetrics().hashCode();
        }
        hash = (29 * hash) + getUnknownFields().hashCode();
        memoizedHashCode = hash;
        return hash;
      }

      public static org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricObject parseFrom(
          java.nio.ByteBuffer data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricObject parseFrom(
          java.nio.ByteBuffer data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricObject parseFrom(
          com.google.protobuf.ByteString data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricObject parseFrom(
          com.google.protobuf.ByteString data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricObject parseFrom(byte[] data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricObject parseFrom(
          byte[] data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricObject parseFrom(java.io.InputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input);
      }
      public static org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricObject parseFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input, extensionRegistry);
      }

      public static org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricObject parseDelimitedFrom(java.io.InputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseDelimitedWithIOException(PARSER, input);
      }

      public static org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricObject parseDelimitedFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
      }
      public static org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricObject parseFrom(
          com.google.protobuf.CodedInputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input);
      }
      public static org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricObject parseFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input, extensionRegistry);
      }

      @Override
      public Builder newBuilderForType() { return newBuilder(); }
      public static Builder newBuilder() {
        return DEFAULT_INSTANCE.toBuilder();
      }
      public static Builder newBuilder(org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricObject prototype) {
        return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
      }
      @Override
      public Builder toBuilder() {
        return this == DEFAULT_INSTANCE
            ? new Builder() : new Builder().mergeFrom(this);
      }

      @Override
      protected Builder newBuilderForType(
          BuilderParent parent) {
        Builder builder = new Builder(parent);
        return builder;
      }
      /**
       * Protobuf type {@code spark.connect.ExecutePlanResponse.Metrics.MetricObject}
       */
      public static final class Builder extends
          com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
          // @@protoc_insertion_point(builder_implements:spark.connect.ExecutePlanResponse.Metrics.MetricObject)
          org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricObjectOrBuilder {
        public static final com.google.protobuf.Descriptors.Descriptor
            getDescriptor() {
          return org.apache.spark.connect.proto.Base.internal_static_spark_connect_ExecutePlanResponse_Metrics_MetricObject_descriptor;
        }

        @SuppressWarnings({"rawtypes"})
        protected com.google.protobuf.MapField internalGetMapField(
            int number) {
          switch (number) {
            case 4:
              return internalGetExecutionMetrics();
            default:
              throw new RuntimeException(
                  "Invalid map field number: " + number);
          }
        }
        @SuppressWarnings({"rawtypes"})
        protected com.google.protobuf.MapField internalGetMutableMapField(
            int number) {
          switch (number) {
            case 4:
              return internalGetMutableExecutionMetrics();
            default:
              throw new RuntimeException(
                  "Invalid map field number: " + number);
          }
        }
        @Override
        protected FieldAccessorTable
            internalGetFieldAccessorTable() {
          return org.apache.spark.connect.proto.Base.internal_static_spark_connect_ExecutePlanResponse_Metrics_MetricObject_fieldAccessorTable
              .ensureFieldAccessorsInitialized(
                  org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricObject.class, org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricObject.Builder.class);
        }

        // Construct using org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricObject.newBuilder()
        private Builder() {

        }

        private Builder(
            BuilderParent parent) {
          super(parent);

        }
        @Override
        public Builder clear() {
          super.clear();
          bitField0_ = 0;
          name_ = "";
          planId_ = 0L;
          parent_ = 0L;
          internalGetMutableExecutionMetrics().clear();
          return this;
        }

        @Override
        public com.google.protobuf.Descriptors.Descriptor
            getDescriptorForType() {
          return org.apache.spark.connect.proto.Base.internal_static_spark_connect_ExecutePlanResponse_Metrics_MetricObject_descriptor;
        }

        @Override
        public org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricObject getDefaultInstanceForType() {
          return org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricObject.getDefaultInstance();
        }

        @Override
        public org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricObject build() {
          org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricObject result = buildPartial();
          if (!result.isInitialized()) {
            throw newUninitializedMessageException(result);
          }
          return result;
        }

        @Override
        public org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricObject buildPartial() {
          org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricObject result = new org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricObject(this);
          if (bitField0_ != 0) { buildPartial0(result); }
          onBuilt();
          return result;
        }

        private void buildPartial0(org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricObject result) {
          int from_bitField0_ = bitField0_;
          if (((from_bitField0_ & 0x00000001) != 0)) {
            result.name_ = name_;
          }
          if (((from_bitField0_ & 0x00000002) != 0)) {
            result.planId_ = planId_;
          }
          if (((from_bitField0_ & 0x00000004) != 0)) {
            result.parent_ = parent_;
          }
          if (((from_bitField0_ & 0x00000008) != 0)) {
            result.executionMetrics_ = internalGetExecutionMetrics();
            result.executionMetrics_.makeImmutable();
          }
        }

        @Override
        public Builder clone() {
          return super.clone();
        }
        @Override
        public Builder setField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            Object value) {
          return super.setField(field, value);
        }
        @Override
        public Builder clearField(
            com.google.protobuf.Descriptors.FieldDescriptor field) {
          return super.clearField(field);
        }
        @Override
        public Builder clearOneof(
            com.google.protobuf.Descriptors.OneofDescriptor oneof) {
          return super.clearOneof(oneof);
        }
        @Override
        public Builder setRepeatedField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            int index, Object value) {
          return super.setRepeatedField(field, index, value);
        }
        @Override
        public Builder addRepeatedField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            Object value) {
          return super.addRepeatedField(field, value);
        }
        @Override
        public Builder mergeFrom(com.google.protobuf.Message other) {
          if (other instanceof org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricObject) {
            return mergeFrom((org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricObject)other);
          } else {
            super.mergeFrom(other);
            return this;
          }
        }

        public Builder mergeFrom(org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricObject other) {
          if (other == org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricObject.getDefaultInstance()) return this;
          if (!other.getName().isEmpty()) {
            name_ = other.name_;
            bitField0_ |= 0x00000001;
            onChanged();
          }
          if (other.getPlanId() != 0L) {
            setPlanId(other.getPlanId());
          }
          if (other.getParent() != 0L) {
            setParent(other.getParent());
          }
          internalGetMutableExecutionMetrics().mergeFrom(
              other.internalGetExecutionMetrics());
          bitField0_ |= 0x00000008;
          this.mergeUnknownFields(other.getUnknownFields());
          onChanged();
          return this;
        }

        @Override
        public final boolean isInitialized() {
          return true;
        }

        @Override
        public Builder mergeFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws java.io.IOException {
          if (extensionRegistry == null) {
            throw new NullPointerException();
          }
          try {
            boolean done = false;
            while (!done) {
              int tag = input.readTag();
              switch (tag) {
                case 0:
                  done = true;
                  break;
                case 10: {
                  name_ = input.readStringRequireUtf8();
                  bitField0_ |= 0x00000001;
                  break;
                } // case 10
                case 16: {
                  planId_ = input.readInt64();
                  bitField0_ |= 0x00000002;
                  break;
                } // case 16
                case 24: {
                  parent_ = input.readInt64();
                  bitField0_ |= 0x00000004;
                  break;
                } // case 24
                case 34: {
                  com.google.protobuf.MapEntry<String, org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricValue>
                  executionMetrics__ = input.readMessage(
                      ExecutionMetricsDefaultEntryHolder.defaultEntry.getParserForType(), extensionRegistry);
                  internalGetMutableExecutionMetrics().getMutableMap().put(
                      executionMetrics__.getKey(), executionMetrics__.getValue());
                  bitField0_ |= 0x00000008;
                  break;
                } // case 34
                default: {
                  if (!super.parseUnknownField(input, extensionRegistry, tag)) {
                    done = true; // was an endgroup tag
                  }
                  break;
                } // default:
              } // switch (tag)
            } // while (!done)
          } catch (com.google.protobuf.InvalidProtocolBufferException e) {
            throw e.unwrapIOException();
          } finally {
            onChanged();
          } // finally
          return this;
        }
        private int bitField0_;

        private Object name_ = "";
        /**
         * <code>string name = 1;</code>
         * @return The name.
         */
        public String getName() {
          Object ref = name_;
          if (!(ref instanceof String)) {
            com.google.protobuf.ByteString bs =
                (com.google.protobuf.ByteString) ref;
            String s = bs.toStringUtf8();
            name_ = s;
            return s;
          } else {
            return (String) ref;
          }
        }
        /**
         * <code>string name = 1;</code>
         * @return The bytes for name.
         */
        public com.google.protobuf.ByteString
            getNameBytes() {
          Object ref = name_;
          if (ref instanceof String) {
            com.google.protobuf.ByteString b = 
                com.google.protobuf.ByteString.copyFromUtf8(
                    (String) ref);
            name_ = b;
            return b;
          } else {
            return (com.google.protobuf.ByteString) ref;
          }
        }
        /**
         * <code>string name = 1;</code>
         * @param value The name to set.
         * @return This builder for chaining.
         */
        public Builder setName(
            String value) {
          if (value == null) { throw new NullPointerException(); }
          name_ = value;
          bitField0_ |= 0x00000001;
          onChanged();
          return this;
        }
        /**
         * <code>string name = 1;</code>
         * @return This builder for chaining.
         */
        public Builder clearName() {
          name_ = getDefaultInstance().getName();
          bitField0_ = (bitField0_ & ~0x00000001);
          onChanged();
          return this;
        }
        /**
         * <code>string name = 1;</code>
         * @param value The bytes for name to set.
         * @return This builder for chaining.
         */
        public Builder setNameBytes(
            com.google.protobuf.ByteString value) {
          if (value == null) { throw new NullPointerException(); }
          checkByteStringIsUtf8(value);
          name_ = value;
          bitField0_ |= 0x00000001;
          onChanged();
          return this;
        }

        private long planId_ ;
        /**
         * <code>int64 plan_id = 2;</code>
         * @return The planId.
         */
        @Override
        public long getPlanId() {
          return planId_;
        }
        /**
         * <code>int64 plan_id = 2;</code>
         * @param value The planId to set.
         * @return This builder for chaining.
         */
        public Builder setPlanId(long value) {

          planId_ = value;
          bitField0_ |= 0x00000002;
          onChanged();
          return this;
        }
        /**
         * <code>int64 plan_id = 2;</code>
         * @return This builder for chaining.
         */
        public Builder clearPlanId() {
          bitField0_ = (bitField0_ & ~0x00000002);
          planId_ = 0L;
          onChanged();
          return this;
        }

        private long parent_ ;
        /**
         * <code>int64 parent = 3;</code>
         * @return The parent.
         */
        @Override
        public long getParent() {
          return parent_;
        }
        /**
         * <code>int64 parent = 3;</code>
         * @param value The parent to set.
         * @return This builder for chaining.
         */
        public Builder setParent(long value) {

          parent_ = value;
          bitField0_ |= 0x00000004;
          onChanged();
          return this;
        }
        /**
         * <code>int64 parent = 3;</code>
         * @return This builder for chaining.
         */
        public Builder clearParent() {
          bitField0_ = (bitField0_ & ~0x00000004);
          parent_ = 0L;
          onChanged();
          return this;
        }

        private com.google.protobuf.MapField<
            String, org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricValue> executionMetrics_;
        private com.google.protobuf.MapField<String, org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricValue>
            internalGetExecutionMetrics() {
          if (executionMetrics_ == null) {
            return com.google.protobuf.MapField.emptyMapField(
                ExecutionMetricsDefaultEntryHolder.defaultEntry);
          }
          return executionMetrics_;
        }
        private com.google.protobuf.MapField<String, org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricValue>
            internalGetMutableExecutionMetrics() {
          if (executionMetrics_ == null) {
            executionMetrics_ = com.google.protobuf.MapField.newMapField(
                ExecutionMetricsDefaultEntryHolder.defaultEntry);
          }
          if (!executionMetrics_.isMutable()) {
            executionMetrics_ = executionMetrics_.copy();
          }
          bitField0_ |= 0x00000008;
          onChanged();
          return executionMetrics_;
        }
        public int getExecutionMetricsCount() {
          return internalGetExecutionMetrics().getMap().size();
        }
        /**
         * <code>map&lt;string, .spark.connect.ExecutePlanResponse.Metrics.MetricValue&gt; execution_metrics = 4;</code>
         */
        @Override
        public boolean containsExecutionMetrics(
            String key) {
          if (key == null) { throw new NullPointerException("map key"); }
          return internalGetExecutionMetrics().getMap().containsKey(key);
        }
        /**
         * Use {@link #getExecutionMetricsMap()} instead.
         */
        @Override
        @Deprecated
        public java.util.Map<String, org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricValue> getExecutionMetrics() {
          return getExecutionMetricsMap();
        }
        /**
         * <code>map&lt;string, .spark.connect.ExecutePlanResponse.Metrics.MetricValue&gt; execution_metrics = 4;</code>
         */
        @Override
        public java.util.Map<String, org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricValue> getExecutionMetricsMap() {
          return internalGetExecutionMetrics().getMap();
        }
        /**
         * <code>map&lt;string, .spark.connect.ExecutePlanResponse.Metrics.MetricValue&gt; execution_metrics = 4;</code>
         */
        @Override
        public /* nullable */
org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricValue getExecutionMetricsOrDefault(
            String key,
            /* nullable */
org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricValue defaultValue) {
          if (key == null) { throw new NullPointerException("map key"); }
          java.util.Map<String, org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricValue> map =
              internalGetExecutionMetrics().getMap();
          return map.containsKey(key) ? map.get(key) : defaultValue;
        }
        /**
         * <code>map&lt;string, .spark.connect.ExecutePlanResponse.Metrics.MetricValue&gt; execution_metrics = 4;</code>
         */
        @Override
        public org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricValue getExecutionMetricsOrThrow(
            String key) {
          if (key == null) { throw new NullPointerException("map key"); }
          java.util.Map<String, org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricValue> map =
              internalGetExecutionMetrics().getMap();
          if (!map.containsKey(key)) {
            throw new IllegalArgumentException();
          }
          return map.get(key);
        }
        public Builder clearExecutionMetrics() {
          bitField0_ = (bitField0_ & ~0x00000008);
          internalGetMutableExecutionMetrics().getMutableMap()
              .clear();
          return this;
        }
        /**
         * <code>map&lt;string, .spark.connect.ExecutePlanResponse.Metrics.MetricValue&gt; execution_metrics = 4;</code>
         */
        public Builder removeExecutionMetrics(
            String key) {
          if (key == null) { throw new NullPointerException("map key"); }
          internalGetMutableExecutionMetrics().getMutableMap()
              .remove(key);
          return this;
        }
        /**
         * Use alternate mutation accessors instead.
         */
        @Deprecated
        public java.util.Map<String, org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricValue>
            getMutableExecutionMetrics() {
          bitField0_ |= 0x00000008;
          return internalGetMutableExecutionMetrics().getMutableMap();
        }
        /**
         * <code>map&lt;string, .spark.connect.ExecutePlanResponse.Metrics.MetricValue&gt; execution_metrics = 4;</code>
         */
        public Builder putExecutionMetrics(
            String key,
            org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricValue value) {
          if (key == null) { throw new NullPointerException("map key"); }
          if (value == null) { throw new NullPointerException("map value"); }
          internalGetMutableExecutionMetrics().getMutableMap()
              .put(key, value);
          bitField0_ |= 0x00000008;
          return this;
        }
        /**
         * <code>map&lt;string, .spark.connect.ExecutePlanResponse.Metrics.MetricValue&gt; execution_metrics = 4;</code>
         */
        public Builder putAllExecutionMetrics(
            java.util.Map<String, org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricValue> values) {
          internalGetMutableExecutionMetrics().getMutableMap()
              .putAll(values);
          bitField0_ |= 0x00000008;
          return this;
        }
        @Override
        public final Builder setUnknownFields(
            final com.google.protobuf.UnknownFieldSet unknownFields) {
          return super.setUnknownFields(unknownFields);
        }

        @Override
        public final Builder mergeUnknownFields(
            final com.google.protobuf.UnknownFieldSet unknownFields) {
          return super.mergeUnknownFields(unknownFields);
        }


        // @@protoc_insertion_point(builder_scope:spark.connect.ExecutePlanResponse.Metrics.MetricObject)
      }

      // @@protoc_insertion_point(class_scope:spark.connect.ExecutePlanResponse.Metrics.MetricObject)
      private static final org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricObject DEFAULT_INSTANCE;
      static {
        DEFAULT_INSTANCE = new org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricObject();
      }

      public static org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricObject getDefaultInstance() {
        return DEFAULT_INSTANCE;
      }

      private static final com.google.protobuf.Parser<MetricObject>
          PARSER = new com.google.protobuf.AbstractParser<MetricObject>() {
        @Override
        public MetricObject parsePartialFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws com.google.protobuf.InvalidProtocolBufferException {
          Builder builder = newBuilder();
          try {
            builder.mergeFrom(input, extensionRegistry);
          } catch (com.google.protobuf.InvalidProtocolBufferException e) {
            throw e.setUnfinishedMessage(builder.buildPartial());
          } catch (com.google.protobuf.UninitializedMessageException e) {
            throw e.asInvalidProtocolBufferException().setUnfinishedMessage(builder.buildPartial());
          } catch (java.io.IOException e) {
            throw new com.google.protobuf.InvalidProtocolBufferException(e)
                .setUnfinishedMessage(builder.buildPartial());
          }
          return builder.buildPartial();
        }
      };

      public static com.google.protobuf.Parser<MetricObject> parser() {
        return PARSER;
      }

      @Override
      public com.google.protobuf.Parser<MetricObject> getParserForType() {
        return PARSER;
      }

      @Override
      public org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricObject getDefaultInstanceForType() {
        return DEFAULT_INSTANCE;
      }

    }

    public interface MetricValueOrBuilder extends
        // @@protoc_insertion_point(interface_extends:spark.connect.ExecutePlanResponse.Metrics.MetricValue)
        com.google.protobuf.MessageOrBuilder {

      /**
       * <code>string name = 1;</code>
       * @return The name.
       */
      String getName();
      /**
       * <code>string name = 1;</code>
       * @return The bytes for name.
       */
      com.google.protobuf.ByteString
          getNameBytes();

      /**
       * <code>int64 value = 2;</code>
       * @return The value.
       */
      long getValue();

      /**
       * <code>string metric_type = 3;</code>
       * @return The metricType.
       */
      String getMetricType();
      /**
       * <code>string metric_type = 3;</code>
       * @return The bytes for metricType.
       */
      com.google.protobuf.ByteString
          getMetricTypeBytes();
    }
    /**
     * Protobuf type {@code spark.connect.ExecutePlanResponse.Metrics.MetricValue}
     */
    public static final class MetricValue extends
        com.google.protobuf.GeneratedMessageV3 implements
        // @@protoc_insertion_point(message_implements:spark.connect.ExecutePlanResponse.Metrics.MetricValue)
        MetricValueOrBuilder {
    private static final long serialVersionUID = 0L;
      // Use MetricValue.newBuilder() to construct.
      private MetricValue(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
        super(builder);
      }
      private MetricValue() {
        name_ = "";
        metricType_ = "";
      }

      @Override
      @SuppressWarnings({"unused"})
      protected Object newInstance(
          UnusedPrivateParameter unused) {
        return new MetricValue();
      }

      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.connect.proto.Base.internal_static_spark_connect_ExecutePlanResponse_Metrics_MetricValue_descriptor;
      }

      @Override
      protected FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.connect.proto.Base.internal_static_spark_connect_ExecutePlanResponse_Metrics_MetricValue_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricValue.class, org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricValue.Builder.class);
      }

      public static final int NAME_FIELD_NUMBER = 1;
      @SuppressWarnings("serial")
      private volatile Object name_ = "";
      /**
       * <code>string name = 1;</code>
       * @return The name.
       */
      @Override
      public String getName() {
        Object ref = name_;
        if (ref instanceof String) {
          return (String) ref;
        } else {
          com.google.protobuf.ByteString bs = 
              (com.google.protobuf.ByteString) ref;
          String s = bs.toStringUtf8();
          name_ = s;
          return s;
        }
      }
      /**
       * <code>string name = 1;</code>
       * @return The bytes for name.
       */
      @Override
      public com.google.protobuf.ByteString
          getNameBytes() {
        Object ref = name_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (String) ref);
          name_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }

      public static final int VALUE_FIELD_NUMBER = 2;
      private long value_ = 0L;
      /**
       * <code>int64 value = 2;</code>
       * @return The value.
       */
      @Override
      public long getValue() {
        return value_;
      }

      public static final int METRIC_TYPE_FIELD_NUMBER = 3;
      @SuppressWarnings("serial")
      private volatile Object metricType_ = "";
      /**
       * <code>string metric_type = 3;</code>
       * @return The metricType.
       */
      @Override
      public String getMetricType() {
        Object ref = metricType_;
        if (ref instanceof String) {
          return (String) ref;
        } else {
          com.google.protobuf.ByteString bs = 
              (com.google.protobuf.ByteString) ref;
          String s = bs.toStringUtf8();
          metricType_ = s;
          return s;
        }
      }
      /**
       * <code>string metric_type = 3;</code>
       * @return The bytes for metricType.
       */
      @Override
      public com.google.protobuf.ByteString
          getMetricTypeBytes() {
        Object ref = metricType_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (String) ref);
          metricType_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }

      private byte memoizedIsInitialized = -1;
      @Override
      public final boolean isInitialized() {
        byte isInitialized = memoizedIsInitialized;
        if (isInitialized == 1) return true;
        if (isInitialized == 0) return false;

        memoizedIsInitialized = 1;
        return true;
      }

      @Override
      public void writeTo(com.google.protobuf.CodedOutputStream output)
                          throws java.io.IOException {
        if (!com.google.protobuf.GeneratedMessageV3.isStringEmpty(name_)) {
          com.google.protobuf.GeneratedMessageV3.writeString(output, 1, name_);
        }
        if (value_ != 0L) {
          output.writeInt64(2, value_);
        }
        if (!com.google.protobuf.GeneratedMessageV3.isStringEmpty(metricType_)) {
          com.google.protobuf.GeneratedMessageV3.writeString(output, 3, metricType_);
        }
        getUnknownFields().writeTo(output);
      }

      @Override
      public int getSerializedSize() {
        int size = memoizedSize;
        if (size != -1) return size;

        size = 0;
        if (!com.google.protobuf.GeneratedMessageV3.isStringEmpty(name_)) {
          size += com.google.protobuf.GeneratedMessageV3.computeStringSize(1, name_);
        }
        if (value_ != 0L) {
          size += com.google.protobuf.CodedOutputStream
            .computeInt64Size(2, value_);
        }
        if (!com.google.protobuf.GeneratedMessageV3.isStringEmpty(metricType_)) {
          size += com.google.protobuf.GeneratedMessageV3.computeStringSize(3, metricType_);
        }
        size += getUnknownFields().getSerializedSize();
        memoizedSize = size;
        return size;
      }

      @Override
      public boolean equals(final Object obj) {
        if (obj == this) {
         return true;
        }
        if (!(obj instanceof org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricValue)) {
          return super.equals(obj);
        }
        org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricValue other = (org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricValue) obj;

        if (!getName()
            .equals(other.getName())) return false;
        if (getValue()
            != other.getValue()) return false;
        if (!getMetricType()
            .equals(other.getMetricType())) return false;
        if (!getUnknownFields().equals(other.getUnknownFields())) return false;
        return true;
      }

      @Override
      public int hashCode() {
        if (memoizedHashCode != 0) {
          return memoizedHashCode;
        }
        int hash = 41;
        hash = (19 * hash) + getDescriptor().hashCode();
        hash = (37 * hash) + NAME_FIELD_NUMBER;
        hash = (53 * hash) + getName().hashCode();
        hash = (37 * hash) + VALUE_FIELD_NUMBER;
        hash = (53 * hash) + com.google.protobuf.Internal.hashLong(
            getValue());
        hash = (37 * hash) + METRIC_TYPE_FIELD_NUMBER;
        hash = (53 * hash) + getMetricType().hashCode();
        hash = (29 * hash) + getUnknownFields().hashCode();
        memoizedHashCode = hash;
        return hash;
      }

      public static org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricValue parseFrom(
          java.nio.ByteBuffer data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricValue parseFrom(
          java.nio.ByteBuffer data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricValue parseFrom(
          com.google.protobuf.ByteString data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricValue parseFrom(
          com.google.protobuf.ByteString data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricValue parseFrom(byte[] data)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data);
      }
      public static org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricValue parseFrom(
          byte[] data,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        return PARSER.parseFrom(data, extensionRegistry);
      }
      public static org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricValue parseFrom(java.io.InputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input);
      }
      public static org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricValue parseFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input, extensionRegistry);
      }

      public static org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricValue parseDelimitedFrom(java.io.InputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseDelimitedWithIOException(PARSER, input);
      }

      public static org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricValue parseDelimitedFrom(
          java.io.InputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
      }
      public static org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricValue parseFrom(
          com.google.protobuf.CodedInputStream input)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input);
      }
      public static org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricValue parseFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        return com.google.protobuf.GeneratedMessageV3
            .parseWithIOException(PARSER, input, extensionRegistry);
      }

      @Override
      public Builder newBuilderForType() { return newBuilder(); }
      public static Builder newBuilder() {
        return DEFAULT_INSTANCE.toBuilder();
      }
      public static Builder newBuilder(org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricValue prototype) {
        return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
      }
      @Override
      public Builder toBuilder() {
        return this == DEFAULT_INSTANCE
            ? new Builder() : new Builder().mergeFrom(this);
      }

      @Override
      protected Builder newBuilderForType(
          BuilderParent parent) {
        Builder builder = new Builder(parent);
        return builder;
      }
      /**
       * Protobuf type {@code spark.connect.ExecutePlanResponse.Metrics.MetricValue}
       */
      public static final class Builder extends
          com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
          // @@protoc_insertion_point(builder_implements:spark.connect.ExecutePlanResponse.Metrics.MetricValue)
          org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricValueOrBuilder {
        public static final com.google.protobuf.Descriptors.Descriptor
            getDescriptor() {
          return org.apache.spark.connect.proto.Base.internal_static_spark_connect_ExecutePlanResponse_Metrics_MetricValue_descriptor;
        }

        @Override
        protected FieldAccessorTable
            internalGetFieldAccessorTable() {
          return org.apache.spark.connect.proto.Base.internal_static_spark_connect_ExecutePlanResponse_Metrics_MetricValue_fieldAccessorTable
              .ensureFieldAccessorsInitialized(
                  org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricValue.class, org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricValue.Builder.class);
        }

        // Construct using org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricValue.newBuilder()
        private Builder() {

        }

        private Builder(
            BuilderParent parent) {
          super(parent);

        }
        @Override
        public Builder clear() {
          super.clear();
          bitField0_ = 0;
          name_ = "";
          value_ = 0L;
          metricType_ = "";
          return this;
        }

        @Override
        public com.google.protobuf.Descriptors.Descriptor
            getDescriptorForType() {
          return org.apache.spark.connect.proto.Base.internal_static_spark_connect_ExecutePlanResponse_Metrics_MetricValue_descriptor;
        }

        @Override
        public org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricValue getDefaultInstanceForType() {
          return org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricValue.getDefaultInstance();
        }

        @Override
        public org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricValue build() {
          org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricValue result = buildPartial();
          if (!result.isInitialized()) {
            throw newUninitializedMessageException(result);
          }
          return result;
        }

        @Override
        public org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricValue buildPartial() {
          org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricValue result = new org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricValue(this);
          if (bitField0_ != 0) { buildPartial0(result); }
          onBuilt();
          return result;
        }

        private void buildPartial0(org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricValue result) {
          int from_bitField0_ = bitField0_;
          if (((from_bitField0_ & 0x00000001) != 0)) {
            result.name_ = name_;
          }
          if (((from_bitField0_ & 0x00000002) != 0)) {
            result.value_ = value_;
          }
          if (((from_bitField0_ & 0x00000004) != 0)) {
            result.metricType_ = metricType_;
          }
        }

        @Override
        public Builder clone() {
          return super.clone();
        }
        @Override
        public Builder setField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            Object value) {
          return super.setField(field, value);
        }
        @Override
        public Builder clearField(
            com.google.protobuf.Descriptors.FieldDescriptor field) {
          return super.clearField(field);
        }
        @Override
        public Builder clearOneof(
            com.google.protobuf.Descriptors.OneofDescriptor oneof) {
          return super.clearOneof(oneof);
        }
        @Override
        public Builder setRepeatedField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            int index, Object value) {
          return super.setRepeatedField(field, index, value);
        }
        @Override
        public Builder addRepeatedField(
            com.google.protobuf.Descriptors.FieldDescriptor field,
            Object value) {
          return super.addRepeatedField(field, value);
        }
        @Override
        public Builder mergeFrom(com.google.protobuf.Message other) {
          if (other instanceof org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricValue) {
            return mergeFrom((org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricValue)other);
          } else {
            super.mergeFrom(other);
            return this;
          }
        }

        public Builder mergeFrom(org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricValue other) {
          if (other == org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricValue.getDefaultInstance()) return this;
          if (!other.getName().isEmpty()) {
            name_ = other.name_;
            bitField0_ |= 0x00000001;
            onChanged();
          }
          if (other.getValue() != 0L) {
            setValue(other.getValue());
          }
          if (!other.getMetricType().isEmpty()) {
            metricType_ = other.metricType_;
            bitField0_ |= 0x00000004;
            onChanged();
          }
          this.mergeUnknownFields(other.getUnknownFields());
          onChanged();
          return this;
        }

        @Override
        public final boolean isInitialized() {
          return true;
        }

        @Override
        public Builder mergeFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws java.io.IOException {
          if (extensionRegistry == null) {
            throw new NullPointerException();
          }
          try {
            boolean done = false;
            while (!done) {
              int tag = input.readTag();
              switch (tag) {
                case 0:
                  done = true;
                  break;
                case 10: {
                  name_ = input.readStringRequireUtf8();
                  bitField0_ |= 0x00000001;
                  break;
                } // case 10
                case 16: {
                  value_ = input.readInt64();
                  bitField0_ |= 0x00000002;
                  break;
                } // case 16
                case 26: {
                  metricType_ = input.readStringRequireUtf8();
                  bitField0_ |= 0x00000004;
                  break;
                } // case 26
                default: {
                  if (!super.parseUnknownField(input, extensionRegistry, tag)) {
                    done = true; // was an endgroup tag
                  }
                  break;
                } // default:
              } // switch (tag)
            } // while (!done)
          } catch (com.google.protobuf.InvalidProtocolBufferException e) {
            throw e.unwrapIOException();
          } finally {
            onChanged();
          } // finally
          return this;
        }
        private int bitField0_;

        private Object name_ = "";
        /**
         * <code>string name = 1;</code>
         * @return The name.
         */
        public String getName() {
          Object ref = name_;
          if (!(ref instanceof String)) {
            com.google.protobuf.ByteString bs =
                (com.google.protobuf.ByteString) ref;
            String s = bs.toStringUtf8();
            name_ = s;
            return s;
          } else {
            return (String) ref;
          }
        }
        /**
         * <code>string name = 1;</code>
         * @return The bytes for name.
         */
        public com.google.protobuf.ByteString
            getNameBytes() {
          Object ref = name_;
          if (ref instanceof String) {
            com.google.protobuf.ByteString b = 
                com.google.protobuf.ByteString.copyFromUtf8(
                    (String) ref);
            name_ = b;
            return b;
          } else {
            return (com.google.protobuf.ByteString) ref;
          }
        }
        /**
         * <code>string name = 1;</code>
         * @param value The name to set.
         * @return This builder for chaining.
         */
        public Builder setName(
            String value) {
          if (value == null) { throw new NullPointerException(); }
          name_ = value;
          bitField0_ |= 0x00000001;
          onChanged();
          return this;
        }
        /**
         * <code>string name = 1;</code>
         * @return This builder for chaining.
         */
        public Builder clearName() {
          name_ = getDefaultInstance().getName();
          bitField0_ = (bitField0_ & ~0x00000001);
          onChanged();
          return this;
        }
        /**
         * <code>string name = 1;</code>
         * @param value The bytes for name to set.
         * @return This builder for chaining.
         */
        public Builder setNameBytes(
            com.google.protobuf.ByteString value) {
          if (value == null) { throw new NullPointerException(); }
          checkByteStringIsUtf8(value);
          name_ = value;
          bitField0_ |= 0x00000001;
          onChanged();
          return this;
        }

        private long value_ ;
        /**
         * <code>int64 value = 2;</code>
         * @return The value.
         */
        @Override
        public long getValue() {
          return value_;
        }
        /**
         * <code>int64 value = 2;</code>
         * @param value The value to set.
         * @return This builder for chaining.
         */
        public Builder setValue(long value) {

          value_ = value;
          bitField0_ |= 0x00000002;
          onChanged();
          return this;
        }
        /**
         * <code>int64 value = 2;</code>
         * @return This builder for chaining.
         */
        public Builder clearValue() {
          bitField0_ = (bitField0_ & ~0x00000002);
          value_ = 0L;
          onChanged();
          return this;
        }

        private Object metricType_ = "";
        /**
         * <code>string metric_type = 3;</code>
         * @return The metricType.
         */
        public String getMetricType() {
          Object ref = metricType_;
          if (!(ref instanceof String)) {
            com.google.protobuf.ByteString bs =
                (com.google.protobuf.ByteString) ref;
            String s = bs.toStringUtf8();
            metricType_ = s;
            return s;
          } else {
            return (String) ref;
          }
        }
        /**
         * <code>string metric_type = 3;</code>
         * @return The bytes for metricType.
         */
        public com.google.protobuf.ByteString
            getMetricTypeBytes() {
          Object ref = metricType_;
          if (ref instanceof String) {
            com.google.protobuf.ByteString b = 
                com.google.protobuf.ByteString.copyFromUtf8(
                    (String) ref);
            metricType_ = b;
            return b;
          } else {
            return (com.google.protobuf.ByteString) ref;
          }
        }
        /**
         * <code>string metric_type = 3;</code>
         * @param value The metricType to set.
         * @return This builder for chaining.
         */
        public Builder setMetricType(
            String value) {
          if (value == null) { throw new NullPointerException(); }
          metricType_ = value;
          bitField0_ |= 0x00000004;
          onChanged();
          return this;
        }
        /**
         * <code>string metric_type = 3;</code>
         * @return This builder for chaining.
         */
        public Builder clearMetricType() {
          metricType_ = getDefaultInstance().getMetricType();
          bitField0_ = (bitField0_ & ~0x00000004);
          onChanged();
          return this;
        }
        /**
         * <code>string metric_type = 3;</code>
         * @param value The bytes for metricType to set.
         * @return This builder for chaining.
         */
        public Builder setMetricTypeBytes(
            com.google.protobuf.ByteString value) {
          if (value == null) { throw new NullPointerException(); }
          checkByteStringIsUtf8(value);
          metricType_ = value;
          bitField0_ |= 0x00000004;
          onChanged();
          return this;
        }
        @Override
        public final Builder setUnknownFields(
            final com.google.protobuf.UnknownFieldSet unknownFields) {
          return super.setUnknownFields(unknownFields);
        }

        @Override
        public final Builder mergeUnknownFields(
            final com.google.protobuf.UnknownFieldSet unknownFields) {
          return super.mergeUnknownFields(unknownFields);
        }


        // @@protoc_insertion_point(builder_scope:spark.connect.ExecutePlanResponse.Metrics.MetricValue)
      }

      // @@protoc_insertion_point(class_scope:spark.connect.ExecutePlanResponse.Metrics.MetricValue)
      private static final org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricValue DEFAULT_INSTANCE;
      static {
        DEFAULT_INSTANCE = new org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricValue();
      }

      public static org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricValue getDefaultInstance() {
        return DEFAULT_INSTANCE;
      }

      private static final com.google.protobuf.Parser<MetricValue>
          PARSER = new com.google.protobuf.AbstractParser<MetricValue>() {
        @Override
        public MetricValue parsePartialFrom(
            com.google.protobuf.CodedInputStream input,
            com.google.protobuf.ExtensionRegistryLite extensionRegistry)
            throws com.google.protobuf.InvalidProtocolBufferException {
          Builder builder = newBuilder();
          try {
            builder.mergeFrom(input, extensionRegistry);
          } catch (com.google.protobuf.InvalidProtocolBufferException e) {
            throw e.setUnfinishedMessage(builder.buildPartial());
          } catch (com.google.protobuf.UninitializedMessageException e) {
            throw e.asInvalidProtocolBufferException().setUnfinishedMessage(builder.buildPartial());
          } catch (java.io.IOException e) {
            throw new com.google.protobuf.InvalidProtocolBufferException(e)
                .setUnfinishedMessage(builder.buildPartial());
          }
          return builder.buildPartial();
        }
      };

      public static com.google.protobuf.Parser<MetricValue> parser() {
        return PARSER;
      }

      @Override
      public com.google.protobuf.Parser<MetricValue> getParserForType() {
        return PARSER;
      }

      @Override
      public org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricValue getDefaultInstanceForType() {
        return DEFAULT_INSTANCE;
      }

    }

    public static final int METRICS_FIELD_NUMBER = 1;
    @SuppressWarnings("serial")
    private java.util.List<org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricObject> metrics_;
    /**
     * <code>repeated .spark.connect.ExecutePlanResponse.Metrics.MetricObject metrics = 1;</code>
     */
    @Override
    public java.util.List<org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricObject> getMetricsList() {
      return metrics_;
    }
    /**
     * <code>repeated .spark.connect.ExecutePlanResponse.Metrics.MetricObject metrics = 1;</code>
     */
    @Override
    public java.util.List<? extends org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricObjectOrBuilder> 
        getMetricsOrBuilderList() {
      return metrics_;
    }
    /**
     * <code>repeated .spark.connect.ExecutePlanResponse.Metrics.MetricObject metrics = 1;</code>
     */
    @Override
    public int getMetricsCount() {
      return metrics_.size();
    }
    /**
     * <code>repeated .spark.connect.ExecutePlanResponse.Metrics.MetricObject metrics = 1;</code>
     */
    @Override
    public org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricObject getMetrics(int index) {
      return metrics_.get(index);
    }
    /**
     * <code>repeated .spark.connect.ExecutePlanResponse.Metrics.MetricObject metrics = 1;</code>
     */
    @Override
    public org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricObjectOrBuilder getMetricsOrBuilder(
        int index) {
      return metrics_.get(index);
    }

    private byte memoizedIsInitialized = -1;
    @Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      for (int i = 0; i < metrics_.size(); i++) {
        output.writeMessage(1, metrics_.get(i));
      }
      getUnknownFields().writeTo(output);
    }

    @Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      for (int i = 0; i < metrics_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(1, metrics_.get(i));
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @Override
    public boolean equals(final Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.connect.proto.ExecutePlanResponse.Metrics)) {
        return super.equals(obj);
      }
      org.apache.spark.connect.proto.ExecutePlanResponse.Metrics other = (org.apache.spark.connect.proto.ExecutePlanResponse.Metrics) obj;

      if (!getMetricsList()
          .equals(other.getMetricsList())) return false;
      if (!getUnknownFields().equals(other.getUnknownFields())) return false;
      return true;
    }

    @Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      if (getMetricsCount() > 0) {
        hash = (37 * hash) + METRICS_FIELD_NUMBER;
        hash = (53 * hash) + getMetricsList().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.connect.proto.ExecutePlanResponse.Metrics parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.connect.proto.ExecutePlanResponse.Metrics parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.connect.proto.ExecutePlanResponse.Metrics parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.connect.proto.ExecutePlanResponse.Metrics parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.connect.proto.ExecutePlanResponse.Metrics parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.connect.proto.ExecutePlanResponse.Metrics parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.connect.proto.ExecutePlanResponse.Metrics parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.connect.proto.ExecutePlanResponse.Metrics parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    public static org.apache.spark.connect.proto.ExecutePlanResponse.Metrics parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }

    public static org.apache.spark.connect.proto.ExecutePlanResponse.Metrics parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.connect.proto.ExecutePlanResponse.Metrics parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.connect.proto.ExecutePlanResponse.Metrics parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.connect.proto.ExecutePlanResponse.Metrics prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @Override
    protected Builder newBuilderForType(
        BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code spark.connect.ExecutePlanResponse.Metrics}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:spark.connect.ExecutePlanResponse.Metrics)
        org.apache.spark.connect.proto.ExecutePlanResponse.MetricsOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.connect.proto.Base.internal_static_spark_connect_ExecutePlanResponse_Metrics_descriptor;
      }

      @Override
      protected FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.connect.proto.Base.internal_static_spark_connect_ExecutePlanResponse_Metrics_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.class, org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.Builder.class);
      }

      // Construct using org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.newBuilder()
      private Builder() {

      }

      private Builder(
          BuilderParent parent) {
        super(parent);

      }
      @Override
      public Builder clear() {
        super.clear();
        bitField0_ = 0;
        if (metricsBuilder_ == null) {
          metrics_ = java.util.Collections.emptyList();
        } else {
          metrics_ = null;
          metricsBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000001);
        return this;
      }

      @Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.connect.proto.Base.internal_static_spark_connect_ExecutePlanResponse_Metrics_descriptor;
      }

      @Override
      public org.apache.spark.connect.proto.ExecutePlanResponse.Metrics getDefaultInstanceForType() {
        return org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.getDefaultInstance();
      }

      @Override
      public org.apache.spark.connect.proto.ExecutePlanResponse.Metrics build() {
        org.apache.spark.connect.proto.ExecutePlanResponse.Metrics result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @Override
      public org.apache.spark.connect.proto.ExecutePlanResponse.Metrics buildPartial() {
        org.apache.spark.connect.proto.ExecutePlanResponse.Metrics result = new org.apache.spark.connect.proto.ExecutePlanResponse.Metrics(this);
        buildPartialRepeatedFields(result);
        if (bitField0_ != 0) { buildPartial0(result); }
        onBuilt();
        return result;
      }

      private void buildPartialRepeatedFields(org.apache.spark.connect.proto.ExecutePlanResponse.Metrics result) {
        if (metricsBuilder_ == null) {
          if (((bitField0_ & 0x00000001) != 0)) {
            metrics_ = java.util.Collections.unmodifiableList(metrics_);
            bitField0_ = (bitField0_ & ~0x00000001);
          }
          result.metrics_ = metrics_;
        } else {
          result.metrics_ = metricsBuilder_.build();
        }
      }

      private void buildPartial0(org.apache.spark.connect.proto.ExecutePlanResponse.Metrics result) {
        int from_bitField0_ = bitField0_;
      }

      @Override
      public Builder clone() {
        return super.clone();
      }
      @Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          Object value) {
        return super.setField(field, value);
      }
      @Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          Object value) {
        return super.addRepeatedField(field, value);
      }
      @Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.connect.proto.ExecutePlanResponse.Metrics) {
          return mergeFrom((org.apache.spark.connect.proto.ExecutePlanResponse.Metrics)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.connect.proto.ExecutePlanResponse.Metrics other) {
        if (other == org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.getDefaultInstance()) return this;
        if (metricsBuilder_ == null) {
          if (!other.metrics_.isEmpty()) {
            if (metrics_.isEmpty()) {
              metrics_ = other.metrics_;
              bitField0_ = (bitField0_ & ~0x00000001);
            } else {
              ensureMetricsIsMutable();
              metrics_.addAll(other.metrics_);
            }
            onChanged();
          }
        } else {
          if (!other.metrics_.isEmpty()) {
            if (metricsBuilder_.isEmpty()) {
              metricsBuilder_.dispose();
              metricsBuilder_ = null;
              metrics_ = other.metrics_;
              bitField0_ = (bitField0_ & ~0x00000001);
              metricsBuilder_ = 
                com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                   getMetricsFieldBuilder() : null;
            } else {
              metricsBuilder_.addAllMessages(other.metrics_);
            }
          }
        }
        this.mergeUnknownFields(other.getUnknownFields());
        onChanged();
        return this;
      }

      @Override
      public final boolean isInitialized() {
        return true;
      }

      @Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        if (extensionRegistry == null) {
          throw new NullPointerException();
        }
        try {
          boolean done = false;
          while (!done) {
            int tag = input.readTag();
            switch (tag) {
              case 0:
                done = true;
                break;
              case 10: {
                org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricObject m =
                    input.readMessage(
                        org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricObject.parser(),
                        extensionRegistry);
                if (metricsBuilder_ == null) {
                  ensureMetricsIsMutable();
                  metrics_.add(m);
                } else {
                  metricsBuilder_.addMessage(m);
                }
                break;
              } // case 10
              default: {
                if (!super.parseUnknownField(input, extensionRegistry, tag)) {
                  done = true; // was an endgroup tag
                }
                break;
              } // default:
            } // switch (tag)
          } // while (!done)
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.unwrapIOException();
        } finally {
          onChanged();
        } // finally
        return this;
      }
      private int bitField0_;

      private java.util.List<org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricObject> metrics_ =
        java.util.Collections.emptyList();
      private void ensureMetricsIsMutable() {
        if (!((bitField0_ & 0x00000001) != 0)) {
          metrics_ = new java.util.ArrayList<org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricObject>(metrics_);
          bitField0_ |= 0x00000001;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilderV3<
          org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricObject, org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricObject.Builder, org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricObjectOrBuilder> metricsBuilder_;

      /**
       * <code>repeated .spark.connect.ExecutePlanResponse.Metrics.MetricObject metrics = 1;</code>
       */
      public java.util.List<org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricObject> getMetricsList() {
        if (metricsBuilder_ == null) {
          return java.util.Collections.unmodifiableList(metrics_);
        } else {
          return metricsBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .spark.connect.ExecutePlanResponse.Metrics.MetricObject metrics = 1;</code>
       */
      public int getMetricsCount() {
        if (metricsBuilder_ == null) {
          return metrics_.size();
        } else {
          return metricsBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .spark.connect.ExecutePlanResponse.Metrics.MetricObject metrics = 1;</code>
       */
      public org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricObject getMetrics(int index) {
        if (metricsBuilder_ == null) {
          return metrics_.get(index);
        } else {
          return metricsBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .spark.connect.ExecutePlanResponse.Metrics.MetricObject metrics = 1;</code>
       */
      public Builder setMetrics(
          int index, org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricObject value) {
        if (metricsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureMetricsIsMutable();
          metrics_.set(index, value);
          onChanged();
        } else {
          metricsBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .spark.connect.ExecutePlanResponse.Metrics.MetricObject metrics = 1;</code>
       */
      public Builder setMetrics(
          int index, org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricObject.Builder builderForValue) {
        if (metricsBuilder_ == null) {
          ensureMetricsIsMutable();
          metrics_.set(index, builderForValue.build());
          onChanged();
        } else {
          metricsBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .spark.connect.ExecutePlanResponse.Metrics.MetricObject metrics = 1;</code>
       */
      public Builder addMetrics(org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricObject value) {
        if (metricsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureMetricsIsMutable();
          metrics_.add(value);
          onChanged();
        } else {
          metricsBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .spark.connect.ExecutePlanResponse.Metrics.MetricObject metrics = 1;</code>
       */
      public Builder addMetrics(
          int index, org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricObject value) {
        if (metricsBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureMetricsIsMutable();
          metrics_.add(index, value);
          onChanged();
        } else {
          metricsBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .spark.connect.ExecutePlanResponse.Metrics.MetricObject metrics = 1;</code>
       */
      public Builder addMetrics(
          org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricObject.Builder builderForValue) {
        if (metricsBuilder_ == null) {
          ensureMetricsIsMutable();
          metrics_.add(builderForValue.build());
          onChanged();
        } else {
          metricsBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .spark.connect.ExecutePlanResponse.Metrics.MetricObject metrics = 1;</code>
       */
      public Builder addMetrics(
          int index, org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricObject.Builder builderForValue) {
        if (metricsBuilder_ == null) {
          ensureMetricsIsMutable();
          metrics_.add(index, builderForValue.build());
          onChanged();
        } else {
          metricsBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .spark.connect.ExecutePlanResponse.Metrics.MetricObject metrics = 1;</code>
       */
      public Builder addAllMetrics(
          Iterable<? extends org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricObject> values) {
        if (metricsBuilder_ == null) {
          ensureMetricsIsMutable();
          com.google.protobuf.AbstractMessageLite.Builder.addAll(
              values, metrics_);
          onChanged();
        } else {
          metricsBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .spark.connect.ExecutePlanResponse.Metrics.MetricObject metrics = 1;</code>
       */
      public Builder clearMetrics() {
        if (metricsBuilder_ == null) {
          metrics_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000001);
          onChanged();
        } else {
          metricsBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .spark.connect.ExecutePlanResponse.Metrics.MetricObject metrics = 1;</code>
       */
      public Builder removeMetrics(int index) {
        if (metricsBuilder_ == null) {
          ensureMetricsIsMutable();
          metrics_.remove(index);
          onChanged();
        } else {
          metricsBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .spark.connect.ExecutePlanResponse.Metrics.MetricObject metrics = 1;</code>
       */
      public org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricObject.Builder getMetricsBuilder(
          int index) {
        return getMetricsFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .spark.connect.ExecutePlanResponse.Metrics.MetricObject metrics = 1;</code>
       */
      public org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricObjectOrBuilder getMetricsOrBuilder(
          int index) {
        if (metricsBuilder_ == null) {
          return metrics_.get(index);  } else {
          return metricsBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .spark.connect.ExecutePlanResponse.Metrics.MetricObject metrics = 1;</code>
       */
      public java.util.List<? extends org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricObjectOrBuilder> 
           getMetricsOrBuilderList() {
        if (metricsBuilder_ != null) {
          return metricsBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(metrics_);
        }
      }
      /**
       * <code>repeated .spark.connect.ExecutePlanResponse.Metrics.MetricObject metrics = 1;</code>
       */
      public org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricObject.Builder addMetricsBuilder() {
        return getMetricsFieldBuilder().addBuilder(
            org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricObject.getDefaultInstance());
      }
      /**
       * <code>repeated .spark.connect.ExecutePlanResponse.Metrics.MetricObject metrics = 1;</code>
       */
      public org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricObject.Builder addMetricsBuilder(
          int index) {
        return getMetricsFieldBuilder().addBuilder(
            index, org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricObject.getDefaultInstance());
      }
      /**
       * <code>repeated .spark.connect.ExecutePlanResponse.Metrics.MetricObject metrics = 1;</code>
       */
      public java.util.List<org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricObject.Builder> 
           getMetricsBuilderList() {
        return getMetricsFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilderV3<
          org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricObject, org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricObject.Builder, org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricObjectOrBuilder> 
          getMetricsFieldBuilder() {
        if (metricsBuilder_ == null) {
          metricsBuilder_ = new com.google.protobuf.RepeatedFieldBuilderV3<
              org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricObject, org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricObject.Builder, org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.MetricObjectOrBuilder>(
                  metrics_,
                  ((bitField0_ & 0x00000001) != 0),
                  getParentForChildren(),
                  isClean());
          metrics_ = null;
        }
        return metricsBuilder_;
      }
      @Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:spark.connect.ExecutePlanResponse.Metrics)
    }

    // @@protoc_insertion_point(class_scope:spark.connect.ExecutePlanResponse.Metrics)
    private static final org.apache.spark.connect.proto.ExecutePlanResponse.Metrics DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.connect.proto.ExecutePlanResponse.Metrics();
    }

    public static org.apache.spark.connect.proto.ExecutePlanResponse.Metrics getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<Metrics>
        PARSER = new com.google.protobuf.AbstractParser<Metrics>() {
      @Override
      public Metrics parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        Builder builder = newBuilder();
        try {
          builder.mergeFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.setUnfinishedMessage(builder.buildPartial());
        } catch (com.google.protobuf.UninitializedMessageException e) {
          throw e.asInvalidProtocolBufferException().setUnfinishedMessage(builder.buildPartial());
        } catch (java.io.IOException e) {
          throw new com.google.protobuf.InvalidProtocolBufferException(e)
              .setUnfinishedMessage(builder.buildPartial());
        }
        return builder.buildPartial();
      }
    };

    public static com.google.protobuf.Parser<Metrics> parser() {
      return PARSER;
    }

    @Override
    public com.google.protobuf.Parser<Metrics> getParserForType() {
      return PARSER;
    }

    @Override
    public org.apache.spark.connect.proto.ExecutePlanResponse.Metrics getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface ObservedMetricsOrBuilder extends
      // @@protoc_insertion_point(interface_extends:spark.connect.ExecutePlanResponse.ObservedMetrics)
      com.google.protobuf.MessageOrBuilder {

    /**
     * <code>string name = 1;</code>
     * @return The name.
     */
    String getName();
    /**
     * <code>string name = 1;</code>
     * @return The bytes for name.
     */
    com.google.protobuf.ByteString
        getNameBytes();

    /**
     * <code>repeated .spark.connect.Expression.Literal values = 2;</code>
     */
    java.util.List<org.apache.spark.connect.proto.Expression.Literal> 
        getValuesList();
    /**
     * <code>repeated .spark.connect.Expression.Literal values = 2;</code>
     */
    org.apache.spark.connect.proto.Expression.Literal getValues(int index);
    /**
     * <code>repeated .spark.connect.Expression.Literal values = 2;</code>
     */
    int getValuesCount();
    /**
     * <code>repeated .spark.connect.Expression.Literal values = 2;</code>
     */
    java.util.List<? extends org.apache.spark.connect.proto.Expression.LiteralOrBuilder> 
        getValuesOrBuilderList();
    /**
     * <code>repeated .spark.connect.Expression.Literal values = 2;</code>
     */
    org.apache.spark.connect.proto.Expression.LiteralOrBuilder getValuesOrBuilder(
        int index);
  }
  /**
   * Protobuf type {@code spark.connect.ExecutePlanResponse.ObservedMetrics}
   */
  public static final class ObservedMetrics extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:spark.connect.ExecutePlanResponse.ObservedMetrics)
      ObservedMetricsOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use ObservedMetrics.newBuilder() to construct.
    private ObservedMetrics(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private ObservedMetrics() {
      name_ = "";
      values_ = java.util.Collections.emptyList();
    }

    @Override
    @SuppressWarnings({"unused"})
    protected Object newInstance(
        UnusedPrivateParameter unused) {
      return new ObservedMetrics();
    }

    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.connect.proto.Base.internal_static_spark_connect_ExecutePlanResponse_ObservedMetrics_descriptor;
    }

    @Override
    protected FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.connect.proto.Base.internal_static_spark_connect_ExecutePlanResponse_ObservedMetrics_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.connect.proto.ExecutePlanResponse.ObservedMetrics.class, org.apache.spark.connect.proto.ExecutePlanResponse.ObservedMetrics.Builder.class);
    }

    public static final int NAME_FIELD_NUMBER = 1;
    @SuppressWarnings("serial")
    private volatile Object name_ = "";
    /**
     * <code>string name = 1;</code>
     * @return The name.
     */
    @Override
    public String getName() {
      Object ref = name_;
      if (ref instanceof String) {
        return (String) ref;
      } else {
        com.google.protobuf.ByteString bs = 
            (com.google.protobuf.ByteString) ref;
        String s = bs.toStringUtf8();
        name_ = s;
        return s;
      }
    }
    /**
     * <code>string name = 1;</code>
     * @return The bytes for name.
     */
    @Override
    public com.google.protobuf.ByteString
        getNameBytes() {
      Object ref = name_;
      if (ref instanceof String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (String) ref);
        name_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }

    public static final int VALUES_FIELD_NUMBER = 2;
    @SuppressWarnings("serial")
    private java.util.List<org.apache.spark.connect.proto.Expression.Literal> values_;
    /**
     * <code>repeated .spark.connect.Expression.Literal values = 2;</code>
     */
    @Override
    public java.util.List<org.apache.spark.connect.proto.Expression.Literal> getValuesList() {
      return values_;
    }
    /**
     * <code>repeated .spark.connect.Expression.Literal values = 2;</code>
     */
    @Override
    public java.util.List<? extends org.apache.spark.connect.proto.Expression.LiteralOrBuilder> 
        getValuesOrBuilderList() {
      return values_;
    }
    /**
     * <code>repeated .spark.connect.Expression.Literal values = 2;</code>
     */
    @Override
    public int getValuesCount() {
      return values_.size();
    }
    /**
     * <code>repeated .spark.connect.Expression.Literal values = 2;</code>
     */
    @Override
    public org.apache.spark.connect.proto.Expression.Literal getValues(int index) {
      return values_.get(index);
    }
    /**
     * <code>repeated .spark.connect.Expression.Literal values = 2;</code>
     */
    @Override
    public org.apache.spark.connect.proto.Expression.LiteralOrBuilder getValuesOrBuilder(
        int index) {
      return values_.get(index);
    }

    private byte memoizedIsInitialized = -1;
    @Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      if (!com.google.protobuf.GeneratedMessageV3.isStringEmpty(name_)) {
        com.google.protobuf.GeneratedMessageV3.writeString(output, 1, name_);
      }
      for (int i = 0; i < values_.size(); i++) {
        output.writeMessage(2, values_.get(i));
      }
      getUnknownFields().writeTo(output);
    }

    @Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      if (!com.google.protobuf.GeneratedMessageV3.isStringEmpty(name_)) {
        size += com.google.protobuf.GeneratedMessageV3.computeStringSize(1, name_);
      }
      for (int i = 0; i < values_.size(); i++) {
        size += com.google.protobuf.CodedOutputStream
          .computeMessageSize(2, values_.get(i));
      }
      size += getUnknownFields().getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @Override
    public boolean equals(final Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.connect.proto.ExecutePlanResponse.ObservedMetrics)) {
        return super.equals(obj);
      }
      org.apache.spark.connect.proto.ExecutePlanResponse.ObservedMetrics other = (org.apache.spark.connect.proto.ExecutePlanResponse.ObservedMetrics) obj;

      if (!getName()
          .equals(other.getName())) return false;
      if (!getValuesList()
          .equals(other.getValuesList())) return false;
      if (!getUnknownFields().equals(other.getUnknownFields())) return false;
      return true;
    }

    @Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      hash = (37 * hash) + NAME_FIELD_NUMBER;
      hash = (53 * hash) + getName().hashCode();
      if (getValuesCount() > 0) {
        hash = (37 * hash) + VALUES_FIELD_NUMBER;
        hash = (53 * hash) + getValuesList().hashCode();
      }
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.connect.proto.ExecutePlanResponse.ObservedMetrics parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.connect.proto.ExecutePlanResponse.ObservedMetrics parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.connect.proto.ExecutePlanResponse.ObservedMetrics parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.connect.proto.ExecutePlanResponse.ObservedMetrics parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.connect.proto.ExecutePlanResponse.ObservedMetrics parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.connect.proto.ExecutePlanResponse.ObservedMetrics parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.connect.proto.ExecutePlanResponse.ObservedMetrics parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.connect.proto.ExecutePlanResponse.ObservedMetrics parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    public static org.apache.spark.connect.proto.ExecutePlanResponse.ObservedMetrics parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }

    public static org.apache.spark.connect.proto.ExecutePlanResponse.ObservedMetrics parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.connect.proto.ExecutePlanResponse.ObservedMetrics parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.connect.proto.ExecutePlanResponse.ObservedMetrics parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.connect.proto.ExecutePlanResponse.ObservedMetrics prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @Override
    protected Builder newBuilderForType(
        BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * Protobuf type {@code spark.connect.ExecutePlanResponse.ObservedMetrics}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:spark.connect.ExecutePlanResponse.ObservedMetrics)
        org.apache.spark.connect.proto.ExecutePlanResponse.ObservedMetricsOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.connect.proto.Base.internal_static_spark_connect_ExecutePlanResponse_ObservedMetrics_descriptor;
      }

      @Override
      protected FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.connect.proto.Base.internal_static_spark_connect_ExecutePlanResponse_ObservedMetrics_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.connect.proto.ExecutePlanResponse.ObservedMetrics.class, org.apache.spark.connect.proto.ExecutePlanResponse.ObservedMetrics.Builder.class);
      }

      // Construct using org.apache.spark.connect.proto.ExecutePlanResponse.ObservedMetrics.newBuilder()
      private Builder() {

      }

      private Builder(
          BuilderParent parent) {
        super(parent);

      }
      @Override
      public Builder clear() {
        super.clear();
        bitField0_ = 0;
        name_ = "";
        if (valuesBuilder_ == null) {
          values_ = java.util.Collections.emptyList();
        } else {
          values_ = null;
          valuesBuilder_.clear();
        }
        bitField0_ = (bitField0_ & ~0x00000002);
        return this;
      }

      @Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.connect.proto.Base.internal_static_spark_connect_ExecutePlanResponse_ObservedMetrics_descriptor;
      }

      @Override
      public org.apache.spark.connect.proto.ExecutePlanResponse.ObservedMetrics getDefaultInstanceForType() {
        return org.apache.spark.connect.proto.ExecutePlanResponse.ObservedMetrics.getDefaultInstance();
      }

      @Override
      public org.apache.spark.connect.proto.ExecutePlanResponse.ObservedMetrics build() {
        org.apache.spark.connect.proto.ExecutePlanResponse.ObservedMetrics result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @Override
      public org.apache.spark.connect.proto.ExecutePlanResponse.ObservedMetrics buildPartial() {
        org.apache.spark.connect.proto.ExecutePlanResponse.ObservedMetrics result = new org.apache.spark.connect.proto.ExecutePlanResponse.ObservedMetrics(this);
        buildPartialRepeatedFields(result);
        if (bitField0_ != 0) { buildPartial0(result); }
        onBuilt();
        return result;
      }

      private void buildPartialRepeatedFields(org.apache.spark.connect.proto.ExecutePlanResponse.ObservedMetrics result) {
        if (valuesBuilder_ == null) {
          if (((bitField0_ & 0x00000002) != 0)) {
            values_ = java.util.Collections.unmodifiableList(values_);
            bitField0_ = (bitField0_ & ~0x00000002);
          }
          result.values_ = values_;
        } else {
          result.values_ = valuesBuilder_.build();
        }
      }

      private void buildPartial0(org.apache.spark.connect.proto.ExecutePlanResponse.ObservedMetrics result) {
        int from_bitField0_ = bitField0_;
        if (((from_bitField0_ & 0x00000001) != 0)) {
          result.name_ = name_;
        }
      }

      @Override
      public Builder clone() {
        return super.clone();
      }
      @Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          Object value) {
        return super.setField(field, value);
      }
      @Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          Object value) {
        return super.addRepeatedField(field, value);
      }
      @Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.connect.proto.ExecutePlanResponse.ObservedMetrics) {
          return mergeFrom((org.apache.spark.connect.proto.ExecutePlanResponse.ObservedMetrics)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.connect.proto.ExecutePlanResponse.ObservedMetrics other) {
        if (other == org.apache.spark.connect.proto.ExecutePlanResponse.ObservedMetrics.getDefaultInstance()) return this;
        if (!other.getName().isEmpty()) {
          name_ = other.name_;
          bitField0_ |= 0x00000001;
          onChanged();
        }
        if (valuesBuilder_ == null) {
          if (!other.values_.isEmpty()) {
            if (values_.isEmpty()) {
              values_ = other.values_;
              bitField0_ = (bitField0_ & ~0x00000002);
            } else {
              ensureValuesIsMutable();
              values_.addAll(other.values_);
            }
            onChanged();
          }
        } else {
          if (!other.values_.isEmpty()) {
            if (valuesBuilder_.isEmpty()) {
              valuesBuilder_.dispose();
              valuesBuilder_ = null;
              values_ = other.values_;
              bitField0_ = (bitField0_ & ~0x00000002);
              valuesBuilder_ = 
                com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                   getValuesFieldBuilder() : null;
            } else {
              valuesBuilder_.addAllMessages(other.values_);
            }
          }
        }
        this.mergeUnknownFields(other.getUnknownFields());
        onChanged();
        return this;
      }

      @Override
      public final boolean isInitialized() {
        return true;
      }

      @Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        if (extensionRegistry == null) {
          throw new NullPointerException();
        }
        try {
          boolean done = false;
          while (!done) {
            int tag = input.readTag();
            switch (tag) {
              case 0:
                done = true;
                break;
              case 10: {
                name_ = input.readStringRequireUtf8();
                bitField0_ |= 0x00000001;
                break;
              } // case 10
              case 18: {
                org.apache.spark.connect.proto.Expression.Literal m =
                    input.readMessage(
                        org.apache.spark.connect.proto.Expression.Literal.parser(),
                        extensionRegistry);
                if (valuesBuilder_ == null) {
                  ensureValuesIsMutable();
                  values_.add(m);
                } else {
                  valuesBuilder_.addMessage(m);
                }
                break;
              } // case 18
              default: {
                if (!super.parseUnknownField(input, extensionRegistry, tag)) {
                  done = true; // was an endgroup tag
                }
                break;
              } // default:
            } // switch (tag)
          } // while (!done)
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.unwrapIOException();
        } finally {
          onChanged();
        } // finally
        return this;
      }
      private int bitField0_;

      private Object name_ = "";
      /**
       * <code>string name = 1;</code>
       * @return The name.
       */
      public String getName() {
        Object ref = name_;
        if (!(ref instanceof String)) {
          com.google.protobuf.ByteString bs =
              (com.google.protobuf.ByteString) ref;
          String s = bs.toStringUtf8();
          name_ = s;
          return s;
        } else {
          return (String) ref;
        }
      }
      /**
       * <code>string name = 1;</code>
       * @return The bytes for name.
       */
      public com.google.protobuf.ByteString
          getNameBytes() {
        Object ref = name_;
        if (ref instanceof String) {
          com.google.protobuf.ByteString b = 
              com.google.protobuf.ByteString.copyFromUtf8(
                  (String) ref);
          name_ = b;
          return b;
        } else {
          return (com.google.protobuf.ByteString) ref;
        }
      }
      /**
       * <code>string name = 1;</code>
       * @param value The name to set.
       * @return This builder for chaining.
       */
      public Builder setName(
          String value) {
        if (value == null) { throw new NullPointerException(); }
        name_ = value;
        bitField0_ |= 0x00000001;
        onChanged();
        return this;
      }
      /**
       * <code>string name = 1;</code>
       * @return This builder for chaining.
       */
      public Builder clearName() {
        name_ = getDefaultInstance().getName();
        bitField0_ = (bitField0_ & ~0x00000001);
        onChanged();
        return this;
      }
      /**
       * <code>string name = 1;</code>
       * @param value The bytes for name to set.
       * @return This builder for chaining.
       */
      public Builder setNameBytes(
          com.google.protobuf.ByteString value) {
        if (value == null) { throw new NullPointerException(); }
        checkByteStringIsUtf8(value);
        name_ = value;
        bitField0_ |= 0x00000001;
        onChanged();
        return this;
      }

      private java.util.List<org.apache.spark.connect.proto.Expression.Literal> values_ =
        java.util.Collections.emptyList();
      private void ensureValuesIsMutable() {
        if (!((bitField0_ & 0x00000002) != 0)) {
          values_ = new java.util.ArrayList<org.apache.spark.connect.proto.Expression.Literal>(values_);
          bitField0_ |= 0x00000002;
         }
      }

      private com.google.protobuf.RepeatedFieldBuilderV3<
          org.apache.spark.connect.proto.Expression.Literal, org.apache.spark.connect.proto.Expression.Literal.Builder, org.apache.spark.connect.proto.Expression.LiteralOrBuilder> valuesBuilder_;

      /**
       * <code>repeated .spark.connect.Expression.Literal values = 2;</code>
       */
      public java.util.List<org.apache.spark.connect.proto.Expression.Literal> getValuesList() {
        if (valuesBuilder_ == null) {
          return java.util.Collections.unmodifiableList(values_);
        } else {
          return valuesBuilder_.getMessageList();
        }
      }
      /**
       * <code>repeated .spark.connect.Expression.Literal values = 2;</code>
       */
      public int getValuesCount() {
        if (valuesBuilder_ == null) {
          return values_.size();
        } else {
          return valuesBuilder_.getCount();
        }
      }
      /**
       * <code>repeated .spark.connect.Expression.Literal values = 2;</code>
       */
      public org.apache.spark.connect.proto.Expression.Literal getValues(int index) {
        if (valuesBuilder_ == null) {
          return values_.get(index);
        } else {
          return valuesBuilder_.getMessage(index);
        }
      }
      /**
       * <code>repeated .spark.connect.Expression.Literal values = 2;</code>
       */
      public Builder setValues(
          int index, org.apache.spark.connect.proto.Expression.Literal value) {
        if (valuesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureValuesIsMutable();
          values_.set(index, value);
          onChanged();
        } else {
          valuesBuilder_.setMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .spark.connect.Expression.Literal values = 2;</code>
       */
      public Builder setValues(
          int index, org.apache.spark.connect.proto.Expression.Literal.Builder builderForValue) {
        if (valuesBuilder_ == null) {
          ensureValuesIsMutable();
          values_.set(index, builderForValue.build());
          onChanged();
        } else {
          valuesBuilder_.setMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .spark.connect.Expression.Literal values = 2;</code>
       */
      public Builder addValues(org.apache.spark.connect.proto.Expression.Literal value) {
        if (valuesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureValuesIsMutable();
          values_.add(value);
          onChanged();
        } else {
          valuesBuilder_.addMessage(value);
        }
        return this;
      }
      /**
       * <code>repeated .spark.connect.Expression.Literal values = 2;</code>
       */
      public Builder addValues(
          int index, org.apache.spark.connect.proto.Expression.Literal value) {
        if (valuesBuilder_ == null) {
          if (value == null) {
            throw new NullPointerException();
          }
          ensureValuesIsMutable();
          values_.add(index, value);
          onChanged();
        } else {
          valuesBuilder_.addMessage(index, value);
        }
        return this;
      }
      /**
       * <code>repeated .spark.connect.Expression.Literal values = 2;</code>
       */
      public Builder addValues(
          org.apache.spark.connect.proto.Expression.Literal.Builder builderForValue) {
        if (valuesBuilder_ == null) {
          ensureValuesIsMutable();
          values_.add(builderForValue.build());
          onChanged();
        } else {
          valuesBuilder_.addMessage(builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .spark.connect.Expression.Literal values = 2;</code>
       */
      public Builder addValues(
          int index, org.apache.spark.connect.proto.Expression.Literal.Builder builderForValue) {
        if (valuesBuilder_ == null) {
          ensureValuesIsMutable();
          values_.add(index, builderForValue.build());
          onChanged();
        } else {
          valuesBuilder_.addMessage(index, builderForValue.build());
        }
        return this;
      }
      /**
       * <code>repeated .spark.connect.Expression.Literal values = 2;</code>
       */
      public Builder addAllValues(
          Iterable<? extends org.apache.spark.connect.proto.Expression.Literal> values) {
        if (valuesBuilder_ == null) {
          ensureValuesIsMutable();
          com.google.protobuf.AbstractMessageLite.Builder.addAll(
              values, values_);
          onChanged();
        } else {
          valuesBuilder_.addAllMessages(values);
        }
        return this;
      }
      /**
       * <code>repeated .spark.connect.Expression.Literal values = 2;</code>
       */
      public Builder clearValues() {
        if (valuesBuilder_ == null) {
          values_ = java.util.Collections.emptyList();
          bitField0_ = (bitField0_ & ~0x00000002);
          onChanged();
        } else {
          valuesBuilder_.clear();
        }
        return this;
      }
      /**
       * <code>repeated .spark.connect.Expression.Literal values = 2;</code>
       */
      public Builder removeValues(int index) {
        if (valuesBuilder_ == null) {
          ensureValuesIsMutable();
          values_.remove(index);
          onChanged();
        } else {
          valuesBuilder_.remove(index);
        }
        return this;
      }
      /**
       * <code>repeated .spark.connect.Expression.Literal values = 2;</code>
       */
      public org.apache.spark.connect.proto.Expression.Literal.Builder getValuesBuilder(
          int index) {
        return getValuesFieldBuilder().getBuilder(index);
      }
      /**
       * <code>repeated .spark.connect.Expression.Literal values = 2;</code>
       */
      public org.apache.spark.connect.proto.Expression.LiteralOrBuilder getValuesOrBuilder(
          int index) {
        if (valuesBuilder_ == null) {
          return values_.get(index);  } else {
          return valuesBuilder_.getMessageOrBuilder(index);
        }
      }
      /**
       * <code>repeated .spark.connect.Expression.Literal values = 2;</code>
       */
      public java.util.List<? extends org.apache.spark.connect.proto.Expression.LiteralOrBuilder> 
           getValuesOrBuilderList() {
        if (valuesBuilder_ != null) {
          return valuesBuilder_.getMessageOrBuilderList();
        } else {
          return java.util.Collections.unmodifiableList(values_);
        }
      }
      /**
       * <code>repeated .spark.connect.Expression.Literal values = 2;</code>
       */
      public org.apache.spark.connect.proto.Expression.Literal.Builder addValuesBuilder() {
        return getValuesFieldBuilder().addBuilder(
            org.apache.spark.connect.proto.Expression.Literal.getDefaultInstance());
      }
      /**
       * <code>repeated .spark.connect.Expression.Literal values = 2;</code>
       */
      public org.apache.spark.connect.proto.Expression.Literal.Builder addValuesBuilder(
          int index) {
        return getValuesFieldBuilder().addBuilder(
            index, org.apache.spark.connect.proto.Expression.Literal.getDefaultInstance());
      }
      /**
       * <code>repeated .spark.connect.Expression.Literal values = 2;</code>
       */
      public java.util.List<org.apache.spark.connect.proto.Expression.Literal.Builder> 
           getValuesBuilderList() {
        return getValuesFieldBuilder().getBuilderList();
      }
      private com.google.protobuf.RepeatedFieldBuilderV3<
          org.apache.spark.connect.proto.Expression.Literal, org.apache.spark.connect.proto.Expression.Literal.Builder, org.apache.spark.connect.proto.Expression.LiteralOrBuilder> 
          getValuesFieldBuilder() {
        if (valuesBuilder_ == null) {
          valuesBuilder_ = new com.google.protobuf.RepeatedFieldBuilderV3<
              org.apache.spark.connect.proto.Expression.Literal, org.apache.spark.connect.proto.Expression.Literal.Builder, org.apache.spark.connect.proto.Expression.LiteralOrBuilder>(
                  values_,
                  ((bitField0_ & 0x00000002) != 0),
                  getParentForChildren(),
                  isClean());
          values_ = null;
        }
        return valuesBuilder_;
      }
      @Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:spark.connect.ExecutePlanResponse.ObservedMetrics)
    }

    // @@protoc_insertion_point(class_scope:spark.connect.ExecutePlanResponse.ObservedMetrics)
    private static final org.apache.spark.connect.proto.ExecutePlanResponse.ObservedMetrics DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.connect.proto.ExecutePlanResponse.ObservedMetrics();
    }

    public static org.apache.spark.connect.proto.ExecutePlanResponse.ObservedMetrics getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<ObservedMetrics>
        PARSER = new com.google.protobuf.AbstractParser<ObservedMetrics>() {
      @Override
      public ObservedMetrics parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        Builder builder = newBuilder();
        try {
          builder.mergeFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.setUnfinishedMessage(builder.buildPartial());
        } catch (com.google.protobuf.UninitializedMessageException e) {
          throw e.asInvalidProtocolBufferException().setUnfinishedMessage(builder.buildPartial());
        } catch (java.io.IOException e) {
          throw new com.google.protobuf.InvalidProtocolBufferException(e)
              .setUnfinishedMessage(builder.buildPartial());
        }
        return builder.buildPartial();
      }
    };

    public static com.google.protobuf.Parser<ObservedMetrics> parser() {
      return PARSER;
    }

    @Override
    public com.google.protobuf.Parser<ObservedMetrics> getParserForType() {
      return PARSER;
    }

    @Override
    public org.apache.spark.connect.proto.ExecutePlanResponse.ObservedMetrics getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  public interface ResultCompleteOrBuilder extends
      // @@protoc_insertion_point(interface_extends:spark.connect.ExecutePlanResponse.ResultComplete)
      com.google.protobuf.MessageOrBuilder {
  }
  /**
   * <pre>
   * If present, in a reattachable execution this means that after server sends onComplete,
   * the execution is complete. If the server sends onComplete without sending a ResultComplete,
   * it means that there is more, and the client should use ReattachExecute RPC to continue.
   * </pre>
   *
   * Protobuf type {@code spark.connect.ExecutePlanResponse.ResultComplete}
   */
  public static final class ResultComplete extends
      com.google.protobuf.GeneratedMessageV3 implements
      // @@protoc_insertion_point(message_implements:spark.connect.ExecutePlanResponse.ResultComplete)
      ResultCompleteOrBuilder {
  private static final long serialVersionUID = 0L;
    // Use ResultComplete.newBuilder() to construct.
    private ResultComplete(com.google.protobuf.GeneratedMessageV3.Builder<?> builder) {
      super(builder);
    }
    private ResultComplete() {
    }

    @Override
    @SuppressWarnings({"unused"})
    protected Object newInstance(
        UnusedPrivateParameter unused) {
      return new ResultComplete();
    }

    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.connect.proto.Base.internal_static_spark_connect_ExecutePlanResponse_ResultComplete_descriptor;
    }

    @Override
    protected FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.connect.proto.Base.internal_static_spark_connect_ExecutePlanResponse_ResultComplete_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.connect.proto.ExecutePlanResponse.ResultComplete.class, org.apache.spark.connect.proto.ExecutePlanResponse.ResultComplete.Builder.class);
    }

    private byte memoizedIsInitialized = -1;
    @Override
    public final boolean isInitialized() {
      byte isInitialized = memoizedIsInitialized;
      if (isInitialized == 1) return true;
      if (isInitialized == 0) return false;

      memoizedIsInitialized = 1;
      return true;
    }

    @Override
    public void writeTo(com.google.protobuf.CodedOutputStream output)
                        throws java.io.IOException {
      getUnknownFields().writeTo(output);
    }

    @Override
    public int getSerializedSize() {
      int size = memoizedSize;
      if (size != -1) return size;

      size = 0;
      size += getUnknownFields().getSerializedSize();
      memoizedSize = size;
      return size;
    }

    @Override
    public boolean equals(final Object obj) {
      if (obj == this) {
       return true;
      }
      if (!(obj instanceof org.apache.spark.connect.proto.ExecutePlanResponse.ResultComplete)) {
        return super.equals(obj);
      }
      org.apache.spark.connect.proto.ExecutePlanResponse.ResultComplete other = (org.apache.spark.connect.proto.ExecutePlanResponse.ResultComplete) obj;

      if (!getUnknownFields().equals(other.getUnknownFields())) return false;
      return true;
    }

    @Override
    public int hashCode() {
      if (memoizedHashCode != 0) {
        return memoizedHashCode;
      }
      int hash = 41;
      hash = (19 * hash) + getDescriptor().hashCode();
      hash = (29 * hash) + getUnknownFields().hashCode();
      memoizedHashCode = hash;
      return hash;
    }

    public static org.apache.spark.connect.proto.ExecutePlanResponse.ResultComplete parseFrom(
        java.nio.ByteBuffer data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.connect.proto.ExecutePlanResponse.ResultComplete parseFrom(
        java.nio.ByteBuffer data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.connect.proto.ExecutePlanResponse.ResultComplete parseFrom(
        com.google.protobuf.ByteString data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.connect.proto.ExecutePlanResponse.ResultComplete parseFrom(
        com.google.protobuf.ByteString data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.connect.proto.ExecutePlanResponse.ResultComplete parseFrom(byte[] data)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data);
    }
    public static org.apache.spark.connect.proto.ExecutePlanResponse.ResultComplete parseFrom(
        byte[] data,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      return PARSER.parseFrom(data, extensionRegistry);
    }
    public static org.apache.spark.connect.proto.ExecutePlanResponse.ResultComplete parseFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.connect.proto.ExecutePlanResponse.ResultComplete parseFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    public static org.apache.spark.connect.proto.ExecutePlanResponse.ResultComplete parseDelimitedFrom(java.io.InputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input);
    }

    public static org.apache.spark.connect.proto.ExecutePlanResponse.ResultComplete parseDelimitedFrom(
        java.io.InputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
    }
    public static org.apache.spark.connect.proto.ExecutePlanResponse.ResultComplete parseFrom(
        com.google.protobuf.CodedInputStream input)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input);
    }
    public static org.apache.spark.connect.proto.ExecutePlanResponse.ResultComplete parseFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      return com.google.protobuf.GeneratedMessageV3
          .parseWithIOException(PARSER, input, extensionRegistry);
    }

    @Override
    public Builder newBuilderForType() { return newBuilder(); }
    public static Builder newBuilder() {
      return DEFAULT_INSTANCE.toBuilder();
    }
    public static Builder newBuilder(org.apache.spark.connect.proto.ExecutePlanResponse.ResultComplete prototype) {
      return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
    }
    @Override
    public Builder toBuilder() {
      return this == DEFAULT_INSTANCE
          ? new Builder() : new Builder().mergeFrom(this);
    }

    @Override
    protected Builder newBuilderForType(
        BuilderParent parent) {
      Builder builder = new Builder(parent);
      return builder;
    }
    /**
     * <pre>
     * If present, in a reattachable execution this means that after server sends onComplete,
     * the execution is complete. If the server sends onComplete without sending a ResultComplete,
     * it means that there is more, and the client should use ReattachExecute RPC to continue.
     * </pre>
     *
     * Protobuf type {@code spark.connect.ExecutePlanResponse.ResultComplete}
     */
    public static final class Builder extends
        com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
        // @@protoc_insertion_point(builder_implements:spark.connect.ExecutePlanResponse.ResultComplete)
        org.apache.spark.connect.proto.ExecutePlanResponse.ResultCompleteOrBuilder {
      public static final com.google.protobuf.Descriptors.Descriptor
          getDescriptor() {
        return org.apache.spark.connect.proto.Base.internal_static_spark_connect_ExecutePlanResponse_ResultComplete_descriptor;
      }

      @Override
      protected FieldAccessorTable
          internalGetFieldAccessorTable() {
        return org.apache.spark.connect.proto.Base.internal_static_spark_connect_ExecutePlanResponse_ResultComplete_fieldAccessorTable
            .ensureFieldAccessorsInitialized(
                org.apache.spark.connect.proto.ExecutePlanResponse.ResultComplete.class, org.apache.spark.connect.proto.ExecutePlanResponse.ResultComplete.Builder.class);
      }

      // Construct using org.apache.spark.connect.proto.ExecutePlanResponse.ResultComplete.newBuilder()
      private Builder() {

      }

      private Builder(
          BuilderParent parent) {
        super(parent);

      }
      @Override
      public Builder clear() {
        super.clear();
        return this;
      }

      @Override
      public com.google.protobuf.Descriptors.Descriptor
          getDescriptorForType() {
        return org.apache.spark.connect.proto.Base.internal_static_spark_connect_ExecutePlanResponse_ResultComplete_descriptor;
      }

      @Override
      public org.apache.spark.connect.proto.ExecutePlanResponse.ResultComplete getDefaultInstanceForType() {
        return org.apache.spark.connect.proto.ExecutePlanResponse.ResultComplete.getDefaultInstance();
      }

      @Override
      public org.apache.spark.connect.proto.ExecutePlanResponse.ResultComplete build() {
        org.apache.spark.connect.proto.ExecutePlanResponse.ResultComplete result = buildPartial();
        if (!result.isInitialized()) {
          throw newUninitializedMessageException(result);
        }
        return result;
      }

      @Override
      public org.apache.spark.connect.proto.ExecutePlanResponse.ResultComplete buildPartial() {
        org.apache.spark.connect.proto.ExecutePlanResponse.ResultComplete result = new org.apache.spark.connect.proto.ExecutePlanResponse.ResultComplete(this);
        onBuilt();
        return result;
      }

      @Override
      public Builder clone() {
        return super.clone();
      }
      @Override
      public Builder setField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          Object value) {
        return super.setField(field, value);
      }
      @Override
      public Builder clearField(
          com.google.protobuf.Descriptors.FieldDescriptor field) {
        return super.clearField(field);
      }
      @Override
      public Builder clearOneof(
          com.google.protobuf.Descriptors.OneofDescriptor oneof) {
        return super.clearOneof(oneof);
      }
      @Override
      public Builder setRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          int index, Object value) {
        return super.setRepeatedField(field, index, value);
      }
      @Override
      public Builder addRepeatedField(
          com.google.protobuf.Descriptors.FieldDescriptor field,
          Object value) {
        return super.addRepeatedField(field, value);
      }
      @Override
      public Builder mergeFrom(com.google.protobuf.Message other) {
        if (other instanceof org.apache.spark.connect.proto.ExecutePlanResponse.ResultComplete) {
          return mergeFrom((org.apache.spark.connect.proto.ExecutePlanResponse.ResultComplete)other);
        } else {
          super.mergeFrom(other);
          return this;
        }
      }

      public Builder mergeFrom(org.apache.spark.connect.proto.ExecutePlanResponse.ResultComplete other) {
        if (other == org.apache.spark.connect.proto.ExecutePlanResponse.ResultComplete.getDefaultInstance()) return this;
        this.mergeUnknownFields(other.getUnknownFields());
        onChanged();
        return this;
      }

      @Override
      public final boolean isInitialized() {
        return true;
      }

      @Override
      public Builder mergeFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws java.io.IOException {
        if (extensionRegistry == null) {
          throw new NullPointerException();
        }
        try {
          boolean done = false;
          while (!done) {
            int tag = input.readTag();
            switch (tag) {
              case 0:
                done = true;
                break;
              default: {
                if (!super.parseUnknownField(input, extensionRegistry, tag)) {
                  done = true; // was an endgroup tag
                }
                break;
              } // default:
            } // switch (tag)
          } // while (!done)
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.unwrapIOException();
        } finally {
          onChanged();
        } // finally
        return this;
      }
      @Override
      public final Builder setUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.setUnknownFields(unknownFields);
      }

      @Override
      public final Builder mergeUnknownFields(
          final com.google.protobuf.UnknownFieldSet unknownFields) {
        return super.mergeUnknownFields(unknownFields);
      }


      // @@protoc_insertion_point(builder_scope:spark.connect.ExecutePlanResponse.ResultComplete)
    }

    // @@protoc_insertion_point(class_scope:spark.connect.ExecutePlanResponse.ResultComplete)
    private static final org.apache.spark.connect.proto.ExecutePlanResponse.ResultComplete DEFAULT_INSTANCE;
    static {
      DEFAULT_INSTANCE = new org.apache.spark.connect.proto.ExecutePlanResponse.ResultComplete();
    }

    public static org.apache.spark.connect.proto.ExecutePlanResponse.ResultComplete getDefaultInstance() {
      return DEFAULT_INSTANCE;
    }

    private static final com.google.protobuf.Parser<ResultComplete>
        PARSER = new com.google.protobuf.AbstractParser<ResultComplete>() {
      @Override
      public ResultComplete parsePartialFrom(
          com.google.protobuf.CodedInputStream input,
          com.google.protobuf.ExtensionRegistryLite extensionRegistry)
          throws com.google.protobuf.InvalidProtocolBufferException {
        Builder builder = newBuilder();
        try {
          builder.mergeFrom(input, extensionRegistry);
        } catch (com.google.protobuf.InvalidProtocolBufferException e) {
          throw e.setUnfinishedMessage(builder.buildPartial());
        } catch (com.google.protobuf.UninitializedMessageException e) {
          throw e.asInvalidProtocolBufferException().setUnfinishedMessage(builder.buildPartial());
        } catch (java.io.IOException e) {
          throw new com.google.protobuf.InvalidProtocolBufferException(e)
              .setUnfinishedMessage(builder.buildPartial());
        }
        return builder.buildPartial();
      }
    };

    public static com.google.protobuf.Parser<ResultComplete> parser() {
      return PARSER;
    }

    @Override
    public com.google.protobuf.Parser<ResultComplete> getParserForType() {
      return PARSER;
    }

    @Override
    public org.apache.spark.connect.proto.ExecutePlanResponse.ResultComplete getDefaultInstanceForType() {
      return DEFAULT_INSTANCE;
    }

  }

  private int responseTypeCase_ = 0;
  @SuppressWarnings("serial")
  private Object responseType_;
  public enum ResponseTypeCase
      implements com.google.protobuf.Internal.EnumLite,
          InternalOneOfEnum {
    ARROW_BATCH(2),
    SQL_COMMAND_RESULT(5),
    WRITE_STREAM_OPERATION_START_RESULT(8),
    STREAMING_QUERY_COMMAND_RESULT(9),
    GET_RESOURCES_COMMAND_RESULT(10),
    STREAMING_QUERY_MANAGER_COMMAND_RESULT(11),
    RESULT_COMPLETE(14),
    EXTENSION(999),
    RESPONSETYPE_NOT_SET(0);
    private final int value;
    private ResponseTypeCase(int value) {
      this.value = value;
    }
    /**
     * @param value The number of the enum to look for.
     * @return The enum associated with the given number.
     * @deprecated Use {@link #forNumber(int)} instead.
     */
    @Deprecated
    public static ResponseTypeCase valueOf(int value) {
      return forNumber(value);
    }

    public static ResponseTypeCase forNumber(int value) {
      switch (value) {
        case 2: return ARROW_BATCH;
        case 5: return SQL_COMMAND_RESULT;
        case 8: return WRITE_STREAM_OPERATION_START_RESULT;
        case 9: return STREAMING_QUERY_COMMAND_RESULT;
        case 10: return GET_RESOURCES_COMMAND_RESULT;
        case 11: return STREAMING_QUERY_MANAGER_COMMAND_RESULT;
        case 14: return RESULT_COMPLETE;
        case 999: return EXTENSION;
        case 0: return RESPONSETYPE_NOT_SET;
        default: return null;
      }
    }
    public int getNumber() {
      return this.value;
    }
  };

  public ResponseTypeCase
  getResponseTypeCase() {
    return ResponseTypeCase.forNumber(
        responseTypeCase_);
  }

  public static final int SESSION_ID_FIELD_NUMBER = 1;
  @SuppressWarnings("serial")
  private volatile Object sessionId_ = "";
  /**
   * <code>string session_id = 1;</code>
   * @return The sessionId.
   */
  @Override
  public String getSessionId() {
    Object ref = sessionId_;
    if (ref instanceof String) {
      return (String) ref;
    } else {
      com.google.protobuf.ByteString bs = 
          (com.google.protobuf.ByteString) ref;
      String s = bs.toStringUtf8();
      sessionId_ = s;
      return s;
    }
  }
  /**
   * <code>string session_id = 1;</code>
   * @return The bytes for sessionId.
   */
  @Override
  public com.google.protobuf.ByteString
      getSessionIdBytes() {
    Object ref = sessionId_;
    if (ref instanceof String) {
      com.google.protobuf.ByteString b = 
          com.google.protobuf.ByteString.copyFromUtf8(
              (String) ref);
      sessionId_ = b;
      return b;
    } else {
      return (com.google.protobuf.ByteString) ref;
    }
  }

  public static final int OPERATION_ID_FIELD_NUMBER = 12;
  @SuppressWarnings("serial")
  private volatile Object operationId_ = "";
  /**
   * <pre>
   * Identifies the ExecutePlan execution.
   * If set by the client in ExecutePlanRequest.operationId, that value is returned.
   * Otherwise generated by the server.
   * It is an UUID string of the format `00112233-4455-6677-8899-aabbccddeeff`
   * </pre>
   *
   * <code>string operation_id = 12;</code>
   * @return The operationId.
   */
  @Override
  public String getOperationId() {
    Object ref = operationId_;
    if (ref instanceof String) {
      return (String) ref;
    } else {
      com.google.protobuf.ByteString bs = 
          (com.google.protobuf.ByteString) ref;
      String s = bs.toStringUtf8();
      operationId_ = s;
      return s;
    }
  }
  /**
   * <pre>
   * Identifies the ExecutePlan execution.
   * If set by the client in ExecutePlanRequest.operationId, that value is returned.
   * Otherwise generated by the server.
   * It is an UUID string of the format `00112233-4455-6677-8899-aabbccddeeff`
   * </pre>
   *
   * <code>string operation_id = 12;</code>
   * @return The bytes for operationId.
   */
  @Override
  public com.google.protobuf.ByteString
      getOperationIdBytes() {
    Object ref = operationId_;
    if (ref instanceof String) {
      com.google.protobuf.ByteString b = 
          com.google.protobuf.ByteString.copyFromUtf8(
              (String) ref);
      operationId_ = b;
      return b;
    } else {
      return (com.google.protobuf.ByteString) ref;
    }
  }

  public static final int RESPONSE_ID_FIELD_NUMBER = 13;
  @SuppressWarnings("serial")
  private volatile Object responseId_ = "";
  /**
   * <pre>
   * Identified the response in the stream.
   * The id is an UUID string of the format `00112233-4455-6677-8899-aabbccddeeff`
   * </pre>
   *
   * <code>string response_id = 13;</code>
   * @return The responseId.
   */
  @Override
  public String getResponseId() {
    Object ref = responseId_;
    if (ref instanceof String) {
      return (String) ref;
    } else {
      com.google.protobuf.ByteString bs = 
          (com.google.protobuf.ByteString) ref;
      String s = bs.toStringUtf8();
      responseId_ = s;
      return s;
    }
  }
  /**
   * <pre>
   * Identified the response in the stream.
   * The id is an UUID string of the format `00112233-4455-6677-8899-aabbccddeeff`
   * </pre>
   *
   * <code>string response_id = 13;</code>
   * @return The bytes for responseId.
   */
  @Override
  public com.google.protobuf.ByteString
      getResponseIdBytes() {
    Object ref = responseId_;
    if (ref instanceof String) {
      com.google.protobuf.ByteString b = 
          com.google.protobuf.ByteString.copyFromUtf8(
              (String) ref);
      responseId_ = b;
      return b;
    } else {
      return (com.google.protobuf.ByteString) ref;
    }
  }

  public static final int ARROW_BATCH_FIELD_NUMBER = 2;
  /**
   * <code>.spark.connect.ExecutePlanResponse.ArrowBatch arrow_batch = 2;</code>
   * @return Whether the arrowBatch field is set.
   */
  @Override
  public boolean hasArrowBatch() {
    return responseTypeCase_ == 2;
  }
  /**
   * <code>.spark.connect.ExecutePlanResponse.ArrowBatch arrow_batch = 2;</code>
   * @return The arrowBatch.
   */
  @Override
  public org.apache.spark.connect.proto.ExecutePlanResponse.ArrowBatch getArrowBatch() {
    if (responseTypeCase_ == 2) {
       return (org.apache.spark.connect.proto.ExecutePlanResponse.ArrowBatch) responseType_;
    }
    return org.apache.spark.connect.proto.ExecutePlanResponse.ArrowBatch.getDefaultInstance();
  }
  /**
   * <code>.spark.connect.ExecutePlanResponse.ArrowBatch arrow_batch = 2;</code>
   */
  @Override
  public org.apache.spark.connect.proto.ExecutePlanResponse.ArrowBatchOrBuilder getArrowBatchOrBuilder() {
    if (responseTypeCase_ == 2) {
       return (org.apache.spark.connect.proto.ExecutePlanResponse.ArrowBatch) responseType_;
    }
    return org.apache.spark.connect.proto.ExecutePlanResponse.ArrowBatch.getDefaultInstance();
  }

  public static final int SQL_COMMAND_RESULT_FIELD_NUMBER = 5;
  /**
   * <pre>
   * Special case for executing SQL commands.
   * </pre>
   *
   * <code>.spark.connect.ExecutePlanResponse.SqlCommandResult sql_command_result = 5;</code>
   * @return Whether the sqlCommandResult field is set.
   */
  @Override
  public boolean hasSqlCommandResult() {
    return responseTypeCase_ == 5;
  }
  /**
   * <pre>
   * Special case for executing SQL commands.
   * </pre>
   *
   * <code>.spark.connect.ExecutePlanResponse.SqlCommandResult sql_command_result = 5;</code>
   * @return The sqlCommandResult.
   */
  @Override
  public org.apache.spark.connect.proto.ExecutePlanResponse.SqlCommandResult getSqlCommandResult() {
    if (responseTypeCase_ == 5) {
       return (org.apache.spark.connect.proto.ExecutePlanResponse.SqlCommandResult) responseType_;
    }
    return org.apache.spark.connect.proto.ExecutePlanResponse.SqlCommandResult.getDefaultInstance();
  }
  /**
   * <pre>
   * Special case for executing SQL commands.
   * </pre>
   *
   * <code>.spark.connect.ExecutePlanResponse.SqlCommandResult sql_command_result = 5;</code>
   */
  @Override
  public org.apache.spark.connect.proto.ExecutePlanResponse.SqlCommandResultOrBuilder getSqlCommandResultOrBuilder() {
    if (responseTypeCase_ == 5) {
       return (org.apache.spark.connect.proto.ExecutePlanResponse.SqlCommandResult) responseType_;
    }
    return org.apache.spark.connect.proto.ExecutePlanResponse.SqlCommandResult.getDefaultInstance();
  }

  public static final int WRITE_STREAM_OPERATION_START_RESULT_FIELD_NUMBER = 8;
  /**
   * <pre>
   * Response for a streaming query.
   * </pre>
   *
   * <code>.spark.connect.WriteStreamOperationStartResult write_stream_operation_start_result = 8;</code>
   * @return Whether the writeStreamOperationStartResult field is set.
   */
  @Override
  public boolean hasWriteStreamOperationStartResult() {
    return responseTypeCase_ == 8;
  }
  /**
   * <pre>
   * Response for a streaming query.
   * </pre>
   *
   * <code>.spark.connect.WriteStreamOperationStartResult write_stream_operation_start_result = 8;</code>
   * @return The writeStreamOperationStartResult.
   */
  @Override
  public org.apache.spark.connect.proto.WriteStreamOperationStartResult getWriteStreamOperationStartResult() {
    if (responseTypeCase_ == 8) {
       return (org.apache.spark.connect.proto.WriteStreamOperationStartResult) responseType_;
    }
    return org.apache.spark.connect.proto.WriteStreamOperationStartResult.getDefaultInstance();
  }
  /**
   * <pre>
   * Response for a streaming query.
   * </pre>
   *
   * <code>.spark.connect.WriteStreamOperationStartResult write_stream_operation_start_result = 8;</code>
   */
  @Override
  public org.apache.spark.connect.proto.WriteStreamOperationStartResultOrBuilder getWriteStreamOperationStartResultOrBuilder() {
    if (responseTypeCase_ == 8) {
       return (org.apache.spark.connect.proto.WriteStreamOperationStartResult) responseType_;
    }
    return org.apache.spark.connect.proto.WriteStreamOperationStartResult.getDefaultInstance();
  }

  public static final int STREAMING_QUERY_COMMAND_RESULT_FIELD_NUMBER = 9;
  /**
   * <pre>
   * Response for commands on a streaming query.
   * </pre>
   *
   * <code>.spark.connect.StreamingQueryCommandResult streaming_query_command_result = 9;</code>
   * @return Whether the streamingQueryCommandResult field is set.
   */
  @Override
  public boolean hasStreamingQueryCommandResult() {
    return responseTypeCase_ == 9;
  }
  /**
   * <pre>
   * Response for commands on a streaming query.
   * </pre>
   *
   * <code>.spark.connect.StreamingQueryCommandResult streaming_query_command_result = 9;</code>
   * @return The streamingQueryCommandResult.
   */
  @Override
  public org.apache.spark.connect.proto.StreamingQueryCommandResult getStreamingQueryCommandResult() {
    if (responseTypeCase_ == 9) {
       return (org.apache.spark.connect.proto.StreamingQueryCommandResult) responseType_;
    }
    return org.apache.spark.connect.proto.StreamingQueryCommandResult.getDefaultInstance();
  }
  /**
   * <pre>
   * Response for commands on a streaming query.
   * </pre>
   *
   * <code>.spark.connect.StreamingQueryCommandResult streaming_query_command_result = 9;</code>
   */
  @Override
  public org.apache.spark.connect.proto.StreamingQueryCommandResultOrBuilder getStreamingQueryCommandResultOrBuilder() {
    if (responseTypeCase_ == 9) {
       return (org.apache.spark.connect.proto.StreamingQueryCommandResult) responseType_;
    }
    return org.apache.spark.connect.proto.StreamingQueryCommandResult.getDefaultInstance();
  }

  public static final int GET_RESOURCES_COMMAND_RESULT_FIELD_NUMBER = 10;
  /**
   * <pre>
   * Response for 'SparkContext.resources'.
   * </pre>
   *
   * <code>.spark.connect.GetResourcesCommandResult get_resources_command_result = 10;</code>
   * @return Whether the getResourcesCommandResult field is set.
   */
  @Override
  public boolean hasGetResourcesCommandResult() {
    return responseTypeCase_ == 10;
  }
  /**
   * <pre>
   * Response for 'SparkContext.resources'.
   * </pre>
   *
   * <code>.spark.connect.GetResourcesCommandResult get_resources_command_result = 10;</code>
   * @return The getResourcesCommandResult.
   */
  @Override
  public org.apache.spark.connect.proto.GetResourcesCommandResult getGetResourcesCommandResult() {
    if (responseTypeCase_ == 10) {
       return (org.apache.spark.connect.proto.GetResourcesCommandResult) responseType_;
    }
    return org.apache.spark.connect.proto.GetResourcesCommandResult.getDefaultInstance();
  }
  /**
   * <pre>
   * Response for 'SparkContext.resources'.
   * </pre>
   *
   * <code>.spark.connect.GetResourcesCommandResult get_resources_command_result = 10;</code>
   */
  @Override
  public org.apache.spark.connect.proto.GetResourcesCommandResultOrBuilder getGetResourcesCommandResultOrBuilder() {
    if (responseTypeCase_ == 10) {
       return (org.apache.spark.connect.proto.GetResourcesCommandResult) responseType_;
    }
    return org.apache.spark.connect.proto.GetResourcesCommandResult.getDefaultInstance();
  }

  public static final int STREAMING_QUERY_MANAGER_COMMAND_RESULT_FIELD_NUMBER = 11;
  /**
   * <pre>
   * Response for commands on the streaming query manager.
   * </pre>
   *
   * <code>.spark.connect.StreamingQueryManagerCommandResult streaming_query_manager_command_result = 11;</code>
   * @return Whether the streamingQueryManagerCommandResult field is set.
   */
  @Override
  public boolean hasStreamingQueryManagerCommandResult() {
    return responseTypeCase_ == 11;
  }
  /**
   * <pre>
   * Response for commands on the streaming query manager.
   * </pre>
   *
   * <code>.spark.connect.StreamingQueryManagerCommandResult streaming_query_manager_command_result = 11;</code>
   * @return The streamingQueryManagerCommandResult.
   */
  @Override
  public org.apache.spark.connect.proto.StreamingQueryManagerCommandResult getStreamingQueryManagerCommandResult() {
    if (responseTypeCase_ == 11) {
       return (org.apache.spark.connect.proto.StreamingQueryManagerCommandResult) responseType_;
    }
    return org.apache.spark.connect.proto.StreamingQueryManagerCommandResult.getDefaultInstance();
  }
  /**
   * <pre>
   * Response for commands on the streaming query manager.
   * </pre>
   *
   * <code>.spark.connect.StreamingQueryManagerCommandResult streaming_query_manager_command_result = 11;</code>
   */
  @Override
  public org.apache.spark.connect.proto.StreamingQueryManagerCommandResultOrBuilder getStreamingQueryManagerCommandResultOrBuilder() {
    if (responseTypeCase_ == 11) {
       return (org.apache.spark.connect.proto.StreamingQueryManagerCommandResult) responseType_;
    }
    return org.apache.spark.connect.proto.StreamingQueryManagerCommandResult.getDefaultInstance();
  }

  public static final int RESULT_COMPLETE_FIELD_NUMBER = 14;
  /**
   * <pre>
   * Response type informing if the stream is complete in reattachable execution.
   * </pre>
   *
   * <code>.spark.connect.ExecutePlanResponse.ResultComplete result_complete = 14;</code>
   * @return Whether the resultComplete field is set.
   */
  @Override
  public boolean hasResultComplete() {
    return responseTypeCase_ == 14;
  }
  /**
   * <pre>
   * Response type informing if the stream is complete in reattachable execution.
   * </pre>
   *
   * <code>.spark.connect.ExecutePlanResponse.ResultComplete result_complete = 14;</code>
   * @return The resultComplete.
   */
  @Override
  public org.apache.spark.connect.proto.ExecutePlanResponse.ResultComplete getResultComplete() {
    if (responseTypeCase_ == 14) {
       return (org.apache.spark.connect.proto.ExecutePlanResponse.ResultComplete) responseType_;
    }
    return org.apache.spark.connect.proto.ExecutePlanResponse.ResultComplete.getDefaultInstance();
  }
  /**
   * <pre>
   * Response type informing if the stream is complete in reattachable execution.
   * </pre>
   *
   * <code>.spark.connect.ExecutePlanResponse.ResultComplete result_complete = 14;</code>
   */
  @Override
  public org.apache.spark.connect.proto.ExecutePlanResponse.ResultCompleteOrBuilder getResultCompleteOrBuilder() {
    if (responseTypeCase_ == 14) {
       return (org.apache.spark.connect.proto.ExecutePlanResponse.ResultComplete) responseType_;
    }
    return org.apache.spark.connect.proto.ExecutePlanResponse.ResultComplete.getDefaultInstance();
  }

  public static final int EXTENSION_FIELD_NUMBER = 999;
  /**
   * <pre>
   * Support arbitrary result objects.
   * </pre>
   *
   * <code>.google.protobuf.Any extension = 999;</code>
   * @return Whether the extension field is set.
   */
  @Override
  public boolean hasExtension() {
    return responseTypeCase_ == 999;
  }
  /**
   * <pre>
   * Support arbitrary result objects.
   * </pre>
   *
   * <code>.google.protobuf.Any extension = 999;</code>
   * @return The extension.
   */
  @Override
  public com.google.protobuf.Any getExtension() {
    if (responseTypeCase_ == 999) {
       return (com.google.protobuf.Any) responseType_;
    }
    return com.google.protobuf.Any.getDefaultInstance();
  }
  /**
   * <pre>
   * Support arbitrary result objects.
   * </pre>
   *
   * <code>.google.protobuf.Any extension = 999;</code>
   */
  @Override
  public com.google.protobuf.AnyOrBuilder getExtensionOrBuilder() {
    if (responseTypeCase_ == 999) {
       return (com.google.protobuf.Any) responseType_;
    }
    return com.google.protobuf.Any.getDefaultInstance();
  }

  public static final int METRICS_FIELD_NUMBER = 4;
  private org.apache.spark.connect.proto.ExecutePlanResponse.Metrics metrics_;
  /**
   * <pre>
   * Metrics for the query execution. Typically, this field is only present in the last
   * batch of results and then represent the overall state of the query execution.
   * </pre>
   *
   * <code>.spark.connect.ExecutePlanResponse.Metrics metrics = 4;</code>
   * @return Whether the metrics field is set.
   */
  @Override
  public boolean hasMetrics() {
    return metrics_ != null;
  }
  /**
   * <pre>
   * Metrics for the query execution. Typically, this field is only present in the last
   * batch of results and then represent the overall state of the query execution.
   * </pre>
   *
   * <code>.spark.connect.ExecutePlanResponse.Metrics metrics = 4;</code>
   * @return The metrics.
   */
  @Override
  public org.apache.spark.connect.proto.ExecutePlanResponse.Metrics getMetrics() {
    return metrics_ == null ? org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.getDefaultInstance() : metrics_;
  }
  /**
   * <pre>
   * Metrics for the query execution. Typically, this field is only present in the last
   * batch of results and then represent the overall state of the query execution.
   * </pre>
   *
   * <code>.spark.connect.ExecutePlanResponse.Metrics metrics = 4;</code>
   */
  @Override
  public org.apache.spark.connect.proto.ExecutePlanResponse.MetricsOrBuilder getMetricsOrBuilder() {
    return metrics_ == null ? org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.getDefaultInstance() : metrics_;
  }

  public static final int OBSERVED_METRICS_FIELD_NUMBER = 6;
  @SuppressWarnings("serial")
  private java.util.List<org.apache.spark.connect.proto.ExecutePlanResponse.ObservedMetrics> observedMetrics_;
  /**
   * <pre>
   * The metrics observed during the execution of the query plan.
   * </pre>
   *
   * <code>repeated .spark.connect.ExecutePlanResponse.ObservedMetrics observed_metrics = 6;</code>
   */
  @Override
  public java.util.List<org.apache.spark.connect.proto.ExecutePlanResponse.ObservedMetrics> getObservedMetricsList() {
    return observedMetrics_;
  }
  /**
   * <pre>
   * The metrics observed during the execution of the query plan.
   * </pre>
   *
   * <code>repeated .spark.connect.ExecutePlanResponse.ObservedMetrics observed_metrics = 6;</code>
   */
  @Override
  public java.util.List<? extends org.apache.spark.connect.proto.ExecutePlanResponse.ObservedMetricsOrBuilder> 
      getObservedMetricsOrBuilderList() {
    return observedMetrics_;
  }
  /**
   * <pre>
   * The metrics observed during the execution of the query plan.
   * </pre>
   *
   * <code>repeated .spark.connect.ExecutePlanResponse.ObservedMetrics observed_metrics = 6;</code>
   */
  @Override
  public int getObservedMetricsCount() {
    return observedMetrics_.size();
  }
  /**
   * <pre>
   * The metrics observed during the execution of the query plan.
   * </pre>
   *
   * <code>repeated .spark.connect.ExecutePlanResponse.ObservedMetrics observed_metrics = 6;</code>
   */
  @Override
  public org.apache.spark.connect.proto.ExecutePlanResponse.ObservedMetrics getObservedMetrics(int index) {
    return observedMetrics_.get(index);
  }
  /**
   * <pre>
   * The metrics observed during the execution of the query plan.
   * </pre>
   *
   * <code>repeated .spark.connect.ExecutePlanResponse.ObservedMetrics observed_metrics = 6;</code>
   */
  @Override
  public org.apache.spark.connect.proto.ExecutePlanResponse.ObservedMetricsOrBuilder getObservedMetricsOrBuilder(
      int index) {
    return observedMetrics_.get(index);
  }

  public static final int SCHEMA_FIELD_NUMBER = 7;
  private org.apache.spark.connect.proto.DataType schema_;
  /**
   * <pre>
   * (Optional) The Spark schema. This field is available when `collect` is called.
   * </pre>
   *
   * <code>.spark.connect.DataType schema = 7;</code>
   * @return Whether the schema field is set.
   */
  @Override
  public boolean hasSchema() {
    return schema_ != null;
  }
  /**
   * <pre>
   * (Optional) The Spark schema. This field is available when `collect` is called.
   * </pre>
   *
   * <code>.spark.connect.DataType schema = 7;</code>
   * @return The schema.
   */
  @Override
  public org.apache.spark.connect.proto.DataType getSchema() {
    return schema_ == null ? org.apache.spark.connect.proto.DataType.getDefaultInstance() : schema_;
  }
  /**
   * <pre>
   * (Optional) The Spark schema. This field is available when `collect` is called.
   * </pre>
   *
   * <code>.spark.connect.DataType schema = 7;</code>
   */
  @Override
  public org.apache.spark.connect.proto.DataTypeOrBuilder getSchemaOrBuilder() {
    return schema_ == null ? org.apache.spark.connect.proto.DataType.getDefaultInstance() : schema_;
  }

  private byte memoizedIsInitialized = -1;
  @Override
  public final boolean isInitialized() {
    byte isInitialized = memoizedIsInitialized;
    if (isInitialized == 1) return true;
    if (isInitialized == 0) return false;

    memoizedIsInitialized = 1;
    return true;
  }

  @Override
  public void writeTo(com.google.protobuf.CodedOutputStream output)
                      throws java.io.IOException {
    if (!com.google.protobuf.GeneratedMessageV3.isStringEmpty(sessionId_)) {
      com.google.protobuf.GeneratedMessageV3.writeString(output, 1, sessionId_);
    }
    if (responseTypeCase_ == 2) {
      output.writeMessage(2, (org.apache.spark.connect.proto.ExecutePlanResponse.ArrowBatch) responseType_);
    }
    if (metrics_ != null) {
      output.writeMessage(4, getMetrics());
    }
    if (responseTypeCase_ == 5) {
      output.writeMessage(5, (org.apache.spark.connect.proto.ExecutePlanResponse.SqlCommandResult) responseType_);
    }
    for (int i = 0; i < observedMetrics_.size(); i++) {
      output.writeMessage(6, observedMetrics_.get(i));
    }
    if (schema_ != null) {
      output.writeMessage(7, getSchema());
    }
    if (responseTypeCase_ == 8) {
      output.writeMessage(8, (org.apache.spark.connect.proto.WriteStreamOperationStartResult) responseType_);
    }
    if (responseTypeCase_ == 9) {
      output.writeMessage(9, (org.apache.spark.connect.proto.StreamingQueryCommandResult) responseType_);
    }
    if (responseTypeCase_ == 10) {
      output.writeMessage(10, (org.apache.spark.connect.proto.GetResourcesCommandResult) responseType_);
    }
    if (responseTypeCase_ == 11) {
      output.writeMessage(11, (org.apache.spark.connect.proto.StreamingQueryManagerCommandResult) responseType_);
    }
    if (!com.google.protobuf.GeneratedMessageV3.isStringEmpty(operationId_)) {
      com.google.protobuf.GeneratedMessageV3.writeString(output, 12, operationId_);
    }
    if (!com.google.protobuf.GeneratedMessageV3.isStringEmpty(responseId_)) {
      com.google.protobuf.GeneratedMessageV3.writeString(output, 13, responseId_);
    }
    if (responseTypeCase_ == 14) {
      output.writeMessage(14, (org.apache.spark.connect.proto.ExecutePlanResponse.ResultComplete) responseType_);
    }
    if (responseTypeCase_ == 999) {
      output.writeMessage(999, (com.google.protobuf.Any) responseType_);
    }
    getUnknownFields().writeTo(output);
  }

  @Override
  public int getSerializedSize() {
    int size = memoizedSize;
    if (size != -1) return size;

    size = 0;
    if (!com.google.protobuf.GeneratedMessageV3.isStringEmpty(sessionId_)) {
      size += com.google.protobuf.GeneratedMessageV3.computeStringSize(1, sessionId_);
    }
    if (responseTypeCase_ == 2) {
      size += com.google.protobuf.CodedOutputStream
        .computeMessageSize(2, (org.apache.spark.connect.proto.ExecutePlanResponse.ArrowBatch) responseType_);
    }
    if (metrics_ != null) {
      size += com.google.protobuf.CodedOutputStream
        .computeMessageSize(4, getMetrics());
    }
    if (responseTypeCase_ == 5) {
      size += com.google.protobuf.CodedOutputStream
        .computeMessageSize(5, (org.apache.spark.connect.proto.ExecutePlanResponse.SqlCommandResult) responseType_);
    }
    for (int i = 0; i < observedMetrics_.size(); i++) {
      size += com.google.protobuf.CodedOutputStream
        .computeMessageSize(6, observedMetrics_.get(i));
    }
    if (schema_ != null) {
      size += com.google.protobuf.CodedOutputStream
        .computeMessageSize(7, getSchema());
    }
    if (responseTypeCase_ == 8) {
      size += com.google.protobuf.CodedOutputStream
        .computeMessageSize(8, (org.apache.spark.connect.proto.WriteStreamOperationStartResult) responseType_);
    }
    if (responseTypeCase_ == 9) {
      size += com.google.protobuf.CodedOutputStream
        .computeMessageSize(9, (org.apache.spark.connect.proto.StreamingQueryCommandResult) responseType_);
    }
    if (responseTypeCase_ == 10) {
      size += com.google.protobuf.CodedOutputStream
        .computeMessageSize(10, (org.apache.spark.connect.proto.GetResourcesCommandResult) responseType_);
    }
    if (responseTypeCase_ == 11) {
      size += com.google.protobuf.CodedOutputStream
        .computeMessageSize(11, (org.apache.spark.connect.proto.StreamingQueryManagerCommandResult) responseType_);
    }
    if (!com.google.protobuf.GeneratedMessageV3.isStringEmpty(operationId_)) {
      size += com.google.protobuf.GeneratedMessageV3.computeStringSize(12, operationId_);
    }
    if (!com.google.protobuf.GeneratedMessageV3.isStringEmpty(responseId_)) {
      size += com.google.protobuf.GeneratedMessageV3.computeStringSize(13, responseId_);
    }
    if (responseTypeCase_ == 14) {
      size += com.google.protobuf.CodedOutputStream
        .computeMessageSize(14, (org.apache.spark.connect.proto.ExecutePlanResponse.ResultComplete) responseType_);
    }
    if (responseTypeCase_ == 999) {
      size += com.google.protobuf.CodedOutputStream
        .computeMessageSize(999, (com.google.protobuf.Any) responseType_);
    }
    size += getUnknownFields().getSerializedSize();
    memoizedSize = size;
    return size;
  }

  @Override
  public boolean equals(final Object obj) {
    if (obj == this) {
     return true;
    }
    if (!(obj instanceof org.apache.spark.connect.proto.ExecutePlanResponse)) {
      return super.equals(obj);
    }
    org.apache.spark.connect.proto.ExecutePlanResponse other = (org.apache.spark.connect.proto.ExecutePlanResponse) obj;

    if (!getSessionId()
        .equals(other.getSessionId())) return false;
    if (!getOperationId()
        .equals(other.getOperationId())) return false;
    if (!getResponseId()
        .equals(other.getResponseId())) return false;
    if (hasMetrics() != other.hasMetrics()) return false;
    if (hasMetrics()) {
      if (!getMetrics()
          .equals(other.getMetrics())) return false;
    }
    if (!getObservedMetricsList()
        .equals(other.getObservedMetricsList())) return false;
    if (hasSchema() != other.hasSchema()) return false;
    if (hasSchema()) {
      if (!getSchema()
          .equals(other.getSchema())) return false;
    }
    if (!getResponseTypeCase().equals(other.getResponseTypeCase())) return false;
    switch (responseTypeCase_) {
      case 2:
        if (!getArrowBatch()
            .equals(other.getArrowBatch())) return false;
        break;
      case 5:
        if (!getSqlCommandResult()
            .equals(other.getSqlCommandResult())) return false;
        break;
      case 8:
        if (!getWriteStreamOperationStartResult()
            .equals(other.getWriteStreamOperationStartResult())) return false;
        break;
      case 9:
        if (!getStreamingQueryCommandResult()
            .equals(other.getStreamingQueryCommandResult())) return false;
        break;
      case 10:
        if (!getGetResourcesCommandResult()
            .equals(other.getGetResourcesCommandResult())) return false;
        break;
      case 11:
        if (!getStreamingQueryManagerCommandResult()
            .equals(other.getStreamingQueryManagerCommandResult())) return false;
        break;
      case 14:
        if (!getResultComplete()
            .equals(other.getResultComplete())) return false;
        break;
      case 999:
        if (!getExtension()
            .equals(other.getExtension())) return false;
        break;
      case 0:
      default:
    }
    if (!getUnknownFields().equals(other.getUnknownFields())) return false;
    return true;
  }

  @Override
  public int hashCode() {
    if (memoizedHashCode != 0) {
      return memoizedHashCode;
    }
    int hash = 41;
    hash = (19 * hash) + getDescriptor().hashCode();
    hash = (37 * hash) + SESSION_ID_FIELD_NUMBER;
    hash = (53 * hash) + getSessionId().hashCode();
    hash = (37 * hash) + OPERATION_ID_FIELD_NUMBER;
    hash = (53 * hash) + getOperationId().hashCode();
    hash = (37 * hash) + RESPONSE_ID_FIELD_NUMBER;
    hash = (53 * hash) + getResponseId().hashCode();
    if (hasMetrics()) {
      hash = (37 * hash) + METRICS_FIELD_NUMBER;
      hash = (53 * hash) + getMetrics().hashCode();
    }
    if (getObservedMetricsCount() > 0) {
      hash = (37 * hash) + OBSERVED_METRICS_FIELD_NUMBER;
      hash = (53 * hash) + getObservedMetricsList().hashCode();
    }
    if (hasSchema()) {
      hash = (37 * hash) + SCHEMA_FIELD_NUMBER;
      hash = (53 * hash) + getSchema().hashCode();
    }
    switch (responseTypeCase_) {
      case 2:
        hash = (37 * hash) + ARROW_BATCH_FIELD_NUMBER;
        hash = (53 * hash) + getArrowBatch().hashCode();
        break;
      case 5:
        hash = (37 * hash) + SQL_COMMAND_RESULT_FIELD_NUMBER;
        hash = (53 * hash) + getSqlCommandResult().hashCode();
        break;
      case 8:
        hash = (37 * hash) + WRITE_STREAM_OPERATION_START_RESULT_FIELD_NUMBER;
        hash = (53 * hash) + getWriteStreamOperationStartResult().hashCode();
        break;
      case 9:
        hash = (37 * hash) + STREAMING_QUERY_COMMAND_RESULT_FIELD_NUMBER;
        hash = (53 * hash) + getStreamingQueryCommandResult().hashCode();
        break;
      case 10:
        hash = (37 * hash) + GET_RESOURCES_COMMAND_RESULT_FIELD_NUMBER;
        hash = (53 * hash) + getGetResourcesCommandResult().hashCode();
        break;
      case 11:
        hash = (37 * hash) + STREAMING_QUERY_MANAGER_COMMAND_RESULT_FIELD_NUMBER;
        hash = (53 * hash) + getStreamingQueryManagerCommandResult().hashCode();
        break;
      case 14:
        hash = (37 * hash) + RESULT_COMPLETE_FIELD_NUMBER;
        hash = (53 * hash) + getResultComplete().hashCode();
        break;
      case 999:
        hash = (37 * hash) + EXTENSION_FIELD_NUMBER;
        hash = (53 * hash) + getExtension().hashCode();
        break;
      case 0:
      default:
    }
    hash = (29 * hash) + getUnknownFields().hashCode();
    memoizedHashCode = hash;
    return hash;
  }

  public static org.apache.spark.connect.proto.ExecutePlanResponse parseFrom(
      java.nio.ByteBuffer data)
      throws com.google.protobuf.InvalidProtocolBufferException {
    return PARSER.parseFrom(data);
  }
  public static org.apache.spark.connect.proto.ExecutePlanResponse parseFrom(
      java.nio.ByteBuffer data,
      com.google.protobuf.ExtensionRegistryLite extensionRegistry)
      throws com.google.protobuf.InvalidProtocolBufferException {
    return PARSER.parseFrom(data, extensionRegistry);
  }
  public static org.apache.spark.connect.proto.ExecutePlanResponse parseFrom(
      com.google.protobuf.ByteString data)
      throws com.google.protobuf.InvalidProtocolBufferException {
    return PARSER.parseFrom(data);
  }
  public static org.apache.spark.connect.proto.ExecutePlanResponse parseFrom(
      com.google.protobuf.ByteString data,
      com.google.protobuf.ExtensionRegistryLite extensionRegistry)
      throws com.google.protobuf.InvalidProtocolBufferException {
    return PARSER.parseFrom(data, extensionRegistry);
  }
  public static org.apache.spark.connect.proto.ExecutePlanResponse parseFrom(byte[] data)
      throws com.google.protobuf.InvalidProtocolBufferException {
    return PARSER.parseFrom(data);
  }
  public static org.apache.spark.connect.proto.ExecutePlanResponse parseFrom(
      byte[] data,
      com.google.protobuf.ExtensionRegistryLite extensionRegistry)
      throws com.google.protobuf.InvalidProtocolBufferException {
    return PARSER.parseFrom(data, extensionRegistry);
  }
  public static org.apache.spark.connect.proto.ExecutePlanResponse parseFrom(java.io.InputStream input)
      throws java.io.IOException {
    return com.google.protobuf.GeneratedMessageV3
        .parseWithIOException(PARSER, input);
  }
  public static org.apache.spark.connect.proto.ExecutePlanResponse parseFrom(
      java.io.InputStream input,
      com.google.protobuf.ExtensionRegistryLite extensionRegistry)
      throws java.io.IOException {
    return com.google.protobuf.GeneratedMessageV3
        .parseWithIOException(PARSER, input, extensionRegistry);
  }

  public static org.apache.spark.connect.proto.ExecutePlanResponse parseDelimitedFrom(java.io.InputStream input)
      throws java.io.IOException {
    return com.google.protobuf.GeneratedMessageV3
        .parseDelimitedWithIOException(PARSER, input);
  }

  public static org.apache.spark.connect.proto.ExecutePlanResponse parseDelimitedFrom(
      java.io.InputStream input,
      com.google.protobuf.ExtensionRegistryLite extensionRegistry)
      throws java.io.IOException {
    return com.google.protobuf.GeneratedMessageV3
        .parseDelimitedWithIOException(PARSER, input, extensionRegistry);
  }
  public static org.apache.spark.connect.proto.ExecutePlanResponse parseFrom(
      com.google.protobuf.CodedInputStream input)
      throws java.io.IOException {
    return com.google.protobuf.GeneratedMessageV3
        .parseWithIOException(PARSER, input);
  }
  public static org.apache.spark.connect.proto.ExecutePlanResponse parseFrom(
      com.google.protobuf.CodedInputStream input,
      com.google.protobuf.ExtensionRegistryLite extensionRegistry)
      throws java.io.IOException {
    return com.google.protobuf.GeneratedMessageV3
        .parseWithIOException(PARSER, input, extensionRegistry);
  }

  @Override
  public Builder newBuilderForType() { return newBuilder(); }
  public static Builder newBuilder() {
    return DEFAULT_INSTANCE.toBuilder();
  }
  public static Builder newBuilder(org.apache.spark.connect.proto.ExecutePlanResponse prototype) {
    return DEFAULT_INSTANCE.toBuilder().mergeFrom(prototype);
  }
  @Override
  public Builder toBuilder() {
    return this == DEFAULT_INSTANCE
        ? new Builder() : new Builder().mergeFrom(this);
  }

  @Override
  protected Builder newBuilderForType(
      BuilderParent parent) {
    Builder builder = new Builder(parent);
    return builder;
  }
  /**
   * <pre>
   * The response of a query, can be one or more for each request. Responses belonging to the
   * same input query, carry the same `session_id`.
   * </pre>
   *
   * Protobuf type {@code spark.connect.ExecutePlanResponse}
   */
  public static final class Builder extends
      com.google.protobuf.GeneratedMessageV3.Builder<Builder> implements
      // @@protoc_insertion_point(builder_implements:spark.connect.ExecutePlanResponse)
      org.apache.spark.connect.proto.ExecutePlanResponseOrBuilder {
    public static final com.google.protobuf.Descriptors.Descriptor
        getDescriptor() {
      return org.apache.spark.connect.proto.Base.internal_static_spark_connect_ExecutePlanResponse_descriptor;
    }

    @Override
    protected FieldAccessorTable
        internalGetFieldAccessorTable() {
      return org.apache.spark.connect.proto.Base.internal_static_spark_connect_ExecutePlanResponse_fieldAccessorTable
          .ensureFieldAccessorsInitialized(
              org.apache.spark.connect.proto.ExecutePlanResponse.class, org.apache.spark.connect.proto.ExecutePlanResponse.Builder.class);
    }

    // Construct using org.apache.spark.connect.proto.ExecutePlanResponse.newBuilder()
    private Builder() {

    }

    private Builder(
        BuilderParent parent) {
      super(parent);

    }
    @Override
    public Builder clear() {
      super.clear();
      bitField0_ = 0;
      sessionId_ = "";
      operationId_ = "";
      responseId_ = "";
      if (arrowBatchBuilder_ != null) {
        arrowBatchBuilder_.clear();
      }
      if (sqlCommandResultBuilder_ != null) {
        sqlCommandResultBuilder_.clear();
      }
      if (writeStreamOperationStartResultBuilder_ != null) {
        writeStreamOperationStartResultBuilder_.clear();
      }
      if (streamingQueryCommandResultBuilder_ != null) {
        streamingQueryCommandResultBuilder_.clear();
      }
      if (getResourcesCommandResultBuilder_ != null) {
        getResourcesCommandResultBuilder_.clear();
      }
      if (streamingQueryManagerCommandResultBuilder_ != null) {
        streamingQueryManagerCommandResultBuilder_.clear();
      }
      if (resultCompleteBuilder_ != null) {
        resultCompleteBuilder_.clear();
      }
      if (extensionBuilder_ != null) {
        extensionBuilder_.clear();
      }
      metrics_ = null;
      if (metricsBuilder_ != null) {
        metricsBuilder_.dispose();
        metricsBuilder_ = null;
      }
      if (observedMetricsBuilder_ == null) {
        observedMetrics_ = java.util.Collections.emptyList();
      } else {
        observedMetrics_ = null;
        observedMetricsBuilder_.clear();
      }
      bitField0_ = (bitField0_ & ~0x00001000);
      schema_ = null;
      if (schemaBuilder_ != null) {
        schemaBuilder_.dispose();
        schemaBuilder_ = null;
      }
      responseTypeCase_ = 0;
      responseType_ = null;
      return this;
    }

    @Override
    public com.google.protobuf.Descriptors.Descriptor
        getDescriptorForType() {
      return org.apache.spark.connect.proto.Base.internal_static_spark_connect_ExecutePlanResponse_descriptor;
    }

    @Override
    public org.apache.spark.connect.proto.ExecutePlanResponse getDefaultInstanceForType() {
      return org.apache.spark.connect.proto.ExecutePlanResponse.getDefaultInstance();
    }

    @Override
    public org.apache.spark.connect.proto.ExecutePlanResponse build() {
      org.apache.spark.connect.proto.ExecutePlanResponse result = buildPartial();
      if (!result.isInitialized()) {
        throw newUninitializedMessageException(result);
      }
      return result;
    }

    @Override
    public org.apache.spark.connect.proto.ExecutePlanResponse buildPartial() {
      org.apache.spark.connect.proto.ExecutePlanResponse result = new org.apache.spark.connect.proto.ExecutePlanResponse(this);
      buildPartialRepeatedFields(result);
      if (bitField0_ != 0) { buildPartial0(result); }
      buildPartialOneofs(result);
      onBuilt();
      return result;
    }

    private void buildPartialRepeatedFields(org.apache.spark.connect.proto.ExecutePlanResponse result) {
      if (observedMetricsBuilder_ == null) {
        if (((bitField0_ & 0x00001000) != 0)) {
          observedMetrics_ = java.util.Collections.unmodifiableList(observedMetrics_);
          bitField0_ = (bitField0_ & ~0x00001000);
        }
        result.observedMetrics_ = observedMetrics_;
      } else {
        result.observedMetrics_ = observedMetricsBuilder_.build();
      }
    }

    private void buildPartial0(org.apache.spark.connect.proto.ExecutePlanResponse result) {
      int from_bitField0_ = bitField0_;
      if (((from_bitField0_ & 0x00000001) != 0)) {
        result.sessionId_ = sessionId_;
      }
      if (((from_bitField0_ & 0x00000002) != 0)) {
        result.operationId_ = operationId_;
      }
      if (((from_bitField0_ & 0x00000004) != 0)) {
        result.responseId_ = responseId_;
      }
      if (((from_bitField0_ & 0x00000800) != 0)) {
        result.metrics_ = metricsBuilder_ == null
            ? metrics_
            : metricsBuilder_.build();
      }
      if (((from_bitField0_ & 0x00002000) != 0)) {
        result.schema_ = schemaBuilder_ == null
            ? schema_
            : schemaBuilder_.build();
      }
    }

    private void buildPartialOneofs(org.apache.spark.connect.proto.ExecutePlanResponse result) {
      result.responseTypeCase_ = responseTypeCase_;
      result.responseType_ = this.responseType_;
      if (responseTypeCase_ == 2 &&
          arrowBatchBuilder_ != null) {
        result.responseType_ = arrowBatchBuilder_.build();
      }
      if (responseTypeCase_ == 5 &&
          sqlCommandResultBuilder_ != null) {
        result.responseType_ = sqlCommandResultBuilder_.build();
      }
      if (responseTypeCase_ == 8 &&
          writeStreamOperationStartResultBuilder_ != null) {
        result.responseType_ = writeStreamOperationStartResultBuilder_.build();
      }
      if (responseTypeCase_ == 9 &&
          streamingQueryCommandResultBuilder_ != null) {
        result.responseType_ = streamingQueryCommandResultBuilder_.build();
      }
      if (responseTypeCase_ == 10 &&
          getResourcesCommandResultBuilder_ != null) {
        result.responseType_ = getResourcesCommandResultBuilder_.build();
      }
      if (responseTypeCase_ == 11 &&
          streamingQueryManagerCommandResultBuilder_ != null) {
        result.responseType_ = streamingQueryManagerCommandResultBuilder_.build();
      }
      if (responseTypeCase_ == 14 &&
          resultCompleteBuilder_ != null) {
        result.responseType_ = resultCompleteBuilder_.build();
      }
      if (responseTypeCase_ == 999 &&
          extensionBuilder_ != null) {
        result.responseType_ = extensionBuilder_.build();
      }
    }

    @Override
    public Builder clone() {
      return super.clone();
    }
    @Override
    public Builder setField(
        com.google.protobuf.Descriptors.FieldDescriptor field,
        Object value) {
      return super.setField(field, value);
    }
    @Override
    public Builder clearField(
        com.google.protobuf.Descriptors.FieldDescriptor field) {
      return super.clearField(field);
    }
    @Override
    public Builder clearOneof(
        com.google.protobuf.Descriptors.OneofDescriptor oneof) {
      return super.clearOneof(oneof);
    }
    @Override
    public Builder setRepeatedField(
        com.google.protobuf.Descriptors.FieldDescriptor field,
        int index, Object value) {
      return super.setRepeatedField(field, index, value);
    }
    @Override
    public Builder addRepeatedField(
        com.google.protobuf.Descriptors.FieldDescriptor field,
        Object value) {
      return super.addRepeatedField(field, value);
    }
    @Override
    public Builder mergeFrom(com.google.protobuf.Message other) {
      if (other instanceof org.apache.spark.connect.proto.ExecutePlanResponse) {
        return mergeFrom((org.apache.spark.connect.proto.ExecutePlanResponse)other);
      } else {
        super.mergeFrom(other);
        return this;
      }
    }

    public Builder mergeFrom(org.apache.spark.connect.proto.ExecutePlanResponse other) {
      if (other == org.apache.spark.connect.proto.ExecutePlanResponse.getDefaultInstance()) return this;
      if (!other.getSessionId().isEmpty()) {
        sessionId_ = other.sessionId_;
        bitField0_ |= 0x00000001;
        onChanged();
      }
      if (!other.getOperationId().isEmpty()) {
        operationId_ = other.operationId_;
        bitField0_ |= 0x00000002;
        onChanged();
      }
      if (!other.getResponseId().isEmpty()) {
        responseId_ = other.responseId_;
        bitField0_ |= 0x00000004;
        onChanged();
      }
      if (other.hasMetrics()) {
        mergeMetrics(other.getMetrics());
      }
      if (observedMetricsBuilder_ == null) {
        if (!other.observedMetrics_.isEmpty()) {
          if (observedMetrics_.isEmpty()) {
            observedMetrics_ = other.observedMetrics_;
            bitField0_ = (bitField0_ & ~0x00001000);
          } else {
            ensureObservedMetricsIsMutable();
            observedMetrics_.addAll(other.observedMetrics_);
          }
          onChanged();
        }
      } else {
        if (!other.observedMetrics_.isEmpty()) {
          if (observedMetricsBuilder_.isEmpty()) {
            observedMetricsBuilder_.dispose();
            observedMetricsBuilder_ = null;
            observedMetrics_ = other.observedMetrics_;
            bitField0_ = (bitField0_ & ~0x00001000);
            observedMetricsBuilder_ = 
              com.google.protobuf.GeneratedMessageV3.alwaysUseFieldBuilders ?
                 getObservedMetricsFieldBuilder() : null;
          } else {
            observedMetricsBuilder_.addAllMessages(other.observedMetrics_);
          }
        }
      }
      if (other.hasSchema()) {
        mergeSchema(other.getSchema());
      }
      switch (other.getResponseTypeCase()) {
        case ARROW_BATCH: {
          mergeArrowBatch(other.getArrowBatch());
          break;
        }
        case SQL_COMMAND_RESULT: {
          mergeSqlCommandResult(other.getSqlCommandResult());
          break;
        }
        case WRITE_STREAM_OPERATION_START_RESULT: {
          mergeWriteStreamOperationStartResult(other.getWriteStreamOperationStartResult());
          break;
        }
        case STREAMING_QUERY_COMMAND_RESULT: {
          mergeStreamingQueryCommandResult(other.getStreamingQueryCommandResult());
          break;
        }
        case GET_RESOURCES_COMMAND_RESULT: {
          mergeGetResourcesCommandResult(other.getGetResourcesCommandResult());
          break;
        }
        case STREAMING_QUERY_MANAGER_COMMAND_RESULT: {
          mergeStreamingQueryManagerCommandResult(other.getStreamingQueryManagerCommandResult());
          break;
        }
        case RESULT_COMPLETE: {
          mergeResultComplete(other.getResultComplete());
          break;
        }
        case EXTENSION: {
          mergeExtension(other.getExtension());
          break;
        }
        case RESPONSETYPE_NOT_SET: {
          break;
        }
      }
      this.mergeUnknownFields(other.getUnknownFields());
      onChanged();
      return this;
    }

    @Override
    public final boolean isInitialized() {
      return true;
    }

    @Override
    public Builder mergeFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws java.io.IOException {
      if (extensionRegistry == null) {
        throw new NullPointerException();
      }
      try {
        boolean done = false;
        while (!done) {
          int tag = input.readTag();
          switch (tag) {
            case 0:
              done = true;
              break;
            case 10: {
              sessionId_ = input.readStringRequireUtf8();
              bitField0_ |= 0x00000001;
              break;
            } // case 10
            case 18: {
              input.readMessage(
                  getArrowBatchFieldBuilder().getBuilder(),
                  extensionRegistry);
              responseTypeCase_ = 2;
              break;
            } // case 18
            case 34: {
              input.readMessage(
                  getMetricsFieldBuilder().getBuilder(),
                  extensionRegistry);
              bitField0_ |= 0x00000800;
              break;
            } // case 34
            case 42: {
              input.readMessage(
                  getSqlCommandResultFieldBuilder().getBuilder(),
                  extensionRegistry);
              responseTypeCase_ = 5;
              break;
            } // case 42
            case 50: {
              org.apache.spark.connect.proto.ExecutePlanResponse.ObservedMetrics m =
                  input.readMessage(
                      org.apache.spark.connect.proto.ExecutePlanResponse.ObservedMetrics.parser(),
                      extensionRegistry);
              if (observedMetricsBuilder_ == null) {
                ensureObservedMetricsIsMutable();
                observedMetrics_.add(m);
              } else {
                observedMetricsBuilder_.addMessage(m);
              }
              break;
            } // case 50
            case 58: {
              input.readMessage(
                  getSchemaFieldBuilder().getBuilder(),
                  extensionRegistry);
              bitField0_ |= 0x00002000;
              break;
            } // case 58
            case 66: {
              input.readMessage(
                  getWriteStreamOperationStartResultFieldBuilder().getBuilder(),
                  extensionRegistry);
              responseTypeCase_ = 8;
              break;
            } // case 66
            case 74: {
              input.readMessage(
                  getStreamingQueryCommandResultFieldBuilder().getBuilder(),
                  extensionRegistry);
              responseTypeCase_ = 9;
              break;
            } // case 74
            case 82: {
              input.readMessage(
                  getGetResourcesCommandResultFieldBuilder().getBuilder(),
                  extensionRegistry);
              responseTypeCase_ = 10;
              break;
            } // case 82
            case 90: {
              input.readMessage(
                  getStreamingQueryManagerCommandResultFieldBuilder().getBuilder(),
                  extensionRegistry);
              responseTypeCase_ = 11;
              break;
            } // case 90
            case 98: {
              operationId_ = input.readStringRequireUtf8();
              bitField0_ |= 0x00000002;
              break;
            } // case 98
            case 106: {
              responseId_ = input.readStringRequireUtf8();
              bitField0_ |= 0x00000004;
              break;
            } // case 106
            case 114: {
              input.readMessage(
                  getResultCompleteFieldBuilder().getBuilder(),
                  extensionRegistry);
              responseTypeCase_ = 14;
              break;
            } // case 114
            case 7994: {
              input.readMessage(
                  getExtensionFieldBuilder().getBuilder(),
                  extensionRegistry);
              responseTypeCase_ = 999;
              break;
            } // case 7994
            default: {
              if (!super.parseUnknownField(input, extensionRegistry, tag)) {
                done = true; // was an endgroup tag
              }
              break;
            } // default:
          } // switch (tag)
        } // while (!done)
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.unwrapIOException();
      } finally {
        onChanged();
      } // finally
      return this;
    }
    private int responseTypeCase_ = 0;
    private Object responseType_;
    public ResponseTypeCase
        getResponseTypeCase() {
      return ResponseTypeCase.forNumber(
          responseTypeCase_);
    }

    public Builder clearResponseType() {
      responseTypeCase_ = 0;
      responseType_ = null;
      onChanged();
      return this;
    }

    private int bitField0_;

    private Object sessionId_ = "";
    /**
     * <code>string session_id = 1;</code>
     * @return The sessionId.
     */
    public String getSessionId() {
      Object ref = sessionId_;
      if (!(ref instanceof String)) {
        com.google.protobuf.ByteString bs =
            (com.google.protobuf.ByteString) ref;
        String s = bs.toStringUtf8();
        sessionId_ = s;
        return s;
      } else {
        return (String) ref;
      }
    }
    /**
     * <code>string session_id = 1;</code>
     * @return The bytes for sessionId.
     */
    public com.google.protobuf.ByteString
        getSessionIdBytes() {
      Object ref = sessionId_;
      if (ref instanceof String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (String) ref);
        sessionId_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }
    /**
     * <code>string session_id = 1;</code>
     * @param value The sessionId to set.
     * @return This builder for chaining.
     */
    public Builder setSessionId(
        String value) {
      if (value == null) { throw new NullPointerException(); }
      sessionId_ = value;
      bitField0_ |= 0x00000001;
      onChanged();
      return this;
    }
    /**
     * <code>string session_id = 1;</code>
     * @return This builder for chaining.
     */
    public Builder clearSessionId() {
      sessionId_ = getDefaultInstance().getSessionId();
      bitField0_ = (bitField0_ & ~0x00000001);
      onChanged();
      return this;
    }
    /**
     * <code>string session_id = 1;</code>
     * @param value The bytes for sessionId to set.
     * @return This builder for chaining.
     */
    public Builder setSessionIdBytes(
        com.google.protobuf.ByteString value) {
      if (value == null) { throw new NullPointerException(); }
      checkByteStringIsUtf8(value);
      sessionId_ = value;
      bitField0_ |= 0x00000001;
      onChanged();
      return this;
    }

    private Object operationId_ = "";
    /**
     * <pre>
     * Identifies the ExecutePlan execution.
     * If set by the client in ExecutePlanRequest.operationId, that value is returned.
     * Otherwise generated by the server.
     * It is an UUID string of the format `00112233-4455-6677-8899-aabbccddeeff`
     * </pre>
     *
     * <code>string operation_id = 12;</code>
     * @return The operationId.
     */
    public String getOperationId() {
      Object ref = operationId_;
      if (!(ref instanceof String)) {
        com.google.protobuf.ByteString bs =
            (com.google.protobuf.ByteString) ref;
        String s = bs.toStringUtf8();
        operationId_ = s;
        return s;
      } else {
        return (String) ref;
      }
    }
    /**
     * <pre>
     * Identifies the ExecutePlan execution.
     * If set by the client in ExecutePlanRequest.operationId, that value is returned.
     * Otherwise generated by the server.
     * It is an UUID string of the format `00112233-4455-6677-8899-aabbccddeeff`
     * </pre>
     *
     * <code>string operation_id = 12;</code>
     * @return The bytes for operationId.
     */
    public com.google.protobuf.ByteString
        getOperationIdBytes() {
      Object ref = operationId_;
      if (ref instanceof String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (String) ref);
        operationId_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }
    /**
     * <pre>
     * Identifies the ExecutePlan execution.
     * If set by the client in ExecutePlanRequest.operationId, that value is returned.
     * Otherwise generated by the server.
     * It is an UUID string of the format `00112233-4455-6677-8899-aabbccddeeff`
     * </pre>
     *
     * <code>string operation_id = 12;</code>
     * @param value The operationId to set.
     * @return This builder for chaining.
     */
    public Builder setOperationId(
        String value) {
      if (value == null) { throw new NullPointerException(); }
      operationId_ = value;
      bitField0_ |= 0x00000002;
      onChanged();
      return this;
    }
    /**
     * <pre>
     * Identifies the ExecutePlan execution.
     * If set by the client in ExecutePlanRequest.operationId, that value is returned.
     * Otherwise generated by the server.
     * It is an UUID string of the format `00112233-4455-6677-8899-aabbccddeeff`
     * </pre>
     *
     * <code>string operation_id = 12;</code>
     * @return This builder for chaining.
     */
    public Builder clearOperationId() {
      operationId_ = getDefaultInstance().getOperationId();
      bitField0_ = (bitField0_ & ~0x00000002);
      onChanged();
      return this;
    }
    /**
     * <pre>
     * Identifies the ExecutePlan execution.
     * If set by the client in ExecutePlanRequest.operationId, that value is returned.
     * Otherwise generated by the server.
     * It is an UUID string of the format `00112233-4455-6677-8899-aabbccddeeff`
     * </pre>
     *
     * <code>string operation_id = 12;</code>
     * @param value The bytes for operationId to set.
     * @return This builder for chaining.
     */
    public Builder setOperationIdBytes(
        com.google.protobuf.ByteString value) {
      if (value == null) { throw new NullPointerException(); }
      checkByteStringIsUtf8(value);
      operationId_ = value;
      bitField0_ |= 0x00000002;
      onChanged();
      return this;
    }

    private Object responseId_ = "";
    /**
     * <pre>
     * Identified the response in the stream.
     * The id is an UUID string of the format `00112233-4455-6677-8899-aabbccddeeff`
     * </pre>
     *
     * <code>string response_id = 13;</code>
     * @return The responseId.
     */
    public String getResponseId() {
      Object ref = responseId_;
      if (!(ref instanceof String)) {
        com.google.protobuf.ByteString bs =
            (com.google.protobuf.ByteString) ref;
        String s = bs.toStringUtf8();
        responseId_ = s;
        return s;
      } else {
        return (String) ref;
      }
    }
    /**
     * <pre>
     * Identified the response in the stream.
     * The id is an UUID string of the format `00112233-4455-6677-8899-aabbccddeeff`
     * </pre>
     *
     * <code>string response_id = 13;</code>
     * @return The bytes for responseId.
     */
    public com.google.protobuf.ByteString
        getResponseIdBytes() {
      Object ref = responseId_;
      if (ref instanceof String) {
        com.google.protobuf.ByteString b = 
            com.google.protobuf.ByteString.copyFromUtf8(
                (String) ref);
        responseId_ = b;
        return b;
      } else {
        return (com.google.protobuf.ByteString) ref;
      }
    }
    /**
     * <pre>
     * Identified the response in the stream.
     * The id is an UUID string of the format `00112233-4455-6677-8899-aabbccddeeff`
     * </pre>
     *
     * <code>string response_id = 13;</code>
     * @param value The responseId to set.
     * @return This builder for chaining.
     */
    public Builder setResponseId(
        String value) {
      if (value == null) { throw new NullPointerException(); }
      responseId_ = value;
      bitField0_ |= 0x00000004;
      onChanged();
      return this;
    }
    /**
     * <pre>
     * Identified the response in the stream.
     * The id is an UUID string of the format `00112233-4455-6677-8899-aabbccddeeff`
     * </pre>
     *
     * <code>string response_id = 13;</code>
     * @return This builder for chaining.
     */
    public Builder clearResponseId() {
      responseId_ = getDefaultInstance().getResponseId();
      bitField0_ = (bitField0_ & ~0x00000004);
      onChanged();
      return this;
    }
    /**
     * <pre>
     * Identified the response in the stream.
     * The id is an UUID string of the format `00112233-4455-6677-8899-aabbccddeeff`
     * </pre>
     *
     * <code>string response_id = 13;</code>
     * @param value The bytes for responseId to set.
     * @return This builder for chaining.
     */
    public Builder setResponseIdBytes(
        com.google.protobuf.ByteString value) {
      if (value == null) { throw new NullPointerException(); }
      checkByteStringIsUtf8(value);
      responseId_ = value;
      bitField0_ |= 0x00000004;
      onChanged();
      return this;
    }

    private com.google.protobuf.SingleFieldBuilderV3<
        org.apache.spark.connect.proto.ExecutePlanResponse.ArrowBatch, org.apache.spark.connect.proto.ExecutePlanResponse.ArrowBatch.Builder, org.apache.spark.connect.proto.ExecutePlanResponse.ArrowBatchOrBuilder> arrowBatchBuilder_;
    /**
     * <code>.spark.connect.ExecutePlanResponse.ArrowBatch arrow_batch = 2;</code>
     * @return Whether the arrowBatch field is set.
     */
    @Override
    public boolean hasArrowBatch() {
      return responseTypeCase_ == 2;
    }
    /**
     * <code>.spark.connect.ExecutePlanResponse.ArrowBatch arrow_batch = 2;</code>
     * @return The arrowBatch.
     */
    @Override
    public org.apache.spark.connect.proto.ExecutePlanResponse.ArrowBatch getArrowBatch() {
      if (arrowBatchBuilder_ == null) {
        if (responseTypeCase_ == 2) {
          return (org.apache.spark.connect.proto.ExecutePlanResponse.ArrowBatch) responseType_;
        }
        return org.apache.spark.connect.proto.ExecutePlanResponse.ArrowBatch.getDefaultInstance();
      } else {
        if (responseTypeCase_ == 2) {
          return arrowBatchBuilder_.getMessage();
        }
        return org.apache.spark.connect.proto.ExecutePlanResponse.ArrowBatch.getDefaultInstance();
      }
    }
    /**
     * <code>.spark.connect.ExecutePlanResponse.ArrowBatch arrow_batch = 2;</code>
     */
    public Builder setArrowBatch(org.apache.spark.connect.proto.ExecutePlanResponse.ArrowBatch value) {
      if (arrowBatchBuilder_ == null) {
        if (value == null) {
          throw new NullPointerException();
        }
        responseType_ = value;
        onChanged();
      } else {
        arrowBatchBuilder_.setMessage(value);
      }
      responseTypeCase_ = 2;
      return this;
    }
    /**
     * <code>.spark.connect.ExecutePlanResponse.ArrowBatch arrow_batch = 2;</code>
     */
    public Builder setArrowBatch(
        org.apache.spark.connect.proto.ExecutePlanResponse.ArrowBatch.Builder builderForValue) {
      if (arrowBatchBuilder_ == null) {
        responseType_ = builderForValue.build();
        onChanged();
      } else {
        arrowBatchBuilder_.setMessage(builderForValue.build());
      }
      responseTypeCase_ = 2;
      return this;
    }
    /**
     * <code>.spark.connect.ExecutePlanResponse.ArrowBatch arrow_batch = 2;</code>
     */
    public Builder mergeArrowBatch(org.apache.spark.connect.proto.ExecutePlanResponse.ArrowBatch value) {
      if (arrowBatchBuilder_ == null) {
        if (responseTypeCase_ == 2 &&
            responseType_ != org.apache.spark.connect.proto.ExecutePlanResponse.ArrowBatch.getDefaultInstance()) {
          responseType_ = org.apache.spark.connect.proto.ExecutePlanResponse.ArrowBatch.newBuilder((org.apache.spark.connect.proto.ExecutePlanResponse.ArrowBatch) responseType_)
              .mergeFrom(value).buildPartial();
        } else {
          responseType_ = value;
        }
        onChanged();
      } else {
        if (responseTypeCase_ == 2) {
          arrowBatchBuilder_.mergeFrom(value);
        } else {
          arrowBatchBuilder_.setMessage(value);
        }
      }
      responseTypeCase_ = 2;
      return this;
    }
    /**
     * <code>.spark.connect.ExecutePlanResponse.ArrowBatch arrow_batch = 2;</code>
     */
    public Builder clearArrowBatch() {
      if (arrowBatchBuilder_ == null) {
        if (responseTypeCase_ == 2) {
          responseTypeCase_ = 0;
          responseType_ = null;
          onChanged();
        }
      } else {
        if (responseTypeCase_ == 2) {
          responseTypeCase_ = 0;
          responseType_ = null;
        }
        arrowBatchBuilder_.clear();
      }
      return this;
    }
    /**
     * <code>.spark.connect.ExecutePlanResponse.ArrowBatch arrow_batch = 2;</code>
     */
    public org.apache.spark.connect.proto.ExecutePlanResponse.ArrowBatch.Builder getArrowBatchBuilder() {
      return getArrowBatchFieldBuilder().getBuilder();
    }
    /**
     * <code>.spark.connect.ExecutePlanResponse.ArrowBatch arrow_batch = 2;</code>
     */
    @Override
    public org.apache.spark.connect.proto.ExecutePlanResponse.ArrowBatchOrBuilder getArrowBatchOrBuilder() {
      if ((responseTypeCase_ == 2) && (arrowBatchBuilder_ != null)) {
        return arrowBatchBuilder_.getMessageOrBuilder();
      } else {
        if (responseTypeCase_ == 2) {
          return (org.apache.spark.connect.proto.ExecutePlanResponse.ArrowBatch) responseType_;
        }
        return org.apache.spark.connect.proto.ExecutePlanResponse.ArrowBatch.getDefaultInstance();
      }
    }
    /**
     * <code>.spark.connect.ExecutePlanResponse.ArrowBatch arrow_batch = 2;</code>
     */
    private com.google.protobuf.SingleFieldBuilderV3<
        org.apache.spark.connect.proto.ExecutePlanResponse.ArrowBatch, org.apache.spark.connect.proto.ExecutePlanResponse.ArrowBatch.Builder, org.apache.spark.connect.proto.ExecutePlanResponse.ArrowBatchOrBuilder> 
        getArrowBatchFieldBuilder() {
      if (arrowBatchBuilder_ == null) {
        if (!(responseTypeCase_ == 2)) {
          responseType_ = org.apache.spark.connect.proto.ExecutePlanResponse.ArrowBatch.getDefaultInstance();
        }
        arrowBatchBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
            org.apache.spark.connect.proto.ExecutePlanResponse.ArrowBatch, org.apache.spark.connect.proto.ExecutePlanResponse.ArrowBatch.Builder, org.apache.spark.connect.proto.ExecutePlanResponse.ArrowBatchOrBuilder>(
                (org.apache.spark.connect.proto.ExecutePlanResponse.ArrowBatch) responseType_,
                getParentForChildren(),
                isClean());
        responseType_ = null;
      }
      responseTypeCase_ = 2;
      onChanged();
      return arrowBatchBuilder_;
    }

    private com.google.protobuf.SingleFieldBuilderV3<
        org.apache.spark.connect.proto.ExecutePlanResponse.SqlCommandResult, org.apache.spark.connect.proto.ExecutePlanResponse.SqlCommandResult.Builder, org.apache.spark.connect.proto.ExecutePlanResponse.SqlCommandResultOrBuilder> sqlCommandResultBuilder_;
    /**
     * <pre>
     * Special case for executing SQL commands.
     * </pre>
     *
     * <code>.spark.connect.ExecutePlanResponse.SqlCommandResult sql_command_result = 5;</code>
     * @return Whether the sqlCommandResult field is set.
     */
    @Override
    public boolean hasSqlCommandResult() {
      return responseTypeCase_ == 5;
    }
    /**
     * <pre>
     * Special case for executing SQL commands.
     * </pre>
     *
     * <code>.spark.connect.ExecutePlanResponse.SqlCommandResult sql_command_result = 5;</code>
     * @return The sqlCommandResult.
     */
    @Override
    public org.apache.spark.connect.proto.ExecutePlanResponse.SqlCommandResult getSqlCommandResult() {
      if (sqlCommandResultBuilder_ == null) {
        if (responseTypeCase_ == 5) {
          return (org.apache.spark.connect.proto.ExecutePlanResponse.SqlCommandResult) responseType_;
        }
        return org.apache.spark.connect.proto.ExecutePlanResponse.SqlCommandResult.getDefaultInstance();
      } else {
        if (responseTypeCase_ == 5) {
          return sqlCommandResultBuilder_.getMessage();
        }
        return org.apache.spark.connect.proto.ExecutePlanResponse.SqlCommandResult.getDefaultInstance();
      }
    }
    /**
     * <pre>
     * Special case for executing SQL commands.
     * </pre>
     *
     * <code>.spark.connect.ExecutePlanResponse.SqlCommandResult sql_command_result = 5;</code>
     */
    public Builder setSqlCommandResult(org.apache.spark.connect.proto.ExecutePlanResponse.SqlCommandResult value) {
      if (sqlCommandResultBuilder_ == null) {
        if (value == null) {
          throw new NullPointerException();
        }
        responseType_ = value;
        onChanged();
      } else {
        sqlCommandResultBuilder_.setMessage(value);
      }
      responseTypeCase_ = 5;
      return this;
    }
    /**
     * <pre>
     * Special case for executing SQL commands.
     * </pre>
     *
     * <code>.spark.connect.ExecutePlanResponse.SqlCommandResult sql_command_result = 5;</code>
     */
    public Builder setSqlCommandResult(
        org.apache.spark.connect.proto.ExecutePlanResponse.SqlCommandResult.Builder builderForValue) {
      if (sqlCommandResultBuilder_ == null) {
        responseType_ = builderForValue.build();
        onChanged();
      } else {
        sqlCommandResultBuilder_.setMessage(builderForValue.build());
      }
      responseTypeCase_ = 5;
      return this;
    }
    /**
     * <pre>
     * Special case for executing SQL commands.
     * </pre>
     *
     * <code>.spark.connect.ExecutePlanResponse.SqlCommandResult sql_command_result = 5;</code>
     */
    public Builder mergeSqlCommandResult(org.apache.spark.connect.proto.ExecutePlanResponse.SqlCommandResult value) {
      if (sqlCommandResultBuilder_ == null) {
        if (responseTypeCase_ == 5 &&
            responseType_ != org.apache.spark.connect.proto.ExecutePlanResponse.SqlCommandResult.getDefaultInstance()) {
          responseType_ = org.apache.spark.connect.proto.ExecutePlanResponse.SqlCommandResult.newBuilder((org.apache.spark.connect.proto.ExecutePlanResponse.SqlCommandResult) responseType_)
              .mergeFrom(value).buildPartial();
        } else {
          responseType_ = value;
        }
        onChanged();
      } else {
        if (responseTypeCase_ == 5) {
          sqlCommandResultBuilder_.mergeFrom(value);
        } else {
          sqlCommandResultBuilder_.setMessage(value);
        }
      }
      responseTypeCase_ = 5;
      return this;
    }
    /**
     * <pre>
     * Special case for executing SQL commands.
     * </pre>
     *
     * <code>.spark.connect.ExecutePlanResponse.SqlCommandResult sql_command_result = 5;</code>
     */
    public Builder clearSqlCommandResult() {
      if (sqlCommandResultBuilder_ == null) {
        if (responseTypeCase_ == 5) {
          responseTypeCase_ = 0;
          responseType_ = null;
          onChanged();
        }
      } else {
        if (responseTypeCase_ == 5) {
          responseTypeCase_ = 0;
          responseType_ = null;
        }
        sqlCommandResultBuilder_.clear();
      }
      return this;
    }
    /**
     * <pre>
     * Special case for executing SQL commands.
     * </pre>
     *
     * <code>.spark.connect.ExecutePlanResponse.SqlCommandResult sql_command_result = 5;</code>
     */
    public org.apache.spark.connect.proto.ExecutePlanResponse.SqlCommandResult.Builder getSqlCommandResultBuilder() {
      return getSqlCommandResultFieldBuilder().getBuilder();
    }
    /**
     * <pre>
     * Special case for executing SQL commands.
     * </pre>
     *
     * <code>.spark.connect.ExecutePlanResponse.SqlCommandResult sql_command_result = 5;</code>
     */
    @Override
    public org.apache.spark.connect.proto.ExecutePlanResponse.SqlCommandResultOrBuilder getSqlCommandResultOrBuilder() {
      if ((responseTypeCase_ == 5) && (sqlCommandResultBuilder_ != null)) {
        return sqlCommandResultBuilder_.getMessageOrBuilder();
      } else {
        if (responseTypeCase_ == 5) {
          return (org.apache.spark.connect.proto.ExecutePlanResponse.SqlCommandResult) responseType_;
        }
        return org.apache.spark.connect.proto.ExecutePlanResponse.SqlCommandResult.getDefaultInstance();
      }
    }
    /**
     * <pre>
     * Special case for executing SQL commands.
     * </pre>
     *
     * <code>.spark.connect.ExecutePlanResponse.SqlCommandResult sql_command_result = 5;</code>
     */
    private com.google.protobuf.SingleFieldBuilderV3<
        org.apache.spark.connect.proto.ExecutePlanResponse.SqlCommandResult, org.apache.spark.connect.proto.ExecutePlanResponse.SqlCommandResult.Builder, org.apache.spark.connect.proto.ExecutePlanResponse.SqlCommandResultOrBuilder> 
        getSqlCommandResultFieldBuilder() {
      if (sqlCommandResultBuilder_ == null) {
        if (!(responseTypeCase_ == 5)) {
          responseType_ = org.apache.spark.connect.proto.ExecutePlanResponse.SqlCommandResult.getDefaultInstance();
        }
        sqlCommandResultBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
            org.apache.spark.connect.proto.ExecutePlanResponse.SqlCommandResult, org.apache.spark.connect.proto.ExecutePlanResponse.SqlCommandResult.Builder, org.apache.spark.connect.proto.ExecutePlanResponse.SqlCommandResultOrBuilder>(
                (org.apache.spark.connect.proto.ExecutePlanResponse.SqlCommandResult) responseType_,
                getParentForChildren(),
                isClean());
        responseType_ = null;
      }
      responseTypeCase_ = 5;
      onChanged();
      return sqlCommandResultBuilder_;
    }

    private com.google.protobuf.SingleFieldBuilderV3<
        org.apache.spark.connect.proto.WriteStreamOperationStartResult, org.apache.spark.connect.proto.WriteStreamOperationStartResult.Builder, org.apache.spark.connect.proto.WriteStreamOperationStartResultOrBuilder> writeStreamOperationStartResultBuilder_;
    /**
     * <pre>
     * Response for a streaming query.
     * </pre>
     *
     * <code>.spark.connect.WriteStreamOperationStartResult write_stream_operation_start_result = 8;</code>
     * @return Whether the writeStreamOperationStartResult field is set.
     */
    @Override
    public boolean hasWriteStreamOperationStartResult() {
      return responseTypeCase_ == 8;
    }
    /**
     * <pre>
     * Response for a streaming query.
     * </pre>
     *
     * <code>.spark.connect.WriteStreamOperationStartResult write_stream_operation_start_result = 8;</code>
     * @return The writeStreamOperationStartResult.
     */
    @Override
    public org.apache.spark.connect.proto.WriteStreamOperationStartResult getWriteStreamOperationStartResult() {
      if (writeStreamOperationStartResultBuilder_ == null) {
        if (responseTypeCase_ == 8) {
          return (org.apache.spark.connect.proto.WriteStreamOperationStartResult) responseType_;
        }
        return org.apache.spark.connect.proto.WriteStreamOperationStartResult.getDefaultInstance();
      } else {
        if (responseTypeCase_ == 8) {
          return writeStreamOperationStartResultBuilder_.getMessage();
        }
        return org.apache.spark.connect.proto.WriteStreamOperationStartResult.getDefaultInstance();
      }
    }
    /**
     * <pre>
     * Response for a streaming query.
     * </pre>
     *
     * <code>.spark.connect.WriteStreamOperationStartResult write_stream_operation_start_result = 8;</code>
     */
    public Builder setWriteStreamOperationStartResult(org.apache.spark.connect.proto.WriteStreamOperationStartResult value) {
      if (writeStreamOperationStartResultBuilder_ == null) {
        if (value == null) {
          throw new NullPointerException();
        }
        responseType_ = value;
        onChanged();
      } else {
        writeStreamOperationStartResultBuilder_.setMessage(value);
      }
      responseTypeCase_ = 8;
      return this;
    }
    /**
     * <pre>
     * Response for a streaming query.
     * </pre>
     *
     * <code>.spark.connect.WriteStreamOperationStartResult write_stream_operation_start_result = 8;</code>
     */
    public Builder setWriteStreamOperationStartResult(
        org.apache.spark.connect.proto.WriteStreamOperationStartResult.Builder builderForValue) {
      if (writeStreamOperationStartResultBuilder_ == null) {
        responseType_ = builderForValue.build();
        onChanged();
      } else {
        writeStreamOperationStartResultBuilder_.setMessage(builderForValue.build());
      }
      responseTypeCase_ = 8;
      return this;
    }
    /**
     * <pre>
     * Response for a streaming query.
     * </pre>
     *
     * <code>.spark.connect.WriteStreamOperationStartResult write_stream_operation_start_result = 8;</code>
     */
    public Builder mergeWriteStreamOperationStartResult(org.apache.spark.connect.proto.WriteStreamOperationStartResult value) {
      if (writeStreamOperationStartResultBuilder_ == null) {
        if (responseTypeCase_ == 8 &&
            responseType_ != org.apache.spark.connect.proto.WriteStreamOperationStartResult.getDefaultInstance()) {
          responseType_ = org.apache.spark.connect.proto.WriteStreamOperationStartResult.newBuilder((org.apache.spark.connect.proto.WriteStreamOperationStartResult) responseType_)
              .mergeFrom(value).buildPartial();
        } else {
          responseType_ = value;
        }
        onChanged();
      } else {
        if (responseTypeCase_ == 8) {
          writeStreamOperationStartResultBuilder_.mergeFrom(value);
        } else {
          writeStreamOperationStartResultBuilder_.setMessage(value);
        }
      }
      responseTypeCase_ = 8;
      return this;
    }
    /**
     * <pre>
     * Response for a streaming query.
     * </pre>
     *
     * <code>.spark.connect.WriteStreamOperationStartResult write_stream_operation_start_result = 8;</code>
     */
    public Builder clearWriteStreamOperationStartResult() {
      if (writeStreamOperationStartResultBuilder_ == null) {
        if (responseTypeCase_ == 8) {
          responseTypeCase_ = 0;
          responseType_ = null;
          onChanged();
        }
      } else {
        if (responseTypeCase_ == 8) {
          responseTypeCase_ = 0;
          responseType_ = null;
        }
        writeStreamOperationStartResultBuilder_.clear();
      }
      return this;
    }
    /**
     * <pre>
     * Response for a streaming query.
     * </pre>
     *
     * <code>.spark.connect.WriteStreamOperationStartResult write_stream_operation_start_result = 8;</code>
     */
    public org.apache.spark.connect.proto.WriteStreamOperationStartResult.Builder getWriteStreamOperationStartResultBuilder() {
      return getWriteStreamOperationStartResultFieldBuilder().getBuilder();
    }
    /**
     * <pre>
     * Response for a streaming query.
     * </pre>
     *
     * <code>.spark.connect.WriteStreamOperationStartResult write_stream_operation_start_result = 8;</code>
     */
    @Override
    public org.apache.spark.connect.proto.WriteStreamOperationStartResultOrBuilder getWriteStreamOperationStartResultOrBuilder() {
      if ((responseTypeCase_ == 8) && (writeStreamOperationStartResultBuilder_ != null)) {
        return writeStreamOperationStartResultBuilder_.getMessageOrBuilder();
      } else {
        if (responseTypeCase_ == 8) {
          return (org.apache.spark.connect.proto.WriteStreamOperationStartResult) responseType_;
        }
        return org.apache.spark.connect.proto.WriteStreamOperationStartResult.getDefaultInstance();
      }
    }
    /**
     * <pre>
     * Response for a streaming query.
     * </pre>
     *
     * <code>.spark.connect.WriteStreamOperationStartResult write_stream_operation_start_result = 8;</code>
     */
    private com.google.protobuf.SingleFieldBuilderV3<
        org.apache.spark.connect.proto.WriteStreamOperationStartResult, org.apache.spark.connect.proto.WriteStreamOperationStartResult.Builder, org.apache.spark.connect.proto.WriteStreamOperationStartResultOrBuilder> 
        getWriteStreamOperationStartResultFieldBuilder() {
      if (writeStreamOperationStartResultBuilder_ == null) {
        if (!(responseTypeCase_ == 8)) {
          responseType_ = org.apache.spark.connect.proto.WriteStreamOperationStartResult.getDefaultInstance();
        }
        writeStreamOperationStartResultBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
            org.apache.spark.connect.proto.WriteStreamOperationStartResult, org.apache.spark.connect.proto.WriteStreamOperationStartResult.Builder, org.apache.spark.connect.proto.WriteStreamOperationStartResultOrBuilder>(
                (org.apache.spark.connect.proto.WriteStreamOperationStartResult) responseType_,
                getParentForChildren(),
                isClean());
        responseType_ = null;
      }
      responseTypeCase_ = 8;
      onChanged();
      return writeStreamOperationStartResultBuilder_;
    }

    private com.google.protobuf.SingleFieldBuilderV3<
        org.apache.spark.connect.proto.StreamingQueryCommandResult, org.apache.spark.connect.proto.StreamingQueryCommandResult.Builder, org.apache.spark.connect.proto.StreamingQueryCommandResultOrBuilder> streamingQueryCommandResultBuilder_;
    /**
     * <pre>
     * Response for commands on a streaming query.
     * </pre>
     *
     * <code>.spark.connect.StreamingQueryCommandResult streaming_query_command_result = 9;</code>
     * @return Whether the streamingQueryCommandResult field is set.
     */
    @Override
    public boolean hasStreamingQueryCommandResult() {
      return responseTypeCase_ == 9;
    }
    /**
     * <pre>
     * Response for commands on a streaming query.
     * </pre>
     *
     * <code>.spark.connect.StreamingQueryCommandResult streaming_query_command_result = 9;</code>
     * @return The streamingQueryCommandResult.
     */
    @Override
    public org.apache.spark.connect.proto.StreamingQueryCommandResult getStreamingQueryCommandResult() {
      if (streamingQueryCommandResultBuilder_ == null) {
        if (responseTypeCase_ == 9) {
          return (org.apache.spark.connect.proto.StreamingQueryCommandResult) responseType_;
        }
        return org.apache.spark.connect.proto.StreamingQueryCommandResult.getDefaultInstance();
      } else {
        if (responseTypeCase_ == 9) {
          return streamingQueryCommandResultBuilder_.getMessage();
        }
        return org.apache.spark.connect.proto.StreamingQueryCommandResult.getDefaultInstance();
      }
    }
    /**
     * <pre>
     * Response for commands on a streaming query.
     * </pre>
     *
     * <code>.spark.connect.StreamingQueryCommandResult streaming_query_command_result = 9;</code>
     */
    public Builder setStreamingQueryCommandResult(org.apache.spark.connect.proto.StreamingQueryCommandResult value) {
      if (streamingQueryCommandResultBuilder_ == null) {
        if (value == null) {
          throw new NullPointerException();
        }
        responseType_ = value;
        onChanged();
      } else {
        streamingQueryCommandResultBuilder_.setMessage(value);
      }
      responseTypeCase_ = 9;
      return this;
    }
    /**
     * <pre>
     * Response for commands on a streaming query.
     * </pre>
     *
     * <code>.spark.connect.StreamingQueryCommandResult streaming_query_command_result = 9;</code>
     */
    public Builder setStreamingQueryCommandResult(
        org.apache.spark.connect.proto.StreamingQueryCommandResult.Builder builderForValue) {
      if (streamingQueryCommandResultBuilder_ == null) {
        responseType_ = builderForValue.build();
        onChanged();
      } else {
        streamingQueryCommandResultBuilder_.setMessage(builderForValue.build());
      }
      responseTypeCase_ = 9;
      return this;
    }
    /**
     * <pre>
     * Response for commands on a streaming query.
     * </pre>
     *
     * <code>.spark.connect.StreamingQueryCommandResult streaming_query_command_result = 9;</code>
     */
    public Builder mergeStreamingQueryCommandResult(org.apache.spark.connect.proto.StreamingQueryCommandResult value) {
      if (streamingQueryCommandResultBuilder_ == null) {
        if (responseTypeCase_ == 9 &&
            responseType_ != org.apache.spark.connect.proto.StreamingQueryCommandResult.getDefaultInstance()) {
          responseType_ = org.apache.spark.connect.proto.StreamingQueryCommandResult.newBuilder((org.apache.spark.connect.proto.StreamingQueryCommandResult) responseType_)
              .mergeFrom(value).buildPartial();
        } else {
          responseType_ = value;
        }
        onChanged();
      } else {
        if (responseTypeCase_ == 9) {
          streamingQueryCommandResultBuilder_.mergeFrom(value);
        } else {
          streamingQueryCommandResultBuilder_.setMessage(value);
        }
      }
      responseTypeCase_ = 9;
      return this;
    }
    /**
     * <pre>
     * Response for commands on a streaming query.
     * </pre>
     *
     * <code>.spark.connect.StreamingQueryCommandResult streaming_query_command_result = 9;</code>
     */
    public Builder clearStreamingQueryCommandResult() {
      if (streamingQueryCommandResultBuilder_ == null) {
        if (responseTypeCase_ == 9) {
          responseTypeCase_ = 0;
          responseType_ = null;
          onChanged();
        }
      } else {
        if (responseTypeCase_ == 9) {
          responseTypeCase_ = 0;
          responseType_ = null;
        }
        streamingQueryCommandResultBuilder_.clear();
      }
      return this;
    }
    /**
     * <pre>
     * Response for commands on a streaming query.
     * </pre>
     *
     * <code>.spark.connect.StreamingQueryCommandResult streaming_query_command_result = 9;</code>
     */
    public org.apache.spark.connect.proto.StreamingQueryCommandResult.Builder getStreamingQueryCommandResultBuilder() {
      return getStreamingQueryCommandResultFieldBuilder().getBuilder();
    }
    /**
     * <pre>
     * Response for commands on a streaming query.
     * </pre>
     *
     * <code>.spark.connect.StreamingQueryCommandResult streaming_query_command_result = 9;</code>
     */
    @Override
    public org.apache.spark.connect.proto.StreamingQueryCommandResultOrBuilder getStreamingQueryCommandResultOrBuilder() {
      if ((responseTypeCase_ == 9) && (streamingQueryCommandResultBuilder_ != null)) {
        return streamingQueryCommandResultBuilder_.getMessageOrBuilder();
      } else {
        if (responseTypeCase_ == 9) {
          return (org.apache.spark.connect.proto.StreamingQueryCommandResult) responseType_;
        }
        return org.apache.spark.connect.proto.StreamingQueryCommandResult.getDefaultInstance();
      }
    }
    /**
     * <pre>
     * Response for commands on a streaming query.
     * </pre>
     *
     * <code>.spark.connect.StreamingQueryCommandResult streaming_query_command_result = 9;</code>
     */
    private com.google.protobuf.SingleFieldBuilderV3<
        org.apache.spark.connect.proto.StreamingQueryCommandResult, org.apache.spark.connect.proto.StreamingQueryCommandResult.Builder, org.apache.spark.connect.proto.StreamingQueryCommandResultOrBuilder> 
        getStreamingQueryCommandResultFieldBuilder() {
      if (streamingQueryCommandResultBuilder_ == null) {
        if (!(responseTypeCase_ == 9)) {
          responseType_ = org.apache.spark.connect.proto.StreamingQueryCommandResult.getDefaultInstance();
        }
        streamingQueryCommandResultBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
            org.apache.spark.connect.proto.StreamingQueryCommandResult, org.apache.spark.connect.proto.StreamingQueryCommandResult.Builder, org.apache.spark.connect.proto.StreamingQueryCommandResultOrBuilder>(
                (org.apache.spark.connect.proto.StreamingQueryCommandResult) responseType_,
                getParentForChildren(),
                isClean());
        responseType_ = null;
      }
      responseTypeCase_ = 9;
      onChanged();
      return streamingQueryCommandResultBuilder_;
    }

    private com.google.protobuf.SingleFieldBuilderV3<
        org.apache.spark.connect.proto.GetResourcesCommandResult, org.apache.spark.connect.proto.GetResourcesCommandResult.Builder, org.apache.spark.connect.proto.GetResourcesCommandResultOrBuilder> getResourcesCommandResultBuilder_;
    /**
     * <pre>
     * Response for 'SparkContext.resources'.
     * </pre>
     *
     * <code>.spark.connect.GetResourcesCommandResult get_resources_command_result = 10;</code>
     * @return Whether the getResourcesCommandResult field is set.
     */
    @Override
    public boolean hasGetResourcesCommandResult() {
      return responseTypeCase_ == 10;
    }
    /**
     * <pre>
     * Response for 'SparkContext.resources'.
     * </pre>
     *
     * <code>.spark.connect.GetResourcesCommandResult get_resources_command_result = 10;</code>
     * @return The getResourcesCommandResult.
     */
    @Override
    public org.apache.spark.connect.proto.GetResourcesCommandResult getGetResourcesCommandResult() {
      if (getResourcesCommandResultBuilder_ == null) {
        if (responseTypeCase_ == 10) {
          return (org.apache.spark.connect.proto.GetResourcesCommandResult) responseType_;
        }
        return org.apache.spark.connect.proto.GetResourcesCommandResult.getDefaultInstance();
      } else {
        if (responseTypeCase_ == 10) {
          return getResourcesCommandResultBuilder_.getMessage();
        }
        return org.apache.spark.connect.proto.GetResourcesCommandResult.getDefaultInstance();
      }
    }
    /**
     * <pre>
     * Response for 'SparkContext.resources'.
     * </pre>
     *
     * <code>.spark.connect.GetResourcesCommandResult get_resources_command_result = 10;</code>
     */
    public Builder setGetResourcesCommandResult(org.apache.spark.connect.proto.GetResourcesCommandResult value) {
      if (getResourcesCommandResultBuilder_ == null) {
        if (value == null) {
          throw new NullPointerException();
        }
        responseType_ = value;
        onChanged();
      } else {
        getResourcesCommandResultBuilder_.setMessage(value);
      }
      responseTypeCase_ = 10;
      return this;
    }
    /**
     * <pre>
     * Response for 'SparkContext.resources'.
     * </pre>
     *
     * <code>.spark.connect.GetResourcesCommandResult get_resources_command_result = 10;</code>
     */
    public Builder setGetResourcesCommandResult(
        org.apache.spark.connect.proto.GetResourcesCommandResult.Builder builderForValue) {
      if (getResourcesCommandResultBuilder_ == null) {
        responseType_ = builderForValue.build();
        onChanged();
      } else {
        getResourcesCommandResultBuilder_.setMessage(builderForValue.build());
      }
      responseTypeCase_ = 10;
      return this;
    }
    /**
     * <pre>
     * Response for 'SparkContext.resources'.
     * </pre>
     *
     * <code>.spark.connect.GetResourcesCommandResult get_resources_command_result = 10;</code>
     */
    public Builder mergeGetResourcesCommandResult(org.apache.spark.connect.proto.GetResourcesCommandResult value) {
      if (getResourcesCommandResultBuilder_ == null) {
        if (responseTypeCase_ == 10 &&
            responseType_ != org.apache.spark.connect.proto.GetResourcesCommandResult.getDefaultInstance()) {
          responseType_ = org.apache.spark.connect.proto.GetResourcesCommandResult.newBuilder((org.apache.spark.connect.proto.GetResourcesCommandResult) responseType_)
              .mergeFrom(value).buildPartial();
        } else {
          responseType_ = value;
        }
        onChanged();
      } else {
        if (responseTypeCase_ == 10) {
          getResourcesCommandResultBuilder_.mergeFrom(value);
        } else {
          getResourcesCommandResultBuilder_.setMessage(value);
        }
      }
      responseTypeCase_ = 10;
      return this;
    }
    /**
     * <pre>
     * Response for 'SparkContext.resources'.
     * </pre>
     *
     * <code>.spark.connect.GetResourcesCommandResult get_resources_command_result = 10;</code>
     */
    public Builder clearGetResourcesCommandResult() {
      if (getResourcesCommandResultBuilder_ == null) {
        if (responseTypeCase_ == 10) {
          responseTypeCase_ = 0;
          responseType_ = null;
          onChanged();
        }
      } else {
        if (responseTypeCase_ == 10) {
          responseTypeCase_ = 0;
          responseType_ = null;
        }
        getResourcesCommandResultBuilder_.clear();
      }
      return this;
    }
    /**
     * <pre>
     * Response for 'SparkContext.resources'.
     * </pre>
     *
     * <code>.spark.connect.GetResourcesCommandResult get_resources_command_result = 10;</code>
     */
    public org.apache.spark.connect.proto.GetResourcesCommandResult.Builder getGetResourcesCommandResultBuilder() {
      return getGetResourcesCommandResultFieldBuilder().getBuilder();
    }
    /**
     * <pre>
     * Response for 'SparkContext.resources'.
     * </pre>
     *
     * <code>.spark.connect.GetResourcesCommandResult get_resources_command_result = 10;</code>
     */
    @Override
    public org.apache.spark.connect.proto.GetResourcesCommandResultOrBuilder getGetResourcesCommandResultOrBuilder() {
      if ((responseTypeCase_ == 10) && (getResourcesCommandResultBuilder_ != null)) {
        return getResourcesCommandResultBuilder_.getMessageOrBuilder();
      } else {
        if (responseTypeCase_ == 10) {
          return (org.apache.spark.connect.proto.GetResourcesCommandResult) responseType_;
        }
        return org.apache.spark.connect.proto.GetResourcesCommandResult.getDefaultInstance();
      }
    }
    /**
     * <pre>
     * Response for 'SparkContext.resources'.
     * </pre>
     *
     * <code>.spark.connect.GetResourcesCommandResult get_resources_command_result = 10;</code>
     */
    private com.google.protobuf.SingleFieldBuilderV3<
        org.apache.spark.connect.proto.GetResourcesCommandResult, org.apache.spark.connect.proto.GetResourcesCommandResult.Builder, org.apache.spark.connect.proto.GetResourcesCommandResultOrBuilder> 
        getGetResourcesCommandResultFieldBuilder() {
      if (getResourcesCommandResultBuilder_ == null) {
        if (!(responseTypeCase_ == 10)) {
          responseType_ = org.apache.spark.connect.proto.GetResourcesCommandResult.getDefaultInstance();
        }
        getResourcesCommandResultBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
            org.apache.spark.connect.proto.GetResourcesCommandResult, org.apache.spark.connect.proto.GetResourcesCommandResult.Builder, org.apache.spark.connect.proto.GetResourcesCommandResultOrBuilder>(
                (org.apache.spark.connect.proto.GetResourcesCommandResult) responseType_,
                getParentForChildren(),
                isClean());
        responseType_ = null;
      }
      responseTypeCase_ = 10;
      onChanged();
      return getResourcesCommandResultBuilder_;
    }

    private com.google.protobuf.SingleFieldBuilderV3<
        org.apache.spark.connect.proto.StreamingQueryManagerCommandResult, org.apache.spark.connect.proto.StreamingQueryManagerCommandResult.Builder, org.apache.spark.connect.proto.StreamingQueryManagerCommandResultOrBuilder> streamingQueryManagerCommandResultBuilder_;
    /**
     * <pre>
     * Response for commands on the streaming query manager.
     * </pre>
     *
     * <code>.spark.connect.StreamingQueryManagerCommandResult streaming_query_manager_command_result = 11;</code>
     * @return Whether the streamingQueryManagerCommandResult field is set.
     */
    @Override
    public boolean hasStreamingQueryManagerCommandResult() {
      return responseTypeCase_ == 11;
    }
    /**
     * <pre>
     * Response for commands on the streaming query manager.
     * </pre>
     *
     * <code>.spark.connect.StreamingQueryManagerCommandResult streaming_query_manager_command_result = 11;</code>
     * @return The streamingQueryManagerCommandResult.
     */
    @Override
    public org.apache.spark.connect.proto.StreamingQueryManagerCommandResult getStreamingQueryManagerCommandResult() {
      if (streamingQueryManagerCommandResultBuilder_ == null) {
        if (responseTypeCase_ == 11) {
          return (org.apache.spark.connect.proto.StreamingQueryManagerCommandResult) responseType_;
        }
        return org.apache.spark.connect.proto.StreamingQueryManagerCommandResult.getDefaultInstance();
      } else {
        if (responseTypeCase_ == 11) {
          return streamingQueryManagerCommandResultBuilder_.getMessage();
        }
        return org.apache.spark.connect.proto.StreamingQueryManagerCommandResult.getDefaultInstance();
      }
    }
    /**
     * <pre>
     * Response for commands on the streaming query manager.
     * </pre>
     *
     * <code>.spark.connect.StreamingQueryManagerCommandResult streaming_query_manager_command_result = 11;</code>
     */
    public Builder setStreamingQueryManagerCommandResult(org.apache.spark.connect.proto.StreamingQueryManagerCommandResult value) {
      if (streamingQueryManagerCommandResultBuilder_ == null) {
        if (value == null) {
          throw new NullPointerException();
        }
        responseType_ = value;
        onChanged();
      } else {
        streamingQueryManagerCommandResultBuilder_.setMessage(value);
      }
      responseTypeCase_ = 11;
      return this;
    }
    /**
     * <pre>
     * Response for commands on the streaming query manager.
     * </pre>
     *
     * <code>.spark.connect.StreamingQueryManagerCommandResult streaming_query_manager_command_result = 11;</code>
     */
    public Builder setStreamingQueryManagerCommandResult(
        org.apache.spark.connect.proto.StreamingQueryManagerCommandResult.Builder builderForValue) {
      if (streamingQueryManagerCommandResultBuilder_ == null) {
        responseType_ = builderForValue.build();
        onChanged();
      } else {
        streamingQueryManagerCommandResultBuilder_.setMessage(builderForValue.build());
      }
      responseTypeCase_ = 11;
      return this;
    }
    /**
     * <pre>
     * Response for commands on the streaming query manager.
     * </pre>
     *
     * <code>.spark.connect.StreamingQueryManagerCommandResult streaming_query_manager_command_result = 11;</code>
     */
    public Builder mergeStreamingQueryManagerCommandResult(org.apache.spark.connect.proto.StreamingQueryManagerCommandResult value) {
      if (streamingQueryManagerCommandResultBuilder_ == null) {
        if (responseTypeCase_ == 11 &&
            responseType_ != org.apache.spark.connect.proto.StreamingQueryManagerCommandResult.getDefaultInstance()) {
          responseType_ = org.apache.spark.connect.proto.StreamingQueryManagerCommandResult.newBuilder((org.apache.spark.connect.proto.StreamingQueryManagerCommandResult) responseType_)
              .mergeFrom(value).buildPartial();
        } else {
          responseType_ = value;
        }
        onChanged();
      } else {
        if (responseTypeCase_ == 11) {
          streamingQueryManagerCommandResultBuilder_.mergeFrom(value);
        } else {
          streamingQueryManagerCommandResultBuilder_.setMessage(value);
        }
      }
      responseTypeCase_ = 11;
      return this;
    }
    /**
     * <pre>
     * Response for commands on the streaming query manager.
     * </pre>
     *
     * <code>.spark.connect.StreamingQueryManagerCommandResult streaming_query_manager_command_result = 11;</code>
     */
    public Builder clearStreamingQueryManagerCommandResult() {
      if (streamingQueryManagerCommandResultBuilder_ == null) {
        if (responseTypeCase_ == 11) {
          responseTypeCase_ = 0;
          responseType_ = null;
          onChanged();
        }
      } else {
        if (responseTypeCase_ == 11) {
          responseTypeCase_ = 0;
          responseType_ = null;
        }
        streamingQueryManagerCommandResultBuilder_.clear();
      }
      return this;
    }
    /**
     * <pre>
     * Response for commands on the streaming query manager.
     * </pre>
     *
     * <code>.spark.connect.StreamingQueryManagerCommandResult streaming_query_manager_command_result = 11;</code>
     */
    public org.apache.spark.connect.proto.StreamingQueryManagerCommandResult.Builder getStreamingQueryManagerCommandResultBuilder() {
      return getStreamingQueryManagerCommandResultFieldBuilder().getBuilder();
    }
    /**
     * <pre>
     * Response for commands on the streaming query manager.
     * </pre>
     *
     * <code>.spark.connect.StreamingQueryManagerCommandResult streaming_query_manager_command_result = 11;</code>
     */
    @Override
    public org.apache.spark.connect.proto.StreamingQueryManagerCommandResultOrBuilder getStreamingQueryManagerCommandResultOrBuilder() {
      if ((responseTypeCase_ == 11) && (streamingQueryManagerCommandResultBuilder_ != null)) {
        return streamingQueryManagerCommandResultBuilder_.getMessageOrBuilder();
      } else {
        if (responseTypeCase_ == 11) {
          return (org.apache.spark.connect.proto.StreamingQueryManagerCommandResult) responseType_;
        }
        return org.apache.spark.connect.proto.StreamingQueryManagerCommandResult.getDefaultInstance();
      }
    }
    /**
     * <pre>
     * Response for commands on the streaming query manager.
     * </pre>
     *
     * <code>.spark.connect.StreamingQueryManagerCommandResult streaming_query_manager_command_result = 11;</code>
     */
    private com.google.protobuf.SingleFieldBuilderV3<
        org.apache.spark.connect.proto.StreamingQueryManagerCommandResult, org.apache.spark.connect.proto.StreamingQueryManagerCommandResult.Builder, org.apache.spark.connect.proto.StreamingQueryManagerCommandResultOrBuilder> 
        getStreamingQueryManagerCommandResultFieldBuilder() {
      if (streamingQueryManagerCommandResultBuilder_ == null) {
        if (!(responseTypeCase_ == 11)) {
          responseType_ = org.apache.spark.connect.proto.StreamingQueryManagerCommandResult.getDefaultInstance();
        }
        streamingQueryManagerCommandResultBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
            org.apache.spark.connect.proto.StreamingQueryManagerCommandResult, org.apache.spark.connect.proto.StreamingQueryManagerCommandResult.Builder, org.apache.spark.connect.proto.StreamingQueryManagerCommandResultOrBuilder>(
                (org.apache.spark.connect.proto.StreamingQueryManagerCommandResult) responseType_,
                getParentForChildren(),
                isClean());
        responseType_ = null;
      }
      responseTypeCase_ = 11;
      onChanged();
      return streamingQueryManagerCommandResultBuilder_;
    }

    private com.google.protobuf.SingleFieldBuilderV3<
        org.apache.spark.connect.proto.ExecutePlanResponse.ResultComplete, org.apache.spark.connect.proto.ExecutePlanResponse.ResultComplete.Builder, org.apache.spark.connect.proto.ExecutePlanResponse.ResultCompleteOrBuilder> resultCompleteBuilder_;
    /**
     * <pre>
     * Response type informing if the stream is complete in reattachable execution.
     * </pre>
     *
     * <code>.spark.connect.ExecutePlanResponse.ResultComplete result_complete = 14;</code>
     * @return Whether the resultComplete field is set.
     */
    @Override
    public boolean hasResultComplete() {
      return responseTypeCase_ == 14;
    }
    /**
     * <pre>
     * Response type informing if the stream is complete in reattachable execution.
     * </pre>
     *
     * <code>.spark.connect.ExecutePlanResponse.ResultComplete result_complete = 14;</code>
     * @return The resultComplete.
     */
    @Override
    public org.apache.spark.connect.proto.ExecutePlanResponse.ResultComplete getResultComplete() {
      if (resultCompleteBuilder_ == null) {
        if (responseTypeCase_ == 14) {
          return (org.apache.spark.connect.proto.ExecutePlanResponse.ResultComplete) responseType_;
        }
        return org.apache.spark.connect.proto.ExecutePlanResponse.ResultComplete.getDefaultInstance();
      } else {
        if (responseTypeCase_ == 14) {
          return resultCompleteBuilder_.getMessage();
        }
        return org.apache.spark.connect.proto.ExecutePlanResponse.ResultComplete.getDefaultInstance();
      }
    }
    /**
     * <pre>
     * Response type informing if the stream is complete in reattachable execution.
     * </pre>
     *
     * <code>.spark.connect.ExecutePlanResponse.ResultComplete result_complete = 14;</code>
     */
    public Builder setResultComplete(org.apache.spark.connect.proto.ExecutePlanResponse.ResultComplete value) {
      if (resultCompleteBuilder_ == null) {
        if (value == null) {
          throw new NullPointerException();
        }
        responseType_ = value;
        onChanged();
      } else {
        resultCompleteBuilder_.setMessage(value);
      }
      responseTypeCase_ = 14;
      return this;
    }
    /**
     * <pre>
     * Response type informing if the stream is complete in reattachable execution.
     * </pre>
     *
     * <code>.spark.connect.ExecutePlanResponse.ResultComplete result_complete = 14;</code>
     */
    public Builder setResultComplete(
        org.apache.spark.connect.proto.ExecutePlanResponse.ResultComplete.Builder builderForValue) {
      if (resultCompleteBuilder_ == null) {
        responseType_ = builderForValue.build();
        onChanged();
      } else {
        resultCompleteBuilder_.setMessage(builderForValue.build());
      }
      responseTypeCase_ = 14;
      return this;
    }
    /**
     * <pre>
     * Response type informing if the stream is complete in reattachable execution.
     * </pre>
     *
     * <code>.spark.connect.ExecutePlanResponse.ResultComplete result_complete = 14;</code>
     */
    public Builder mergeResultComplete(org.apache.spark.connect.proto.ExecutePlanResponse.ResultComplete value) {
      if (resultCompleteBuilder_ == null) {
        if (responseTypeCase_ == 14 &&
            responseType_ != org.apache.spark.connect.proto.ExecutePlanResponse.ResultComplete.getDefaultInstance()) {
          responseType_ = org.apache.spark.connect.proto.ExecutePlanResponse.ResultComplete.newBuilder((org.apache.spark.connect.proto.ExecutePlanResponse.ResultComplete) responseType_)
              .mergeFrom(value).buildPartial();
        } else {
          responseType_ = value;
        }
        onChanged();
      } else {
        if (responseTypeCase_ == 14) {
          resultCompleteBuilder_.mergeFrom(value);
        } else {
          resultCompleteBuilder_.setMessage(value);
        }
      }
      responseTypeCase_ = 14;
      return this;
    }
    /**
     * <pre>
     * Response type informing if the stream is complete in reattachable execution.
     * </pre>
     *
     * <code>.spark.connect.ExecutePlanResponse.ResultComplete result_complete = 14;</code>
     */
    public Builder clearResultComplete() {
      if (resultCompleteBuilder_ == null) {
        if (responseTypeCase_ == 14) {
          responseTypeCase_ = 0;
          responseType_ = null;
          onChanged();
        }
      } else {
        if (responseTypeCase_ == 14) {
          responseTypeCase_ = 0;
          responseType_ = null;
        }
        resultCompleteBuilder_.clear();
      }
      return this;
    }
    /**
     * <pre>
     * Response type informing if the stream is complete in reattachable execution.
     * </pre>
     *
     * <code>.spark.connect.ExecutePlanResponse.ResultComplete result_complete = 14;</code>
     */
    public org.apache.spark.connect.proto.ExecutePlanResponse.ResultComplete.Builder getResultCompleteBuilder() {
      return getResultCompleteFieldBuilder().getBuilder();
    }
    /**
     * <pre>
     * Response type informing if the stream is complete in reattachable execution.
     * </pre>
     *
     * <code>.spark.connect.ExecutePlanResponse.ResultComplete result_complete = 14;</code>
     */
    @Override
    public org.apache.spark.connect.proto.ExecutePlanResponse.ResultCompleteOrBuilder getResultCompleteOrBuilder() {
      if ((responseTypeCase_ == 14) && (resultCompleteBuilder_ != null)) {
        return resultCompleteBuilder_.getMessageOrBuilder();
      } else {
        if (responseTypeCase_ == 14) {
          return (org.apache.spark.connect.proto.ExecutePlanResponse.ResultComplete) responseType_;
        }
        return org.apache.spark.connect.proto.ExecutePlanResponse.ResultComplete.getDefaultInstance();
      }
    }
    /**
     * <pre>
     * Response type informing if the stream is complete in reattachable execution.
     * </pre>
     *
     * <code>.spark.connect.ExecutePlanResponse.ResultComplete result_complete = 14;</code>
     */
    private com.google.protobuf.SingleFieldBuilderV3<
        org.apache.spark.connect.proto.ExecutePlanResponse.ResultComplete, org.apache.spark.connect.proto.ExecutePlanResponse.ResultComplete.Builder, org.apache.spark.connect.proto.ExecutePlanResponse.ResultCompleteOrBuilder> 
        getResultCompleteFieldBuilder() {
      if (resultCompleteBuilder_ == null) {
        if (!(responseTypeCase_ == 14)) {
          responseType_ = org.apache.spark.connect.proto.ExecutePlanResponse.ResultComplete.getDefaultInstance();
        }
        resultCompleteBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
            org.apache.spark.connect.proto.ExecutePlanResponse.ResultComplete, org.apache.spark.connect.proto.ExecutePlanResponse.ResultComplete.Builder, org.apache.spark.connect.proto.ExecutePlanResponse.ResultCompleteOrBuilder>(
                (org.apache.spark.connect.proto.ExecutePlanResponse.ResultComplete) responseType_,
                getParentForChildren(),
                isClean());
        responseType_ = null;
      }
      responseTypeCase_ = 14;
      onChanged();
      return resultCompleteBuilder_;
    }

    private com.google.protobuf.SingleFieldBuilderV3<
        com.google.protobuf.Any, com.google.protobuf.Any.Builder, com.google.protobuf.AnyOrBuilder> extensionBuilder_;
    /**
     * <pre>
     * Support arbitrary result objects.
     * </pre>
     *
     * <code>.google.protobuf.Any extension = 999;</code>
     * @return Whether the extension field is set.
     */
    @Override
    public boolean hasExtension() {
      return responseTypeCase_ == 999;
    }
    /**
     * <pre>
     * Support arbitrary result objects.
     * </pre>
     *
     * <code>.google.protobuf.Any extension = 999;</code>
     * @return The extension.
     */
    @Override
    public com.google.protobuf.Any getExtension() {
      if (extensionBuilder_ == null) {
        if (responseTypeCase_ == 999) {
          return (com.google.protobuf.Any) responseType_;
        }
        return com.google.protobuf.Any.getDefaultInstance();
      } else {
        if (responseTypeCase_ == 999) {
          return extensionBuilder_.getMessage();
        }
        return com.google.protobuf.Any.getDefaultInstance();
      }
    }
    /**
     * <pre>
     * Support arbitrary result objects.
     * </pre>
     *
     * <code>.google.protobuf.Any extension = 999;</code>
     */
    public Builder setExtension(com.google.protobuf.Any value) {
      if (extensionBuilder_ == null) {
        if (value == null) {
          throw new NullPointerException();
        }
        responseType_ = value;
        onChanged();
      } else {
        extensionBuilder_.setMessage(value);
      }
      responseTypeCase_ = 999;
      return this;
    }
    /**
     * <pre>
     * Support arbitrary result objects.
     * </pre>
     *
     * <code>.google.protobuf.Any extension = 999;</code>
     */
    public Builder setExtension(
        com.google.protobuf.Any.Builder builderForValue) {
      if (extensionBuilder_ == null) {
        responseType_ = builderForValue.build();
        onChanged();
      } else {
        extensionBuilder_.setMessage(builderForValue.build());
      }
      responseTypeCase_ = 999;
      return this;
    }
    /**
     * <pre>
     * Support arbitrary result objects.
     * </pre>
     *
     * <code>.google.protobuf.Any extension = 999;</code>
     */
    public Builder mergeExtension(com.google.protobuf.Any value) {
      if (extensionBuilder_ == null) {
        if (responseTypeCase_ == 999 &&
            responseType_ != com.google.protobuf.Any.getDefaultInstance()) {
          responseType_ = com.google.protobuf.Any.newBuilder((com.google.protobuf.Any) responseType_)
              .mergeFrom(value).buildPartial();
        } else {
          responseType_ = value;
        }
        onChanged();
      } else {
        if (responseTypeCase_ == 999) {
          extensionBuilder_.mergeFrom(value);
        } else {
          extensionBuilder_.setMessage(value);
        }
      }
      responseTypeCase_ = 999;
      return this;
    }
    /**
     * <pre>
     * Support arbitrary result objects.
     * </pre>
     *
     * <code>.google.protobuf.Any extension = 999;</code>
     */
    public Builder clearExtension() {
      if (extensionBuilder_ == null) {
        if (responseTypeCase_ == 999) {
          responseTypeCase_ = 0;
          responseType_ = null;
          onChanged();
        }
      } else {
        if (responseTypeCase_ == 999) {
          responseTypeCase_ = 0;
          responseType_ = null;
        }
        extensionBuilder_.clear();
      }
      return this;
    }
    /**
     * <pre>
     * Support arbitrary result objects.
     * </pre>
     *
     * <code>.google.protobuf.Any extension = 999;</code>
     */
    public com.google.protobuf.Any.Builder getExtensionBuilder() {
      return getExtensionFieldBuilder().getBuilder();
    }
    /**
     * <pre>
     * Support arbitrary result objects.
     * </pre>
     *
     * <code>.google.protobuf.Any extension = 999;</code>
     */
    @Override
    public com.google.protobuf.AnyOrBuilder getExtensionOrBuilder() {
      if ((responseTypeCase_ == 999) && (extensionBuilder_ != null)) {
        return extensionBuilder_.getMessageOrBuilder();
      } else {
        if (responseTypeCase_ == 999) {
          return (com.google.protobuf.Any) responseType_;
        }
        return com.google.protobuf.Any.getDefaultInstance();
      }
    }
    /**
     * <pre>
     * Support arbitrary result objects.
     * </pre>
     *
     * <code>.google.protobuf.Any extension = 999;</code>
     */
    private com.google.protobuf.SingleFieldBuilderV3<
        com.google.protobuf.Any, com.google.protobuf.Any.Builder, com.google.protobuf.AnyOrBuilder> 
        getExtensionFieldBuilder() {
      if (extensionBuilder_ == null) {
        if (!(responseTypeCase_ == 999)) {
          responseType_ = com.google.protobuf.Any.getDefaultInstance();
        }
        extensionBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
            com.google.protobuf.Any, com.google.protobuf.Any.Builder, com.google.protobuf.AnyOrBuilder>(
                (com.google.protobuf.Any) responseType_,
                getParentForChildren(),
                isClean());
        responseType_ = null;
      }
      responseTypeCase_ = 999;
      onChanged();
      return extensionBuilder_;
    }

    private org.apache.spark.connect.proto.ExecutePlanResponse.Metrics metrics_;
    private com.google.protobuf.SingleFieldBuilderV3<
        org.apache.spark.connect.proto.ExecutePlanResponse.Metrics, org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.Builder, org.apache.spark.connect.proto.ExecutePlanResponse.MetricsOrBuilder> metricsBuilder_;
    /**
     * <pre>
     * Metrics for the query execution. Typically, this field is only present in the last
     * batch of results and then represent the overall state of the query execution.
     * </pre>
     *
     * <code>.spark.connect.ExecutePlanResponse.Metrics metrics = 4;</code>
     * @return Whether the metrics field is set.
     */
    public boolean hasMetrics() {
      return ((bitField0_ & 0x00000800) != 0);
    }
    /**
     * <pre>
     * Metrics for the query execution. Typically, this field is only present in the last
     * batch of results and then represent the overall state of the query execution.
     * </pre>
     *
     * <code>.spark.connect.ExecutePlanResponse.Metrics metrics = 4;</code>
     * @return The metrics.
     */
    public org.apache.spark.connect.proto.ExecutePlanResponse.Metrics getMetrics() {
      if (metricsBuilder_ == null) {
        return metrics_ == null ? org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.getDefaultInstance() : metrics_;
      } else {
        return metricsBuilder_.getMessage();
      }
    }
    /**
     * <pre>
     * Metrics for the query execution. Typically, this field is only present in the last
     * batch of results and then represent the overall state of the query execution.
     * </pre>
     *
     * <code>.spark.connect.ExecutePlanResponse.Metrics metrics = 4;</code>
     */
    public Builder setMetrics(org.apache.spark.connect.proto.ExecutePlanResponse.Metrics value) {
      if (metricsBuilder_ == null) {
        if (value == null) {
          throw new NullPointerException();
        }
        metrics_ = value;
      } else {
        metricsBuilder_.setMessage(value);
      }
      bitField0_ |= 0x00000800;
      onChanged();
      return this;
    }
    /**
     * <pre>
     * Metrics for the query execution. Typically, this field is only present in the last
     * batch of results and then represent the overall state of the query execution.
     * </pre>
     *
     * <code>.spark.connect.ExecutePlanResponse.Metrics metrics = 4;</code>
     */
    public Builder setMetrics(
        org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.Builder builderForValue) {
      if (metricsBuilder_ == null) {
        metrics_ = builderForValue.build();
      } else {
        metricsBuilder_.setMessage(builderForValue.build());
      }
      bitField0_ |= 0x00000800;
      onChanged();
      return this;
    }
    /**
     * <pre>
     * Metrics for the query execution. Typically, this field is only present in the last
     * batch of results and then represent the overall state of the query execution.
     * </pre>
     *
     * <code>.spark.connect.ExecutePlanResponse.Metrics metrics = 4;</code>
     */
    public Builder mergeMetrics(org.apache.spark.connect.proto.ExecutePlanResponse.Metrics value) {
      if (metricsBuilder_ == null) {
        if (((bitField0_ & 0x00000800) != 0) &&
          metrics_ != null &&
          metrics_ != org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.getDefaultInstance()) {
          getMetricsBuilder().mergeFrom(value);
        } else {
          metrics_ = value;
        }
      } else {
        metricsBuilder_.mergeFrom(value);
      }
      bitField0_ |= 0x00000800;
      onChanged();
      return this;
    }
    /**
     * <pre>
     * Metrics for the query execution. Typically, this field is only present in the last
     * batch of results and then represent the overall state of the query execution.
     * </pre>
     *
     * <code>.spark.connect.ExecutePlanResponse.Metrics metrics = 4;</code>
     */
    public Builder clearMetrics() {
      bitField0_ = (bitField0_ & ~0x00000800);
      metrics_ = null;
      if (metricsBuilder_ != null) {
        metricsBuilder_.dispose();
        metricsBuilder_ = null;
      }
      onChanged();
      return this;
    }
    /**
     * <pre>
     * Metrics for the query execution. Typically, this field is only present in the last
     * batch of results and then represent the overall state of the query execution.
     * </pre>
     *
     * <code>.spark.connect.ExecutePlanResponse.Metrics metrics = 4;</code>
     */
    public org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.Builder getMetricsBuilder() {
      bitField0_ |= 0x00000800;
      onChanged();
      return getMetricsFieldBuilder().getBuilder();
    }
    /**
     * <pre>
     * Metrics for the query execution. Typically, this field is only present in the last
     * batch of results and then represent the overall state of the query execution.
     * </pre>
     *
     * <code>.spark.connect.ExecutePlanResponse.Metrics metrics = 4;</code>
     */
    public org.apache.spark.connect.proto.ExecutePlanResponse.MetricsOrBuilder getMetricsOrBuilder() {
      if (metricsBuilder_ != null) {
        return metricsBuilder_.getMessageOrBuilder();
      } else {
        return metrics_ == null ?
            org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.getDefaultInstance() : metrics_;
      }
    }
    /**
     * <pre>
     * Metrics for the query execution. Typically, this field is only present in the last
     * batch of results and then represent the overall state of the query execution.
     * </pre>
     *
     * <code>.spark.connect.ExecutePlanResponse.Metrics metrics = 4;</code>
     */
    private com.google.protobuf.SingleFieldBuilderV3<
        org.apache.spark.connect.proto.ExecutePlanResponse.Metrics, org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.Builder, org.apache.spark.connect.proto.ExecutePlanResponse.MetricsOrBuilder> 
        getMetricsFieldBuilder() {
      if (metricsBuilder_ == null) {
        metricsBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
            org.apache.spark.connect.proto.ExecutePlanResponse.Metrics, org.apache.spark.connect.proto.ExecutePlanResponse.Metrics.Builder, org.apache.spark.connect.proto.ExecutePlanResponse.MetricsOrBuilder>(
                getMetrics(),
                getParentForChildren(),
                isClean());
        metrics_ = null;
      }
      return metricsBuilder_;
    }

    private java.util.List<org.apache.spark.connect.proto.ExecutePlanResponse.ObservedMetrics> observedMetrics_ =
      java.util.Collections.emptyList();
    private void ensureObservedMetricsIsMutable() {
      if (!((bitField0_ & 0x00001000) != 0)) {
        observedMetrics_ = new java.util.ArrayList<org.apache.spark.connect.proto.ExecutePlanResponse.ObservedMetrics>(observedMetrics_);
        bitField0_ |= 0x00001000;
       }
    }

    private com.google.protobuf.RepeatedFieldBuilderV3<
        org.apache.spark.connect.proto.ExecutePlanResponse.ObservedMetrics, org.apache.spark.connect.proto.ExecutePlanResponse.ObservedMetrics.Builder, org.apache.spark.connect.proto.ExecutePlanResponse.ObservedMetricsOrBuilder> observedMetricsBuilder_;

    /**
     * <pre>
     * The metrics observed during the execution of the query plan.
     * </pre>
     *
     * <code>repeated .spark.connect.ExecutePlanResponse.ObservedMetrics observed_metrics = 6;</code>
     */
    public java.util.List<org.apache.spark.connect.proto.ExecutePlanResponse.ObservedMetrics> getObservedMetricsList() {
      if (observedMetricsBuilder_ == null) {
        return java.util.Collections.unmodifiableList(observedMetrics_);
      } else {
        return observedMetricsBuilder_.getMessageList();
      }
    }
    /**
     * <pre>
     * The metrics observed during the execution of the query plan.
     * </pre>
     *
     * <code>repeated .spark.connect.ExecutePlanResponse.ObservedMetrics observed_metrics = 6;</code>
     */
    public int getObservedMetricsCount() {
      if (observedMetricsBuilder_ == null) {
        return observedMetrics_.size();
      } else {
        return observedMetricsBuilder_.getCount();
      }
    }
    /**
     * <pre>
     * The metrics observed during the execution of the query plan.
     * </pre>
     *
     * <code>repeated .spark.connect.ExecutePlanResponse.ObservedMetrics observed_metrics = 6;</code>
     */
    public org.apache.spark.connect.proto.ExecutePlanResponse.ObservedMetrics getObservedMetrics(int index) {
      if (observedMetricsBuilder_ == null) {
        return observedMetrics_.get(index);
      } else {
        return observedMetricsBuilder_.getMessage(index);
      }
    }
    /**
     * <pre>
     * The metrics observed during the execution of the query plan.
     * </pre>
     *
     * <code>repeated .spark.connect.ExecutePlanResponse.ObservedMetrics observed_metrics = 6;</code>
     */
    public Builder setObservedMetrics(
        int index, org.apache.spark.connect.proto.ExecutePlanResponse.ObservedMetrics value) {
      if (observedMetricsBuilder_ == null) {
        if (value == null) {
          throw new NullPointerException();
        }
        ensureObservedMetricsIsMutable();
        observedMetrics_.set(index, value);
        onChanged();
      } else {
        observedMetricsBuilder_.setMessage(index, value);
      }
      return this;
    }
    /**
     * <pre>
     * The metrics observed during the execution of the query plan.
     * </pre>
     *
     * <code>repeated .spark.connect.ExecutePlanResponse.ObservedMetrics observed_metrics = 6;</code>
     */
    public Builder setObservedMetrics(
        int index, org.apache.spark.connect.proto.ExecutePlanResponse.ObservedMetrics.Builder builderForValue) {
      if (observedMetricsBuilder_ == null) {
        ensureObservedMetricsIsMutable();
        observedMetrics_.set(index, builderForValue.build());
        onChanged();
      } else {
        observedMetricsBuilder_.setMessage(index, builderForValue.build());
      }
      return this;
    }
    /**
     * <pre>
     * The metrics observed during the execution of the query plan.
     * </pre>
     *
     * <code>repeated .spark.connect.ExecutePlanResponse.ObservedMetrics observed_metrics = 6;</code>
     */
    public Builder addObservedMetrics(org.apache.spark.connect.proto.ExecutePlanResponse.ObservedMetrics value) {
      if (observedMetricsBuilder_ == null) {
        if (value == null) {
          throw new NullPointerException();
        }
        ensureObservedMetricsIsMutable();
        observedMetrics_.add(value);
        onChanged();
      } else {
        observedMetricsBuilder_.addMessage(value);
      }
      return this;
    }
    /**
     * <pre>
     * The metrics observed during the execution of the query plan.
     * </pre>
     *
     * <code>repeated .spark.connect.ExecutePlanResponse.ObservedMetrics observed_metrics = 6;</code>
     */
    public Builder addObservedMetrics(
        int index, org.apache.spark.connect.proto.ExecutePlanResponse.ObservedMetrics value) {
      if (observedMetricsBuilder_ == null) {
        if (value == null) {
          throw new NullPointerException();
        }
        ensureObservedMetricsIsMutable();
        observedMetrics_.add(index, value);
        onChanged();
      } else {
        observedMetricsBuilder_.addMessage(index, value);
      }
      return this;
    }
    /**
     * <pre>
     * The metrics observed during the execution of the query plan.
     * </pre>
     *
     * <code>repeated .spark.connect.ExecutePlanResponse.ObservedMetrics observed_metrics = 6;</code>
     */
    public Builder addObservedMetrics(
        org.apache.spark.connect.proto.ExecutePlanResponse.ObservedMetrics.Builder builderForValue) {
      if (observedMetricsBuilder_ == null) {
        ensureObservedMetricsIsMutable();
        observedMetrics_.add(builderForValue.build());
        onChanged();
      } else {
        observedMetricsBuilder_.addMessage(builderForValue.build());
      }
      return this;
    }
    /**
     * <pre>
     * The metrics observed during the execution of the query plan.
     * </pre>
     *
     * <code>repeated .spark.connect.ExecutePlanResponse.ObservedMetrics observed_metrics = 6;</code>
     */
    public Builder addObservedMetrics(
        int index, org.apache.spark.connect.proto.ExecutePlanResponse.ObservedMetrics.Builder builderForValue) {
      if (observedMetricsBuilder_ == null) {
        ensureObservedMetricsIsMutable();
        observedMetrics_.add(index, builderForValue.build());
        onChanged();
      } else {
        observedMetricsBuilder_.addMessage(index, builderForValue.build());
      }
      return this;
    }
    /**
     * <pre>
     * The metrics observed during the execution of the query plan.
     * </pre>
     *
     * <code>repeated .spark.connect.ExecutePlanResponse.ObservedMetrics observed_metrics = 6;</code>
     */
    public Builder addAllObservedMetrics(
        Iterable<? extends org.apache.spark.connect.proto.ExecutePlanResponse.ObservedMetrics> values) {
      if (observedMetricsBuilder_ == null) {
        ensureObservedMetricsIsMutable();
        com.google.protobuf.AbstractMessageLite.Builder.addAll(
            values, observedMetrics_);
        onChanged();
      } else {
        observedMetricsBuilder_.addAllMessages(values);
      }
      return this;
    }
    /**
     * <pre>
     * The metrics observed during the execution of the query plan.
     * </pre>
     *
     * <code>repeated .spark.connect.ExecutePlanResponse.ObservedMetrics observed_metrics = 6;</code>
     */
    public Builder clearObservedMetrics() {
      if (observedMetricsBuilder_ == null) {
        observedMetrics_ = java.util.Collections.emptyList();
        bitField0_ = (bitField0_ & ~0x00001000);
        onChanged();
      } else {
        observedMetricsBuilder_.clear();
      }
      return this;
    }
    /**
     * <pre>
     * The metrics observed during the execution of the query plan.
     * </pre>
     *
     * <code>repeated .spark.connect.ExecutePlanResponse.ObservedMetrics observed_metrics = 6;</code>
     */
    public Builder removeObservedMetrics(int index) {
      if (observedMetricsBuilder_ == null) {
        ensureObservedMetricsIsMutable();
        observedMetrics_.remove(index);
        onChanged();
      } else {
        observedMetricsBuilder_.remove(index);
      }
      return this;
    }
    /**
     * <pre>
     * The metrics observed during the execution of the query plan.
     * </pre>
     *
     * <code>repeated .spark.connect.ExecutePlanResponse.ObservedMetrics observed_metrics = 6;</code>
     */
    public org.apache.spark.connect.proto.ExecutePlanResponse.ObservedMetrics.Builder getObservedMetricsBuilder(
        int index) {
      return getObservedMetricsFieldBuilder().getBuilder(index);
    }
    /**
     * <pre>
     * The metrics observed during the execution of the query plan.
     * </pre>
     *
     * <code>repeated .spark.connect.ExecutePlanResponse.ObservedMetrics observed_metrics = 6;</code>
     */
    public org.apache.spark.connect.proto.ExecutePlanResponse.ObservedMetricsOrBuilder getObservedMetricsOrBuilder(
        int index) {
      if (observedMetricsBuilder_ == null) {
        return observedMetrics_.get(index);  } else {
        return observedMetricsBuilder_.getMessageOrBuilder(index);
      }
    }
    /**
     * <pre>
     * The metrics observed during the execution of the query plan.
     * </pre>
     *
     * <code>repeated .spark.connect.ExecutePlanResponse.ObservedMetrics observed_metrics = 6;</code>
     */
    public java.util.List<? extends org.apache.spark.connect.proto.ExecutePlanResponse.ObservedMetricsOrBuilder> 
         getObservedMetricsOrBuilderList() {
      if (observedMetricsBuilder_ != null) {
        return observedMetricsBuilder_.getMessageOrBuilderList();
      } else {
        return java.util.Collections.unmodifiableList(observedMetrics_);
      }
    }
    /**
     * <pre>
     * The metrics observed during the execution of the query plan.
     * </pre>
     *
     * <code>repeated .spark.connect.ExecutePlanResponse.ObservedMetrics observed_metrics = 6;</code>
     */
    public org.apache.spark.connect.proto.ExecutePlanResponse.ObservedMetrics.Builder addObservedMetricsBuilder() {
      return getObservedMetricsFieldBuilder().addBuilder(
          org.apache.spark.connect.proto.ExecutePlanResponse.ObservedMetrics.getDefaultInstance());
    }
    /**
     * <pre>
     * The metrics observed during the execution of the query plan.
     * </pre>
     *
     * <code>repeated .spark.connect.ExecutePlanResponse.ObservedMetrics observed_metrics = 6;</code>
     */
    public org.apache.spark.connect.proto.ExecutePlanResponse.ObservedMetrics.Builder addObservedMetricsBuilder(
        int index) {
      return getObservedMetricsFieldBuilder().addBuilder(
          index, org.apache.spark.connect.proto.ExecutePlanResponse.ObservedMetrics.getDefaultInstance());
    }
    /**
     * <pre>
     * The metrics observed during the execution of the query plan.
     * </pre>
     *
     * <code>repeated .spark.connect.ExecutePlanResponse.ObservedMetrics observed_metrics = 6;</code>
     */
    public java.util.List<org.apache.spark.connect.proto.ExecutePlanResponse.ObservedMetrics.Builder> 
         getObservedMetricsBuilderList() {
      return getObservedMetricsFieldBuilder().getBuilderList();
    }
    private com.google.protobuf.RepeatedFieldBuilderV3<
        org.apache.spark.connect.proto.ExecutePlanResponse.ObservedMetrics, org.apache.spark.connect.proto.ExecutePlanResponse.ObservedMetrics.Builder, org.apache.spark.connect.proto.ExecutePlanResponse.ObservedMetricsOrBuilder> 
        getObservedMetricsFieldBuilder() {
      if (observedMetricsBuilder_ == null) {
        observedMetricsBuilder_ = new com.google.protobuf.RepeatedFieldBuilderV3<
            org.apache.spark.connect.proto.ExecutePlanResponse.ObservedMetrics, org.apache.spark.connect.proto.ExecutePlanResponse.ObservedMetrics.Builder, org.apache.spark.connect.proto.ExecutePlanResponse.ObservedMetricsOrBuilder>(
                observedMetrics_,
                ((bitField0_ & 0x00001000) != 0),
                getParentForChildren(),
                isClean());
        observedMetrics_ = null;
      }
      return observedMetricsBuilder_;
    }

    private org.apache.spark.connect.proto.DataType schema_;
    private com.google.protobuf.SingleFieldBuilderV3<
        org.apache.spark.connect.proto.DataType, org.apache.spark.connect.proto.DataType.Builder, org.apache.spark.connect.proto.DataTypeOrBuilder> schemaBuilder_;
    /**
     * <pre>
     * (Optional) The Spark schema. This field is available when `collect` is called.
     * </pre>
     *
     * <code>.spark.connect.DataType schema = 7;</code>
     * @return Whether the schema field is set.
     */
    public boolean hasSchema() {
      return ((bitField0_ & 0x00002000) != 0);
    }
    /**
     * <pre>
     * (Optional) The Spark schema. This field is available when `collect` is called.
     * </pre>
     *
     * <code>.spark.connect.DataType schema = 7;</code>
     * @return The schema.
     */
    public org.apache.spark.connect.proto.DataType getSchema() {
      if (schemaBuilder_ == null) {
        return schema_ == null ? org.apache.spark.connect.proto.DataType.getDefaultInstance() : schema_;
      } else {
        return schemaBuilder_.getMessage();
      }
    }
    /**
     * <pre>
     * (Optional) The Spark schema. This field is available when `collect` is called.
     * </pre>
     *
     * <code>.spark.connect.DataType schema = 7;</code>
     */
    public Builder setSchema(org.apache.spark.connect.proto.DataType value) {
      if (schemaBuilder_ == null) {
        if (value == null) {
          throw new NullPointerException();
        }
        schema_ = value;
      } else {
        schemaBuilder_.setMessage(value);
      }
      bitField0_ |= 0x00002000;
      onChanged();
      return this;
    }
    /**
     * <pre>
     * (Optional) The Spark schema. This field is available when `collect` is called.
     * </pre>
     *
     * <code>.spark.connect.DataType schema = 7;</code>
     */
    public Builder setSchema(
        org.apache.spark.connect.proto.DataType.Builder builderForValue) {
      if (schemaBuilder_ == null) {
        schema_ = builderForValue.build();
      } else {
        schemaBuilder_.setMessage(builderForValue.build());
      }
      bitField0_ |= 0x00002000;
      onChanged();
      return this;
    }
    /**
     * <pre>
     * (Optional) The Spark schema. This field is available when `collect` is called.
     * </pre>
     *
     * <code>.spark.connect.DataType schema = 7;</code>
     */
    public Builder mergeSchema(org.apache.spark.connect.proto.DataType value) {
      if (schemaBuilder_ == null) {
        if (((bitField0_ & 0x00002000) != 0) &&
          schema_ != null &&
          schema_ != org.apache.spark.connect.proto.DataType.getDefaultInstance()) {
          getSchemaBuilder().mergeFrom(value);
        } else {
          schema_ = value;
        }
      } else {
        schemaBuilder_.mergeFrom(value);
      }
      bitField0_ |= 0x00002000;
      onChanged();
      return this;
    }
    /**
     * <pre>
     * (Optional) The Spark schema. This field is available when `collect` is called.
     * </pre>
     *
     * <code>.spark.connect.DataType schema = 7;</code>
     */
    public Builder clearSchema() {
      bitField0_ = (bitField0_ & ~0x00002000);
      schema_ = null;
      if (schemaBuilder_ != null) {
        schemaBuilder_.dispose();
        schemaBuilder_ = null;
      }
      onChanged();
      return this;
    }
    /**
     * <pre>
     * (Optional) The Spark schema. This field is available when `collect` is called.
     * </pre>
     *
     * <code>.spark.connect.DataType schema = 7;</code>
     */
    public org.apache.spark.connect.proto.DataType.Builder getSchemaBuilder() {
      bitField0_ |= 0x00002000;
      onChanged();
      return getSchemaFieldBuilder().getBuilder();
    }
    /**
     * <pre>
     * (Optional) The Spark schema. This field is available when `collect` is called.
     * </pre>
     *
     * <code>.spark.connect.DataType schema = 7;</code>
     */
    public org.apache.spark.connect.proto.DataTypeOrBuilder getSchemaOrBuilder() {
      if (schemaBuilder_ != null) {
        return schemaBuilder_.getMessageOrBuilder();
      } else {
        return schema_ == null ?
            org.apache.spark.connect.proto.DataType.getDefaultInstance() : schema_;
      }
    }
    /**
     * <pre>
     * (Optional) The Spark schema. This field is available when `collect` is called.
     * </pre>
     *
     * <code>.spark.connect.DataType schema = 7;</code>
     */
    private com.google.protobuf.SingleFieldBuilderV3<
        org.apache.spark.connect.proto.DataType, org.apache.spark.connect.proto.DataType.Builder, org.apache.spark.connect.proto.DataTypeOrBuilder> 
        getSchemaFieldBuilder() {
      if (schemaBuilder_ == null) {
        schemaBuilder_ = new com.google.protobuf.SingleFieldBuilderV3<
            org.apache.spark.connect.proto.DataType, org.apache.spark.connect.proto.DataType.Builder, org.apache.spark.connect.proto.DataTypeOrBuilder>(
                getSchema(),
                getParentForChildren(),
                isClean());
        schema_ = null;
      }
      return schemaBuilder_;
    }
    @Override
    public final Builder setUnknownFields(
        final com.google.protobuf.UnknownFieldSet unknownFields) {
      return super.setUnknownFields(unknownFields);
    }

    @Override
    public final Builder mergeUnknownFields(
        final com.google.protobuf.UnknownFieldSet unknownFields) {
      return super.mergeUnknownFields(unknownFields);
    }


    // @@protoc_insertion_point(builder_scope:spark.connect.ExecutePlanResponse)
  }

  // @@protoc_insertion_point(class_scope:spark.connect.ExecutePlanResponse)
  private static final org.apache.spark.connect.proto.ExecutePlanResponse DEFAULT_INSTANCE;
  static {
    DEFAULT_INSTANCE = new org.apache.spark.connect.proto.ExecutePlanResponse();
  }

  public static org.apache.spark.connect.proto.ExecutePlanResponse getDefaultInstance() {
    return DEFAULT_INSTANCE;
  }

  private static final com.google.protobuf.Parser<ExecutePlanResponse>
      PARSER = new com.google.protobuf.AbstractParser<ExecutePlanResponse>() {
    @Override
    public ExecutePlanResponse parsePartialFrom(
        com.google.protobuf.CodedInputStream input,
        com.google.protobuf.ExtensionRegistryLite extensionRegistry)
        throws com.google.protobuf.InvalidProtocolBufferException {
      Builder builder = newBuilder();
      try {
        builder.mergeFrom(input, extensionRegistry);
      } catch (com.google.protobuf.InvalidProtocolBufferException e) {
        throw e.setUnfinishedMessage(builder.buildPartial());
      } catch (com.google.protobuf.UninitializedMessageException e) {
        throw e.asInvalidProtocolBufferException().setUnfinishedMessage(builder.buildPartial());
      } catch (java.io.IOException e) {
        throw new com.google.protobuf.InvalidProtocolBufferException(e)
            .setUnfinishedMessage(builder.buildPartial());
      }
      return builder.buildPartial();
    }
  };

  public static com.google.protobuf.Parser<ExecutePlanResponse> parser() {
    return PARSER;
  }

  @Override
  public com.google.protobuf.Parser<ExecutePlanResponse> getParserForType() {
    return PARSER;
  }

  @Override
  public org.apache.spark.connect.proto.ExecutePlanResponse getDefaultInstanceForType() {
    return DEFAULT_INSTANCE;
  }

}


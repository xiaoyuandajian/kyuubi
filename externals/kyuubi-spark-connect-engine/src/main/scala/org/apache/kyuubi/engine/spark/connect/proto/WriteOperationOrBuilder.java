// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: spark/connect/commands.proto

package org.apache.kyuubi.engine.spark.connect.proto;

public interface WriteOperationOrBuilder extends
    // @@protoc_insertion_point(interface_extends:spark.connect.WriteOperation)
    com.google.protobuf.MessageOrBuilder {

  /**
   * <pre>
   * (Required) The output of the `input` relation will be persisted according to the options.
   * </pre>
   *
   * <code>.spark.connect.Relation input = 1;</code>
   * @return Whether the input field is set.
   */
  boolean hasInput();
  /**
   * <pre>
   * (Required) The output of the `input` relation will be persisted according to the options.
   * </pre>
   *
   * <code>.spark.connect.Relation input = 1;</code>
   * @return The input.
   */
  org.apache.spark.connect.proto.Relation getInput();
  /**
   * <pre>
   * (Required) The output of the `input` relation will be persisted according to the options.
   * </pre>
   *
   * <code>.spark.connect.Relation input = 1;</code>
   */
  org.apache.spark.connect.proto.RelationOrBuilder getInputOrBuilder();

  /**
   * <pre>
   * (Optional) Format value according to the Spark documentation. Examples are: text, parquet, delta.
   * </pre>
   *
   * <code>optional string source = 2;</code>
   * @return Whether the source field is set.
   */
  boolean hasSource();
  /**
   * <pre>
   * (Optional) Format value according to the Spark documentation. Examples are: text, parquet, delta.
   * </pre>
   *
   * <code>optional string source = 2;</code>
   * @return The source.
   */
  String getSource();
  /**
   * <pre>
   * (Optional) Format value according to the Spark documentation. Examples are: text, parquet, delta.
   * </pre>
   *
   * <code>optional string source = 2;</code>
   * @return The bytes for source.
   */
  com.google.protobuf.ByteString
      getSourceBytes();

  /**
   * <code>string path = 3;</code>
   * @return Whether the path field is set.
   */
  boolean hasPath();
  /**
   * <code>string path = 3;</code>
   * @return The path.
   */
  String getPath();
  /**
   * <code>string path = 3;</code>
   * @return The bytes for path.
   */
  com.google.protobuf.ByteString
      getPathBytes();

  /**
   * <code>.spark.connect.WriteOperation.SaveTable table = 4;</code>
   * @return Whether the table field is set.
   */
  boolean hasTable();
  /**
   * <code>.spark.connect.WriteOperation.SaveTable table = 4;</code>
   * @return The table.
   */
  org.apache.spark.connect.proto.WriteOperation.SaveTable getTable();
  /**
   * <code>.spark.connect.WriteOperation.SaveTable table = 4;</code>
   */
  org.apache.spark.connect.proto.WriteOperation.SaveTableOrBuilder getTableOrBuilder();

  /**
   * <pre>
   * (Required) the save mode.
   * </pre>
   *
   * <code>.spark.connect.WriteOperation.SaveMode mode = 5;</code>
   * @return The enum numeric value on the wire for mode.
   */
  int getModeValue();
  /**
   * <pre>
   * (Required) the save mode.
   * </pre>
   *
   * <code>.spark.connect.WriteOperation.SaveMode mode = 5;</code>
   * @return The mode.
   */
  org.apache.spark.connect.proto.WriteOperation.SaveMode getMode();

  /**
   * <pre>
   * (Optional) List of columns to sort the output by.
   * </pre>
   *
   * <code>repeated string sort_column_names = 6;</code>
   * @return A list containing the sortColumnNames.
   */
  java.util.List<String>
      getSortColumnNamesList();
  /**
   * <pre>
   * (Optional) List of columns to sort the output by.
   * </pre>
   *
   * <code>repeated string sort_column_names = 6;</code>
   * @return The count of sortColumnNames.
   */
  int getSortColumnNamesCount();
  /**
   * <pre>
   * (Optional) List of columns to sort the output by.
   * </pre>
   *
   * <code>repeated string sort_column_names = 6;</code>
   * @param index The index of the element to return.
   * @return The sortColumnNames at the given index.
   */
  String getSortColumnNames(int index);
  /**
   * <pre>
   * (Optional) List of columns to sort the output by.
   * </pre>
   *
   * <code>repeated string sort_column_names = 6;</code>
   * @param index The index of the value to return.
   * @return The bytes of the sortColumnNames at the given index.
   */
  com.google.protobuf.ByteString
      getSortColumnNamesBytes(int index);

  /**
   * <pre>
   * (Optional) List of columns for partitioning.
   * </pre>
   *
   * <code>repeated string partitioning_columns = 7;</code>
   * @return A list containing the partitioningColumns.
   */
  java.util.List<String>
      getPartitioningColumnsList();
  /**
   * <pre>
   * (Optional) List of columns for partitioning.
   * </pre>
   *
   * <code>repeated string partitioning_columns = 7;</code>
   * @return The count of partitioningColumns.
   */
  int getPartitioningColumnsCount();
  /**
   * <pre>
   * (Optional) List of columns for partitioning.
   * </pre>
   *
   * <code>repeated string partitioning_columns = 7;</code>
   * @param index The index of the element to return.
   * @return The partitioningColumns at the given index.
   */
  String getPartitioningColumns(int index);
  /**
   * <pre>
   * (Optional) List of columns for partitioning.
   * </pre>
   *
   * <code>repeated string partitioning_columns = 7;</code>
   * @param index The index of the value to return.
   * @return The bytes of the partitioningColumns at the given index.
   */
  com.google.protobuf.ByteString
      getPartitioningColumnsBytes(int index);

  /**
   * <pre>
   * (Optional) Bucketing specification. Bucketing must set the number of buckets and the columns
   * to bucket by.
   * </pre>
   *
   * <code>.spark.connect.WriteOperation.BucketBy bucket_by = 8;</code>
   * @return Whether the bucketBy field is set.
   */
  boolean hasBucketBy();
  /**
   * <pre>
   * (Optional) Bucketing specification. Bucketing must set the number of buckets and the columns
   * to bucket by.
   * </pre>
   *
   * <code>.spark.connect.WriteOperation.BucketBy bucket_by = 8;</code>
   * @return The bucketBy.
   */
  org.apache.spark.connect.proto.WriteOperation.BucketBy getBucketBy();
  /**
   * <pre>
   * (Optional) Bucketing specification. Bucketing must set the number of buckets and the columns
   * to bucket by.
   * </pre>
   *
   * <code>.spark.connect.WriteOperation.BucketBy bucket_by = 8;</code>
   */
  org.apache.spark.connect.proto.WriteOperation.BucketByOrBuilder getBucketByOrBuilder();

  /**
   * <pre>
   * (Optional) A list of configuration options.
   * </pre>
   *
   * <code>map&lt;string, string&gt; options = 9;</code>
   */
  int getOptionsCount();
  /**
   * <pre>
   * (Optional) A list of configuration options.
   * </pre>
   *
   * <code>map&lt;string, string&gt; options = 9;</code>
   */
  boolean containsOptions(
      String key);
  /**
   * Use {@link #getOptionsMap()} instead.
   */
  @Deprecated
  java.util.Map<String, String>
  getOptions();
  /**
   * <pre>
   * (Optional) A list of configuration options.
   * </pre>
   *
   * <code>map&lt;string, string&gt; options = 9;</code>
   */
  java.util.Map<String, String>
  getOptionsMap();
  /**
   * <pre>
   * (Optional) A list of configuration options.
   * </pre>
   *
   * <code>map&lt;string, string&gt; options = 9;</code>
   */
  /* nullable */
String getOptionsOrDefault(
      String key,
      /* nullable */
String defaultValue);
  /**
   * <pre>
   * (Optional) A list of configuration options.
   * </pre>
   *
   * <code>map&lt;string, string&gt; options = 9;</code>
   */
  String getOptionsOrThrow(
      String key);

  org.apache.spark.connect.proto.WriteOperation.SaveTypeCase getSaveTypeCase();
}
